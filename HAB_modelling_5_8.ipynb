{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57d854a4-aa15-40da-bd45-4df04e0043ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from dateutil.parser import parse\n",
    "import datetime\n",
    "from dateutil.parser import parse\n",
    "import math\n",
    "from numpy import mean\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold, GridSearchCV, cross_validate, StratifiedKFold\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline as SKLpipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.tree import export_text\n",
    "from dtreeviz.trees import dtreeviz \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as IMBLpipeline\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import ipynbname\n",
    "notebook_name = ipynbname.name()\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 30)\n",
    "pd.set_option(\"display.max_columns\", 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d530319-a99e-4824-982c-5d531a8ac548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DSP                        1\n",
       "Dinophysis caudata         1\n",
       "Dinophysis fortii          1\n",
       "Phalacroma rotundatum      1\n",
       "Dinophysis sacculus        1\n",
       "Dinophysis tripos          1\n",
       "sun [h]                    0\n",
       "air temp                   0\n",
       "wind strength              0\n",
       "precipitation              0\n",
       "Chl-a                    422\n",
       "salinity                  21\n",
       "T                         59\n",
       "SECCHI                   450\n",
       "DIN                      352\n",
       "PO4-P                    349\n",
       "Soca                       0\n",
       "month                      0\n",
       "lipophylic_toxins        320\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read df pickle\n",
    "df_alg = pd.read_pickle(\"objects/df_alg-HAB_preprocessing_5_1\")\n",
    "# data = pd.read_pickle(\"data/preprocessed/hab_org-data-HAB_part2-preprocessing-5_2\")\n",
    "data = pd.read_pickle(\"data/preprocessed/hab_interp_data-HAB_part2-preprocessing-5_2\")\n",
    "\n",
    "data.drop(columns=[\"sampling station\", \"date\"], inplace=True)\n",
    "# data.set_index('date', inplace=True)\n",
    "\n",
    "\n",
    "# slice by station and time\n",
    "# data = data[data[\"sampling station\"] == \"Debeli_rtic\"].loc[\"2008-01-01\" : \"2021-12-31\"]\n",
    "\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4ce4a24-a8cc-4258-9d9b-3b925dc86a52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neg    996\n",
       "NaN    320\n",
       "poz    136\n",
       "Name: lipophylic_toxins, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class distribution\n",
    "data[\"lipophylic_toxins\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9945a3bf-fecf-4702-863e-7d4f489e41a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move month to first place\n",
    "cols = data.columns.tolist()  # Get a list of column names\n",
    "cols = [cols[-2]] + cols[:-2] + [cols[-1]]  # Move the one before the last column to the first position\n",
    "data = data[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e54e5716-91c5-48f2-b201-73f7444bd46c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "month                      0\n",
       "DSP                        1\n",
       "Dinophysis caudata         1\n",
       "Dinophysis fortii          1\n",
       "Phalacroma rotundatum      1\n",
       "Dinophysis sacculus        1\n",
       "Dinophysis tripos          1\n",
       "sun [h]                    0\n",
       "air temp                   0\n",
       "wind strength              0\n",
       "precipitation              0\n",
       "Chl-a                    422\n",
       "salinity                  21\n",
       "T                         59\n",
       "DIN                      352\n",
       "PO4-P                    349\n",
       "Soca                       0\n",
       "lipophylic_toxins        320\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data.drop(columns=[\"Chl-a\",\"PO4-P\",\"DIN\",\"SECCHI\"], inplace=True)\n",
    "data.drop(columns=[\"SECCHI\",  ], inplace=True)#,\"Chl-a\", \"PO4-P\", \"DIN\",\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45d93c2-6bcb-4298-bca9-f268e6330756",
   "metadata": {},
   "source": [
    "# Descriptive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73b54946-f380-4fd8-9d45-1ea70fd7965f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>DSP</th>\n",
       "      <th>Dinophysis caudata</th>\n",
       "      <th>Dinophysis fortii</th>\n",
       "      <th>Phalacroma rotundatum</th>\n",
       "      <th>Dinophysis sacculus</th>\n",
       "      <th>Dinophysis tripos</th>\n",
       "      <th>sun [h]</th>\n",
       "      <th>air temp</th>\n",
       "      <th>wind strength</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>Chl-a</th>\n",
       "      <th>salinity</th>\n",
       "      <th>T</th>\n",
       "      <th>DIN</th>\n",
       "      <th>PO4-P</th>\n",
       "      <th>Soca</th>\n",
       "      <th>lipophylic_toxins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1452.00</td>\n",
       "      <td>1451.00</td>\n",
       "      <td>1451.00</td>\n",
       "      <td>1451.00</td>\n",
       "      <td>1451.00</td>\n",
       "      <td>1451.00</td>\n",
       "      <td>1451.00</td>\n",
       "      <td>1452.00</td>\n",
       "      <td>1452.00</td>\n",
       "      <td>1452.00</td>\n",
       "      <td>1452.00</td>\n",
       "      <td>1030.00</td>\n",
       "      <td>1431.00</td>\n",
       "      <td>1393.00</td>\n",
       "      <td>1100.00</td>\n",
       "      <td>1103.00</td>\n",
       "      <td>1452.00</td>\n",
       "      <td>1132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.38</td>\n",
       "      <td>105.75</td>\n",
       "      <td>24.92</td>\n",
       "      <td>26.15</td>\n",
       "      <td>14.20</td>\n",
       "      <td>28.55</td>\n",
       "      <td>4.50</td>\n",
       "      <td>158.80</td>\n",
       "      <td>16.60</td>\n",
       "      <td>2.99</td>\n",
       "      <td>57.58</td>\n",
       "      <td>0.85</td>\n",
       "      <td>37.08</td>\n",
       "      <td>19.46</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.06</td>\n",
       "      <td>3294.64</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.82</td>\n",
       "      <td>339.89</td>\n",
       "      <td>87.92</td>\n",
       "      <td>160.06</td>\n",
       "      <td>29.93</td>\n",
       "      <td>177.90</td>\n",
       "      <td>41.18</td>\n",
       "      <td>60.63</td>\n",
       "      <td>6.06</td>\n",
       "      <td>0.41</td>\n",
       "      <td>50.71</td>\n",
       "      <td>0.71</td>\n",
       "      <td>25.50</td>\n",
       "      <td>5.33</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2329.62</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.80</td>\n",
       "      <td>-0.82</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>24.13</td>\n",
       "      <td>6.24</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>593.92</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>112.60</td>\n",
       "      <td>12.18</td>\n",
       "      <td>2.74</td>\n",
       "      <td>19.68</td>\n",
       "      <td>0.39</td>\n",
       "      <td>35.85</td>\n",
       "      <td>15.50</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1636.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>161.40</td>\n",
       "      <td>17.44</td>\n",
       "      <td>2.98</td>\n",
       "      <td>44.70</td>\n",
       "      <td>0.68</td>\n",
       "      <td>36.89</td>\n",
       "      <td>20.47</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2580.82</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10.00</td>\n",
       "      <td>90.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>207.30</td>\n",
       "      <td>21.74</td>\n",
       "      <td>3.21</td>\n",
       "      <td>79.00</td>\n",
       "      <td>1.09</td>\n",
       "      <td>37.48</td>\n",
       "      <td>23.91</td>\n",
       "      <td>3.96</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4236.82</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12.00</td>\n",
       "      <td>7630.00</td>\n",
       "      <td>1309.00</td>\n",
       "      <td>4624.00</td>\n",
       "      <td>393.00</td>\n",
       "      <td>4639.00</td>\n",
       "      <td>1139.00</td>\n",
       "      <td>277.80</td>\n",
       "      <td>26.57</td>\n",
       "      <td>5.26</td>\n",
       "      <td>267.70</td>\n",
       "      <td>9.25</td>\n",
       "      <td>999.00</td>\n",
       "      <td>28.37</td>\n",
       "      <td>35.47</td>\n",
       "      <td>3.54</td>\n",
       "      <td>16039.87</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing_values</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>422.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>59.00</td>\n",
       "      <td>352.00</td>\n",
       "      <td>349.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  month      DSP  Dinophysis caudata  Dinophysis fortii  \\\n",
       "count           1452.00  1451.00             1451.00            1451.00   \n",
       "mean               7.38   105.75               24.92              26.15   \n",
       "std                2.82   339.89               87.92             160.06   \n",
       "min                1.00     0.00                0.00               0.00   \n",
       "25%                5.00    10.00                0.00               0.00   \n",
       "50%                8.00    37.00                0.00               0.00   \n",
       "75%               10.00    90.00               13.00              10.00   \n",
       "max               12.00  7630.00             1309.00            4624.00   \n",
       "missing_values     0.00     1.00                1.00               1.00   \n",
       "\n",
       "                Phalacroma rotundatum  Dinophysis sacculus  Dinophysis tripos  \\\n",
       "count                         1451.00              1451.00            1451.00   \n",
       "mean                            14.20                28.55               4.50   \n",
       "std                             29.93               177.90              41.18   \n",
       "min                              0.00                 0.00               0.00   \n",
       "25%                              0.00                 0.00               0.00   \n",
       "50%                             10.00                 0.00               0.00   \n",
       "75%                             20.00                10.00               0.00   \n",
       "max                            393.00              4639.00            1139.00   \n",
       "missing_values                   1.00                 1.00               1.00   \n",
       "\n",
       "                sun [h]  air temp  wind strength  precipitation    Chl-a  \\\n",
       "count           1452.00   1452.00        1452.00        1452.00  1030.00   \n",
       "mean             158.80     16.60           2.99          57.58     0.85   \n",
       "std               60.63      6.06           0.41          50.71     0.71   \n",
       "min               22.80     -0.82           1.48           0.00     0.09   \n",
       "25%              112.60     12.18           2.74          19.68     0.39   \n",
       "50%              161.40     17.44           2.98          44.70     0.68   \n",
       "75%              207.30     21.74           3.21          79.00     1.09   \n",
       "max              277.80     26.57           5.26         267.70     9.25   \n",
       "missing_values     0.00      0.00           0.00           0.00   422.00   \n",
       "\n",
       "                salinity        T      DIN    PO4-P      Soca  \\\n",
       "count            1431.00  1393.00  1100.00  1103.00   1452.00   \n",
       "mean               37.08    19.46     3.29     0.06   3294.64   \n",
       "std                25.50     5.33     3.94     0.16   2329.62   \n",
       "min                24.13     6.24     0.06     0.00    593.92   \n",
       "25%                35.85    15.50     0.87     0.02   1636.00   \n",
       "50%                36.89    20.47     1.98     0.04   2580.82   \n",
       "75%                37.48    23.91     3.96     0.08   4236.82   \n",
       "max               999.00    28.37    35.47     3.54  16039.87   \n",
       "missing_values     21.00    59.00   352.00   349.00      0.00   \n",
       "\n",
       "               lipophylic_toxins  \n",
       "count                       1132  \n",
       "mean                         NaN  \n",
       "std                          NaN  \n",
       "min                          NaN  \n",
       "25%                          NaN  \n",
       "50%                          NaN  \n",
       "75%                          NaN  \n",
       "max                          NaN  \n",
       "missing_values               320  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# describe\n",
    "description = data.describe(include='all').round(2)\n",
    "\n",
    "# Calculate the number of missing values for each column\n",
    "missing_values = data.isna().sum()\n",
    "missing_values.name = 'missing_values'\n",
    "\n",
    "# Append the missing_values row to the description DataFrame\n",
    "description_with_missing = description.append(missing_values)\n",
    "description_with_missing = description_with_missing.drop(['unique', 'top', 'freq'])\n",
    "\n",
    "description_with_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7501e318-06de-4e74-ac3e-04965a6ccd58",
   "metadata": {},
   "source": [
    "## Descriptive analysis by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3c39a42-f94a-476e-9e49-1962461bf7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DSP</th>\n",
       "      <th>Dinophysis caudata</th>\n",
       "      <th>Dinophysis fortii</th>\n",
       "      <th>Phalacroma rotundatum</th>\n",
       "      <th>Dinophysis sacculus</th>\n",
       "      <th>Dinophysis tripos</th>\n",
       "      <th>sun [h]</th>\n",
       "      <th>air temp</th>\n",
       "      <th>wind strength</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>Chl-a</th>\n",
       "      <th>salinity</th>\n",
       "      <th>T</th>\n",
       "      <th>DIN</th>\n",
       "      <th>PO4-P</th>\n",
       "      <th>Soca</th>\n",
       "      <th>neg</th>\n",
       "      <th>poz</th>\n",
       "      <th>poz %</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>January</th>\n",
       "      <td>9.00</td>\n",
       "      <td>1.16</td>\n",
       "      <td>2.96</td>\n",
       "      <td>3.66</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.20</td>\n",
       "      <td>72.62</td>\n",
       "      <td>4.95</td>\n",
       "      <td>2.71</td>\n",
       "      <td>46.53</td>\n",
       "      <td>0.76</td>\n",
       "      <td>36.95</td>\n",
       "      <td>11.07</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4652.16</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>February</th>\n",
       "      <td>12.86</td>\n",
       "      <td>1.02</td>\n",
       "      <td>4.08</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.69</td>\n",
       "      <td>80.19</td>\n",
       "      <td>5.52</td>\n",
       "      <td>3.06</td>\n",
       "      <td>60.66</td>\n",
       "      <td>0.77</td>\n",
       "      <td>37.02</td>\n",
       "      <td>10.22</td>\n",
       "      <td>6.66</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4378.79</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>March</th>\n",
       "      <td>12.38</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.68</td>\n",
       "      <td>3.79</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.64</td>\n",
       "      <td>118.56</td>\n",
       "      <td>7.55</td>\n",
       "      <td>3.30</td>\n",
       "      <td>30.01</td>\n",
       "      <td>0.77</td>\n",
       "      <td>37.17</td>\n",
       "      <td>9.49</td>\n",
       "      <td>4.37</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3257.07</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>April</th>\n",
       "      <td>15.49</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.48</td>\n",
       "      <td>5.24</td>\n",
       "      <td>4.51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>144.63</td>\n",
       "      <td>11.69</td>\n",
       "      <td>3.17</td>\n",
       "      <td>54.90</td>\n",
       "      <td>0.68</td>\n",
       "      <td>36.93</td>\n",
       "      <td>11.68</td>\n",
       "      <td>4.68</td>\n",
       "      <td>0.06</td>\n",
       "      <td>3826.10</td>\n",
       "      <td>66</td>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>May</th>\n",
       "      <td>59.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.97</td>\n",
       "      <td>9.67</td>\n",
       "      <td>34.25</td>\n",
       "      <td>0.15</td>\n",
       "      <td>171.96</td>\n",
       "      <td>16.22</td>\n",
       "      <td>2.89</td>\n",
       "      <td>48.55</td>\n",
       "      <td>0.96</td>\n",
       "      <td>36.57</td>\n",
       "      <td>14.87</td>\n",
       "      <td>3.92</td>\n",
       "      <td>0.07</td>\n",
       "      <td>3864.97</td>\n",
       "      <td>89</td>\n",
       "      <td>14</td>\n",
       "      <td>10.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>June</th>\n",
       "      <td>146.89</td>\n",
       "      <td>29.23</td>\n",
       "      <td>1.49</td>\n",
       "      <td>12.87</td>\n",
       "      <td>93.28</td>\n",
       "      <td>0.09</td>\n",
       "      <td>202.85</td>\n",
       "      <td>20.51</td>\n",
       "      <td>2.83</td>\n",
       "      <td>45.16</td>\n",
       "      <td>1.09</td>\n",
       "      <td>35.60</td>\n",
       "      <td>20.28</td>\n",
       "      <td>3.58</td>\n",
       "      <td>0.13</td>\n",
       "      <td>3436.93</td>\n",
       "      <td>115</td>\n",
       "      <td>12</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>July</th>\n",
       "      <td>237.04</td>\n",
       "      <td>67.07</td>\n",
       "      <td>44.50</td>\n",
       "      <td>19.57</td>\n",
       "      <td>97.25</td>\n",
       "      <td>0.04</td>\n",
       "      <td>227.47</td>\n",
       "      <td>23.31</td>\n",
       "      <td>2.88</td>\n",
       "      <td>38.27</td>\n",
       "      <td>0.75</td>\n",
       "      <td>35.40</td>\n",
       "      <td>23.48</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2473.18</td>\n",
       "      <td>125</td>\n",
       "      <td>18</td>\n",
       "      <td>13.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>August</th>\n",
       "      <td>129.67</td>\n",
       "      <td>55.46</td>\n",
       "      <td>24.23</td>\n",
       "      <td>19.01</td>\n",
       "      <td>22.96</td>\n",
       "      <td>0.08</td>\n",
       "      <td>220.46</td>\n",
       "      <td>23.46</td>\n",
       "      <td>3.03</td>\n",
       "      <td>54.38</td>\n",
       "      <td>0.63</td>\n",
       "      <td>41.92</td>\n",
       "      <td>24.93</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1637.28</td>\n",
       "      <td>126</td>\n",
       "      <td>11</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>September</th>\n",
       "      <td>116.79</td>\n",
       "      <td>28.04</td>\n",
       "      <td>58.58</td>\n",
       "      <td>12.81</td>\n",
       "      <td>3.57</td>\n",
       "      <td>6.04</td>\n",
       "      <td>174.65</td>\n",
       "      <td>20.06</td>\n",
       "      <td>3.11</td>\n",
       "      <td>69.73</td>\n",
       "      <td>0.64</td>\n",
       "      <td>36.59</td>\n",
       "      <td>24.38</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2102.01</td>\n",
       "      <td>143</td>\n",
       "      <td>31</td>\n",
       "      <td>22.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>October</th>\n",
       "      <td>97.76</td>\n",
       "      <td>13.21</td>\n",
       "      <td>35.56</td>\n",
       "      <td>19.39</td>\n",
       "      <td>2.17</td>\n",
       "      <td>19.76</td>\n",
       "      <td>127.60</td>\n",
       "      <td>15.56</td>\n",
       "      <td>3.08</td>\n",
       "      <td>72.07</td>\n",
       "      <td>0.80</td>\n",
       "      <td>36.91</td>\n",
       "      <td>21.70</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2988.28</td>\n",
       "      <td>138</td>\n",
       "      <td>24</td>\n",
       "      <td>17.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>November</th>\n",
       "      <td>80.54</td>\n",
       "      <td>13.81</td>\n",
       "      <td>29.33</td>\n",
       "      <td>19.09</td>\n",
       "      <td>1.11</td>\n",
       "      <td>8.90</td>\n",
       "      <td>82.48</td>\n",
       "      <td>11.35</td>\n",
       "      <td>3.01</td>\n",
       "      <td>74.21</td>\n",
       "      <td>1.20</td>\n",
       "      <td>36.72</td>\n",
       "      <td>17.61</td>\n",
       "      <td>3.09</td>\n",
       "      <td>0.06</td>\n",
       "      <td>5263.97</td>\n",
       "      <td>91</td>\n",
       "      <td>13</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>December</th>\n",
       "      <td>81.96</td>\n",
       "      <td>2.33</td>\n",
       "      <td>51.95</td>\n",
       "      <td>16.06</td>\n",
       "      <td>1.18</td>\n",
       "      <td>4.77</td>\n",
       "      <td>65.97</td>\n",
       "      <td>7.51</td>\n",
       "      <td>2.83</td>\n",
       "      <td>81.32</td>\n",
       "      <td>1.20</td>\n",
       "      <td>36.81</td>\n",
       "      <td>15.56</td>\n",
       "      <td>4.09</td>\n",
       "      <td>0.06</td>\n",
       "      <td>5868.74</td>\n",
       "      <td>39</td>\n",
       "      <td>3</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              DSP  Dinophysis caudata  Dinophysis fortii  \\\n",
       "month                                                      \n",
       "January      9.00                1.16               2.96   \n",
       "February    12.86                1.02               4.08   \n",
       "March       12.38                0.21               0.68   \n",
       "April       15.49                0.97               0.48   \n",
       "May         59.00                6.00               0.97   \n",
       "June       146.89               29.23               1.49   \n",
       "July       237.04               67.07              44.50   \n",
       "August     129.67               55.46              24.23   \n",
       "September  116.79               28.04              58.58   \n",
       "October     97.76               13.21              35.56   \n",
       "November    80.54               13.81              29.33   \n",
       "December    81.96                2.33              51.95   \n",
       "\n",
       "           Phalacroma rotundatum  Dinophysis sacculus  Dinophysis tripos  \\\n",
       "month                                                                      \n",
       "January                     3.66                 0.09               0.20   \n",
       "February                    3.38                 0.20               0.69   \n",
       "March                       3.79                 0.43               0.64   \n",
       "April                       5.24                 4.51               0.00   \n",
       "May                         9.67                34.25               0.15   \n",
       "June                       12.87                93.28               0.09   \n",
       "July                       19.57                97.25               0.04   \n",
       "August                     19.01                22.96               0.08   \n",
       "September                  12.81                 3.57               6.04   \n",
       "October                    19.39                 2.17              19.76   \n",
       "November                   19.09                 1.11               8.90   \n",
       "December                   16.06                 1.18               4.77   \n",
       "\n",
       "           sun [h]  air temp  wind strength  precipitation  Chl-a  salinity  \\\n",
       "month                                                                         \n",
       "January      72.62      4.95           2.71          46.53   0.76     36.95   \n",
       "February     80.19      5.52           3.06          60.66   0.77     37.02   \n",
       "March       118.56      7.55           3.30          30.01   0.77     37.17   \n",
       "April       144.63     11.69           3.17          54.90   0.68     36.93   \n",
       "May         171.96     16.22           2.89          48.55   0.96     36.57   \n",
       "June        202.85     20.51           2.83          45.16   1.09     35.60   \n",
       "July        227.47     23.31           2.88          38.27   0.75     35.40   \n",
       "August      220.46     23.46           3.03          54.38   0.63     41.92   \n",
       "September   174.65     20.06           3.11          69.73   0.64     36.59   \n",
       "October     127.60     15.56           3.08          72.07   0.80     36.91   \n",
       "November     82.48     11.35           3.01          74.21   1.20     36.72   \n",
       "December     65.97      7.51           2.83          81.32   1.20     36.81   \n",
       "\n",
       "               T   DIN  PO4-P     Soca  neg  poz  poz %  \n",
       "month                                                    \n",
       "January    11.07  6.79   0.07  4652.16   15    3    2.2  \n",
       "February   10.22  6.66   0.08  4378.79   21    3    2.2  \n",
       "March       9.49  4.37   0.05  3257.07   28    2    1.5  \n",
       "April      11.68  4.68   0.06  3826.10   66    2    1.5  \n",
       "May        14.87  3.92   0.07  3864.97   89   14   10.3  \n",
       "June       20.28  3.58   0.13  3436.93  115   12    8.8  \n",
       "July       23.48  3.38   0.05  2473.18  125   18   13.2  \n",
       "August     24.93  2.03   0.05  1637.28  126   11    8.1  \n",
       "September  24.38  2.06   0.05  2102.01  143   31   22.8  \n",
       "October    21.70  1.95   0.05  2988.28  138   24   17.6  \n",
       "November   17.61  3.09   0.06  5263.97   91   13    9.6  \n",
       "December   15.56  4.09   0.06  5868.74   39    3    2.2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# table of mean values for each feature by month \n",
    "import calendar\n",
    "\n",
    "grouped_means = data.groupby('month').mean()\n",
    "\n",
    "# Count binary values for the categorical feature grouped by month\n",
    "binary_counts = data.groupby('month')['lipophylic_toxins'].value_counts().unstack()\n",
    "\n",
    "# Calculate the ratio of positive values for each month\n",
    "sum_positive = binary_counts[\"poz\"].sum()\n",
    "positive_ratios = [i for i in (binary_counts[\"poz\"] / sum_positive)] \n",
    "positive_ratios = [round(v * 100, 1) for v in positive_ratios]\n",
    "\n",
    "# Change month names\n",
    "month_names = {i: calendar.month_name[i] for i in range(1, 13)}\n",
    "\n",
    "# Update the index using the month_names dictionary\n",
    "grouped_means.index = grouped_means.index.map(month_names)\n",
    "binary_counts.index = binary_counts.index.map(month_names)\n",
    "\n",
    "# Concatenate the grouped_means and binary_counts DataFrames\n",
    "result = pd.concat([grouped_means, binary_counts], axis=1).round(2)\n",
    "\n",
    "# Add the positive_ratios Series as a new column to the result DataFrame\n",
    "result[\"poz %\"] = positive_ratios\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeda4953-b0aa-423e-b844-606188480710",
   "metadata": {},
   "source": [
    "# Data preprocessing for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc307518-a30c-493f-a0a9-888382107aa9",
   "metadata": {},
   "source": [
    "## Removing instances with unlabeled target, label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67f0106d-3b54-4623-8102-48ba8abd2460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class distribution:\n",
      "neg    662\n",
      "poz     88\n",
      "Name: lipophylic_toxins, dtype: int64\n",
      "class encoding: ['neg','poz'] -> [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Prepare for ML in scikit-learn\n",
    "# labeled and unlabeled part\n",
    "data_l = data[data['lipophylic_toxins'].notnull()]\n",
    "data_ul = data[data['lipophylic_toxins'].isnull()]\n",
    "\n",
    "# Remove missing values\n",
    "data_l = data_l.dropna(how=\"any\")\n",
    "print(f\"class distribution:\")\n",
    "print(data_l[\"lipophylic_toxins\"].value_counts(dropna=False))\n",
    "\n",
    "X = data_l.drop(\"lipophylic_toxins\", axis=1)\n",
    "y = data_l[\"lipophylic_toxins\"]\n",
    "\n",
    "# sklearn lable encoding\n",
    "le = LabelEncoder()\n",
    "le.fit(y)\n",
    "y = le.transform(y)\n",
    "print(f\"class encoding: ['neg','poz'] -> {le.transform(['neg','poz'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4e96a9-b60a-4ac7-9efe-2780a5c9ef12",
   "metadata": {},
   "source": [
    "## Clean instances close to the decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cbf75b-2474-48d0-b650-6c60d21d4829",
   "metadata": {},
   "source": [
    "Clean the dataset by removing samples close to the decision boundary. Because the dataset is heavily imbalanced in favor of clas 0 (neg) we will remove instances from this class whenever finding samples which do not agree “enough” with their neighboorhood. The EditedNearestNeighbours will be used. One other option is to use Tomek links but it is more conservative and was found to perform slightly worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b9e6e31-35f7-41b5-a9b0-a1f1876c9a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: Counter({0: 662, 1: 88})\n",
      "Resampled dataset shape Counter({0: 540, 1: 88})\n",
      "Resampled dataset shape Counter({0: 501, 1: 88})\n",
      "Resampled dataset shape Counter({0: 490, 1: 88})\n",
      "Resampled dataset shape Counter({0: 488, 1: 88})\n",
      "Resampled dataset shape Counter({0: 487, 1: 88})\n",
      "Cannot remove any more samples\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours\n",
    "\n",
    "print(f'Original dataset shape: {Counter(y)}')\n",
    "usmp = EditedNearestNeighbours()\n",
    "lastMajorityCount = Counter(y)[0]\n",
    "for i in range(10):\n",
    "    X_res, y_res = usmp.fit_resample(X, y)\n",
    "    if Counter(y_res)[0] == lastMajorityCount:\n",
    "        print('Cannot remove any more samples')\n",
    "        break\n",
    "    else:\n",
    "        print(f'Resampled dataset shape {Counter(y_res)}')\n",
    "        lastMajorityCount = Counter(y_res)[0]\n",
    "    X = X_res\n",
    "    y = y_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d58759b-b41a-4938-9acb-c736a6016b63",
   "metadata": {},
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d406548-0ce8-44bf-9ba2-6311c8c9c2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X, X_eval, y, y_eval = train_test_split(X, y, shuffle=True, stratify=y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e68a779-d7ad-4bf9-b597-594cfbf760e6",
   "metadata": {},
   "source": [
    "# Correlation analysis on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e93f704-b7e9-4383-bf61-a15e4e4080ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Pearson  Spearman\n",
      "DSP                    0.050517  0.112360\n",
      "T                      0.133431  0.108940\n",
      "Phalacroma rotundatum  0.027626  0.078264\n",
      "Chl-a                 -0.081980 -0.067924\n",
      "air temp               0.101362  0.061298\n",
      "month                  0.079418  0.060695\n",
      "Dinophysis caudata     0.159930  0.055140\n",
      "DIN                    0.048785  0.038620\n",
      "PO4-P                 -0.046319 -0.037694\n",
      "Dinophysis sacculus   -0.012564 -0.037541\n",
      "precipitation          0.032200  0.036034\n",
      "wind strength          0.003026  0.035853\n",
      "sun [h]                0.048582  0.035070\n",
      "salinity              -0.055177 -0.025085\n",
      "Dinophysis tripos      0.076581  0.018183\n",
      "Soca                  -0.043240 -0.011008\n",
      "Dinophysis fortii     -0.035430 -0.002735\n"
     ]
    }
   ],
   "source": [
    "# Calculate Pearson correlation between numeric features and binary target variable\n",
    "pearson_correlations = X.corrwith(pd.Series(y), method='pearson')\n",
    "pearson_correlations.name = 'Pearson'\n",
    "\n",
    "# Calculate Spearman rank correlation between numeric features and binary target variable\n",
    "spearman_correlations = X.corrwith(pd.Series(y), method='spearman')\n",
    "spearman_correlations.name = 'Spearman'\n",
    "\n",
    "# Combine the correlation values into a single dataframe\n",
    "corr_df = pd.concat([pearson_correlations, spearman_correlations], axis=1)\n",
    "\n",
    "# Create a new column for absolute Spearman correlation values\n",
    "corr_df['Spearman_abs'] = corr_df['Spearman'].abs()\n",
    "\n",
    "# Sort the dataframe by absolute Spearman correlation values\n",
    "corr_df_sorted = corr_df.sort_values(by=['Spearman_abs'], ascending=False)\n",
    "\n",
    "# Drop the absolute Spearman correlation column and return the sorted dataframe\n",
    "corr_df_ranked = corr_df_sorted.drop(columns=['Spearman_abs'])\n",
    "\n",
    "print(corr_df_ranked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b146cf-97fb-4a9b-8e33-dac52f91b48c",
   "metadata": {},
   "source": [
    "## Logistic regression and p-value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89623bf-dd24-405f-9c3d-c53725a26763",
   "metadata": {},
   "source": [
    "We use the coef_pval() method to calculate the p-values for each coefficient in the logistic regression model, and create a dataframe with the logistic regression coefficients and corresponding p-values. Finally, we rank the features by their absolute logistic regression coefficients and print the resulting dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dc96193-51d8-41c2-8019-e653f50e2f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.342655\n",
      "         Iterations 8\n",
      "                       Coefficient  P-value\n",
      "PO4-P                        3.269    0.306\n",
      "wind strength               -0.950    0.034\n",
      "Chl-a                        0.420    0.133\n",
      "salinity                    -0.164    0.119\n",
      "DIN                          0.080    0.136\n",
      "month                       -0.056    0.546\n",
      "Dinophysis tripos            0.041    0.028\n",
      "Phalacroma rotundatum       -0.029    0.079\n",
      "T                            0.012    0.872\n",
      "precipitation                0.009    0.015\n",
      "sun [h]                     -0.008    0.304\n",
      "air temp                     0.008    0.930\n",
      "Dinophysis fortii            0.007    0.463\n",
      "Dinophysis sacculus          0.001    0.921\n",
      "DSP                         -0.001    0.929\n",
      "Soca                        -0.000    0.001\n",
      "Dinophysis caudata           0.000    0.999\n"
     ]
    }
   ],
   "source": [
    "# logistic regression and p-value\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Fit logistic regression model\n",
    "logit_model = sm.Logit(y, sm.add_constant(X))\n",
    "result = logit_model.fit()\n",
    "\n",
    "# Calculate p-values for the logistic regression coefficients\n",
    "p_values = result.pvalues[1:]\n",
    "\n",
    "# Create a dataframe with the logistic regression coefficients and corresponding p-values\n",
    "coef_df = pd.DataFrame({'Coefficient': result.params[1:], 'P-value': p_values})\n",
    "\n",
    "# Add feature names as the index\n",
    "coef_df.index = X.columns\n",
    "\n",
    "# Rank the features by absolute logistic regression coefficients\n",
    "coef_df['Absolute Coefficient'] = coef_df['Coefficient'].abs()\n",
    "coef_df_sorted = coef_df.sort_values(by=['Absolute Coefficient'], ascending=False)\n",
    "coef_df_ranked = coef_df_sorted.drop(columns=['Absolute Coefficient'])\n",
    "\n",
    "print(coef_df_ranked.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17e63f2-39c7-4ab8-bb0e-22e01991f9e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afe5386-abb2-4fac-bb14-3e35e9124faa",
   "metadata": {},
   "source": [
    "## Baseline model - logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f171e07e-333c-4786-8814-efec9ee7009f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.87      0.97      0.92       147\n",
      "         poz       0.56      0.19      0.29        26\n",
      "\n",
      "    accuracy                           0.86       173\n",
      "   macro avg       0.71      0.58      0.60       173\n",
      "weighted avg       0.82      0.86      0.82       173\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
     ]
    }
   ],
   "source": [
    "# baseline model\n",
    "\n",
    "# Fit logistic regression model using scikit-learn\n",
    "logreg = LogisticRegression(max_iter=100)\n",
    "logreg.fit(X, y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logreg.predict(X_eval)\n",
    "\n",
    "# Evaluation on test data\n",
    "y_pred = logreg.predict(X_eval)\n",
    "lr_classification_report = classification_report(y_eval, y_pred)\n",
    "\n",
    "# Create classification report as dictionary\n",
    "lr_report_dict = classification_report(y_eval, y_pred, output_dict=True)\n",
    "\n",
    "print(classification_report(y_eval, y_pred, target_names=[\"neg\", \"poz\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "937d04bc-1607-4daf-97e8-4df4f04007ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving models\n",
    "\n",
    "# Get the current date and time as a string\n",
    "timestamp = datetime.datetime.now().strftime('%d%m%Y_%H%M')\n",
    "\n",
    "# Construct the directory path with the models folder and timestamp\n",
    "dir_path = f\"models/{timestamp}\"\n",
    "    \n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(dir_path):\n",
    "    os.makedirs(dir_path)\n",
    "\n",
    "# Function to save best estimator\n",
    "def save_best_estimator(grid_search_cv, classifier_name, notebook_name):\n",
    "    # Get the best estimator from the GridSearchCV object\n",
    "    best_estimator = grid_search_cv.best_estimator_\n",
    "\n",
    "    # Construct the file name with the classifier name and notebook name\n",
    "    pickle_file_name = f\"{dir_path}/{classifier_name}-{notebook_name}-{timestamp}.pkl\"\n",
    "    \n",
    "    # Save the best estimator to the file\n",
    "    with open(pickle_file_name, 'wb') as f:\n",
    "        pickle.dump(best_estimator, f)\n",
    "\n",
    "    print(f\"Best estimator saved as: {pickle_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5621711-ef2c-49e0-b509-f879c47dffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for loading saved models\n",
    "\n",
    "def load_best_estimator(pickle_file_name):\n",
    "    with open(pickle_file_name, 'rb') as f:\n",
    "        best_estimator = pickle.load(f)\n",
    "    return best_estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3996100-0bd8-447a-a1b9-3a090bd55cab",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "907477fc-ebae-4d60-a61f-8bacc5693a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n",
      "Best estimator saved as: models/11042023_1923/SVC-HAB_modelling_5_8-11042023_1923.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>133</th>\n",
       "      <th>132</th>\n",
       "      <th>134</th>\n",
       "      <th>128</th>\n",
       "      <th>122</th>\n",
       "      <th>125</th>\n",
       "      <th>111</th>\n",
       "      <th>114</th>\n",
       "      <th>129</th>\n",
       "      <th>123</th>\n",
       "      <th>117</th>\n",
       "      <th>18</th>\n",
       "      <th>24</th>\n",
       "      <th>67</th>\n",
       "      <th>21</th>\n",
       "      <th>54</th>\n",
       "      <th>127</th>\n",
       "      <th>...</th>\n",
       "      <th>42</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>46</th>\n",
       "      <th>53</th>\n",
       "      <th>34</th>\n",
       "      <th>106</th>\n",
       "      <th>105</th>\n",
       "      <th>104</th>\n",
       "      <th>103</th>\n",
       "      <th>102</th>\n",
       "      <th>100</th>\n",
       "      <th>99</th>\n",
       "      <th>48</th>\n",
       "      <th>52</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>0.025344</td>\n",
       "      <td>0.026636</td>\n",
       "      <td>0.023242</td>\n",
       "      <td>0.022241</td>\n",
       "      <td>0.020892</td>\n",
       "      <td>0.025486</td>\n",
       "      <td>0.023287</td>\n",
       "      <td>0.02705</td>\n",
       "      <td>0.023229</td>\n",
       "      <td>0.025859</td>\n",
       "      <td>0.022815</td>\n",
       "      <td>0.020464</td>\n",
       "      <td>0.027063</td>\n",
       "      <td>0.022156</td>\n",
       "      <td>0.022236</td>\n",
       "      <td>0.025043</td>\n",
       "      <td>0.021825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026041</td>\n",
       "      <td>0.021706</td>\n",
       "      <td>0.022629</td>\n",
       "      <td>0.015893</td>\n",
       "      <td>0.018674</td>\n",
       "      <td>0.01967</td>\n",
       "      <td>0.023284</td>\n",
       "      <td>0.023406</td>\n",
       "      <td>0.025392</td>\n",
       "      <td>0.025357</td>\n",
       "      <td>0.02278</td>\n",
       "      <td>0.021172</td>\n",
       "      <td>0.023036</td>\n",
       "      <td>0.019765</td>\n",
       "      <td>0.020109</td>\n",
       "      <td>0.021999</td>\n",
       "      <td>0.02433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.000894</td>\n",
       "      <td>0.000637</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.00097</td>\n",
       "      <td>0.001029</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>0.001761</td>\n",
       "      <td>0.000962</td>\n",
       "      <td>0.001687</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.00554</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002695</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.003505</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.005904</td>\n",
       "      <td>0.002642</td>\n",
       "      <td>0.002358</td>\n",
       "      <td>0.002658</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.0013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.023527</td>\n",
       "      <td>0.024804</td>\n",
       "      <td>0.023794</td>\n",
       "      <td>0.017368</td>\n",
       "      <td>0.020337</td>\n",
       "      <td>0.02406</td>\n",
       "      <td>0.02176</td>\n",
       "      <td>0.025543</td>\n",
       "      <td>0.023244</td>\n",
       "      <td>0.025022</td>\n",
       "      <td>0.018517</td>\n",
       "      <td>0.017126</td>\n",
       "      <td>0.02633</td>\n",
       "      <td>0.020831</td>\n",
       "      <td>0.024156</td>\n",
       "      <td>0.023716</td>\n",
       "      <td>0.018995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022005</td>\n",
       "      <td>0.019321</td>\n",
       "      <td>0.015919</td>\n",
       "      <td>0.013993</td>\n",
       "      <td>0.022419</td>\n",
       "      <td>0.017481</td>\n",
       "      <td>0.022354</td>\n",
       "      <td>0.022058</td>\n",
       "      <td>0.02618</td>\n",
       "      <td>0.022059</td>\n",
       "      <td>0.019854</td>\n",
       "      <td>0.021078</td>\n",
       "      <td>0.019764</td>\n",
       "      <td>0.01739</td>\n",
       "      <td>0.018541</td>\n",
       "      <td>0.020493</td>\n",
       "      <td>0.021843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.002499</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>0.001178</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.004026</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>0.003108</td>\n",
       "      <td>0.00329</td>\n",
       "      <td>0.00617</td>\n",
       "      <td>0.000696</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.00148</td>\n",
       "      <td>0.003024</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.001557</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.001282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__C</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__class_weight</th>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_smt__k_neighbors</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_smt__sampling_strategy</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_under__sampling_strategy</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'clf__C': 10, 'clf__class_weight': 'balanced'...</td>\n",
       "      <td>{'clf__C': 10, 'clf__class_weight': 'balanced'...</td>\n",
       "      <td>{'clf__C': 10, 'clf__class_weight': 'balanced'...</td>\n",
       "      <td>{'clf__C': 10, 'clf__class_weight': 'balanced'...</td>\n",
       "      <td>{'clf__C': 10, 'clf__class_weight': 'balanced'...</td>\n",
       "      <td>{'clf__C': 10, 'clf__class_weight': 'balanced'...</td>\n",
       "      <td>{'clf__C': 10, 'clf__class_weight': 'balanced'...</td>\n",
       "      <td>{'clf__C': 10, 'clf__class_weight': 'balanced'...</td>\n",
       "      <td>{'clf__C': 10, 'clf__class_weight': 'balanced'...</td>\n",
       "      <td>{'clf__C': 10, 'clf__class_weight': 'balanced'...</td>\n",
       "      <td>{'clf__C': 10, 'clf__class_weight': 'balanced'...</td>\n",
       "      <td>{'clf__C': 0.1, 'clf__class_weight': 'balanced...</td>\n",
       "      <td>{'clf__C': 0.1, 'clf__class_weight': 'balanced...</td>\n",
       "      <td>{'clf__C': 1, 'clf__class_weight': 'balanced',...</td>\n",
       "      <td>{'clf__C': 0.1, 'clf__class_weight': 'balanced...</td>\n",
       "      <td>{'clf__C': 1, 'clf__class_weight': 'balanced',...</td>\n",
       "      <td>{'clf__C': 10, 'clf__class_weight': 'balanced'...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'clf__C': 0.1, 'clf__class_weight': None, 'sm...</td>\n",
       "      <td>{'clf__C': 0.1, 'clf__class_weight': None, 'sm...</td>\n",
       "      <td>{'clf__C': 0.1, 'clf__class_weight': None, 'sm...</td>\n",
       "      <td>{'clf__C': 0.1, 'clf__class_weight': None, 'sm...</td>\n",
       "      <td>{'clf__C': 0.1, 'clf__class_weight': None, 'sm...</td>\n",
       "      <td>{'clf__C': 0.1, 'clf__class_weight': None, 'sm...</td>\n",
       "      <td>{'clf__C': 0.1, 'clf__class_weight': None, 'sm...</td>\n",
       "      <td>{'clf__C': 0.1, 'clf__class_weight': None, 'sm...</td>\n",
       "      <td>{'clf__C': 1, 'clf__class_weight': None, 'smt_...</td>\n",
       "      <td>{'clf__C': 1, 'clf__class_weight': None, 'smt_...</td>\n",
       "      <td>{'clf__C': 1, 'clf__class_weight': None, 'smt_...</td>\n",
       "      <td>{'clf__C': 1, 'clf__class_weight': None, 'smt_...</td>\n",
       "      <td>{'clf__C': 1, 'clf__class_weight': None, 'smt_...</td>\n",
       "      <td>{'clf__C': 1, 'clf__class_weight': None, 'smt_...</td>\n",
       "      <td>{'clf__C': 1, 'clf__class_weight': None, 'smt_...</td>\n",
       "      <td>{'clf__C': 0.1, 'clf__class_weight': None, 'sm...</td>\n",
       "      <td>{'clf__C': 0.1, 'clf__class_weight': None, 'sm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_recall</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_recall</th>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_recall</th>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_recall</th>\n",
       "      <td>0.468254</td>\n",
       "      <td>0.484127</td>\n",
       "      <td>0.548413</td>\n",
       "      <td>0.484921</td>\n",
       "      <td>0.499206</td>\n",
       "      <td>0.450794</td>\n",
       "      <td>0.500794</td>\n",
       "      <td>0.436508</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.468254</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.484921</td>\n",
       "      <td>0.484921</td>\n",
       "      <td>0.515873</td>\n",
       "      <td>0.468254</td>\n",
       "      <td>0.46746</td>\n",
       "      <td>0.46746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_recall</th>\n",
       "      <td>0.062492</td>\n",
       "      <td>0.011224</td>\n",
       "      <td>0.097208</td>\n",
       "      <td>0.049956</td>\n",
       "      <td>0.052176</td>\n",
       "      <td>0.035917</td>\n",
       "      <td>0.052176</td>\n",
       "      <td>0.0736</td>\n",
       "      <td>0.062904</td>\n",
       "      <td>0.029696</td>\n",
       "      <td>0.033672</td>\n",
       "      <td>0.049956</td>\n",
       "      <td>0.049956</td>\n",
       "      <td>0.040468</td>\n",
       "      <td>0.029696</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_recall</th>\n",
       "      <td>29</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>40</td>\n",
       "      <td>12</td>\n",
       "      <td>44</td>\n",
       "      <td>26</td>\n",
       "      <td>29</td>\n",
       "      <td>39</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_precision</th>\n",
       "      <td>0.25641</td>\n",
       "      <td>0.27027</td>\n",
       "      <td>0.22449</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.27027</td>\n",
       "      <td>0.163636</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.234043</td>\n",
       "      <td>0.203704</td>\n",
       "      <td>0.204082</td>\n",
       "      <td>0.204082</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_precision</th>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.219512</td>\n",
       "      <td>0.232558</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.232558</td>\n",
       "      <td>0.209302</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.209302</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.218182</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.243902</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_precision</th>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.21875</td>\n",
       "      <td>0.292683</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.232558</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_precision</th>\n",
       "      <td>0.273343</td>\n",
       "      <td>0.257174</td>\n",
       "      <td>0.232849</td>\n",
       "      <td>0.243873</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.244186</td>\n",
       "      <td>0.232925</td>\n",
       "      <td>0.242372</td>\n",
       "      <td>0.231054</td>\n",
       "      <td>0.233165</td>\n",
       "      <td>0.236434</td>\n",
       "      <td>0.228772</td>\n",
       "      <td>0.228158</td>\n",
       "      <td>0.22012</td>\n",
       "      <td>0.230308</td>\n",
       "      <td>0.224038</td>\n",
       "      <td>0.223257</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_precision</th>\n",
       "      <td>0.022787</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.015476</td>\n",
       "      <td>0.017283</td>\n",
       "      <td>0.029998</td>\n",
       "      <td>0.024125</td>\n",
       "      <td>0.040166</td>\n",
       "      <td>0.021249</td>\n",
       "      <td>0.052842</td>\n",
       "      <td>0.012081</td>\n",
       "      <td>0.021988</td>\n",
       "      <td>0.003841</td>\n",
       "      <td>0.01899</td>\n",
       "      <td>0.013953</td>\n",
       "      <td>0.019307</td>\n",
       "      <td>0.020599</td>\n",
       "      <td>0.010568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_precision</th>\n",
       "      <td>25</td>\n",
       "      <td>27</td>\n",
       "      <td>42</td>\n",
       "      <td>31</td>\n",
       "      <td>39</td>\n",
       "      <td>30</td>\n",
       "      <td>41</td>\n",
       "      <td>32</td>\n",
       "      <td>45</td>\n",
       "      <td>40</td>\n",
       "      <td>36</td>\n",
       "      <td>47</td>\n",
       "      <td>48</td>\n",
       "      <td>58</td>\n",
       "      <td>46</td>\n",
       "      <td>52</td>\n",
       "      <td>54</td>\n",
       "      <td>...</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_f1</th>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.350877</td>\n",
       "      <td>0.318841</td>\n",
       "      <td>0.360656</td>\n",
       "      <td>0.276923</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.350877</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.328358</td>\n",
       "      <td>0.297297</td>\n",
       "      <td>0.289855</td>\n",
       "      <td>0.289855</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_f1</th>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.31746</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.28125</td>\n",
       "      <td>0.31746</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.30303</td>\n",
       "      <td>0.28125</td>\n",
       "      <td>0.295082</td>\n",
       "      <td>0.327869</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.31746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_f1</th>\n",
       "      <td>0.385965</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.369231</td>\n",
       "      <td>0.350877</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.31746</td>\n",
       "      <td>0.327869</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.298507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_f1</th>\n",
       "      <td>0.344213</td>\n",
       "      <td>0.335774</td>\n",
       "      <td>0.325861</td>\n",
       "      <td>0.324385</td>\n",
       "      <td>0.317949</td>\n",
       "      <td>0.316364</td>\n",
       "      <td>0.315822</td>\n",
       "      <td>0.310829</td>\n",
       "      <td>0.310727</td>\n",
       "      <td>0.31044</td>\n",
       "      <td>0.310387</td>\n",
       "      <td>0.310377</td>\n",
       "      <td>0.308389</td>\n",
       "      <td>0.307702</td>\n",
       "      <td>0.307603</td>\n",
       "      <td>0.302603</td>\n",
       "      <td>0.302097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_f1</th>\n",
       "      <td>0.032168</td>\n",
       "      <td>0.01383</td>\n",
       "      <td>0.032268</td>\n",
       "      <td>0.02615</td>\n",
       "      <td>0.038376</td>\n",
       "      <td>0.026743</td>\n",
       "      <td>0.040648</td>\n",
       "      <td>0.035715</td>\n",
       "      <td>0.060184</td>\n",
       "      <td>0.005409</td>\n",
       "      <td>0.026225</td>\n",
       "      <td>0.013717</td>\n",
       "      <td>0.013819</td>\n",
       "      <td>0.012638</td>\n",
       "      <td>0.015621</td>\n",
       "      <td>0.021522</td>\n",
       "      <td>0.011366</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_f1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_roc_auc</th>\n",
       "      <td>0.690351</td>\n",
       "      <td>0.663596</td>\n",
       "      <td>0.64693</td>\n",
       "      <td>0.642105</td>\n",
       "      <td>0.678509</td>\n",
       "      <td>0.616228</td>\n",
       "      <td>0.664912</td>\n",
       "      <td>0.677632</td>\n",
       "      <td>0.532018</td>\n",
       "      <td>0.70307</td>\n",
       "      <td>0.607895</td>\n",
       "      <td>0.590351</td>\n",
       "      <td>0.592544</td>\n",
       "      <td>0.592544</td>\n",
       "      <td>0.584211</td>\n",
       "      <td>0.601754</td>\n",
       "      <td>0.615789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.778509</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.689035</td>\n",
       "      <td>0.563596</td>\n",
       "      <td>0.710088</td>\n",
       "      <td>0.573684</td>\n",
       "      <td>0.76886</td>\n",
       "      <td>0.685088</td>\n",
       "      <td>0.72807</td>\n",
       "      <td>0.792544</td>\n",
       "      <td>0.585526</td>\n",
       "      <td>0.63114</td>\n",
       "      <td>0.732456</td>\n",
       "      <td>0.750877</td>\n",
       "      <td>0.795614</td>\n",
       "      <td>0.74386</td>\n",
       "      <td>0.644298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_roc_auc</th>\n",
       "      <td>0.670459</td>\n",
       "      <td>0.639697</td>\n",
       "      <td>0.62284</td>\n",
       "      <td>0.59882</td>\n",
       "      <td>0.600506</td>\n",
       "      <td>0.608091</td>\n",
       "      <td>0.626212</td>\n",
       "      <td>0.648968</td>\n",
       "      <td>0.638854</td>\n",
       "      <td>0.643068</td>\n",
       "      <td>0.610619</td>\n",
       "      <td>0.536452</td>\n",
       "      <td>0.535609</td>\n",
       "      <td>0.554994</td>\n",
       "      <td>0.536873</td>\n",
       "      <td>0.549094</td>\n",
       "      <td>0.565107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.66161</td>\n",
       "      <td>0.714707</td>\n",
       "      <td>0.65866</td>\n",
       "      <td>0.648125</td>\n",
       "      <td>0.637168</td>\n",
       "      <td>0.689001</td>\n",
       "      <td>0.689423</td>\n",
       "      <td>0.659081</td>\n",
       "      <td>0.681837</td>\n",
       "      <td>0.663717</td>\n",
       "      <td>0.581121</td>\n",
       "      <td>0.625369</td>\n",
       "      <td>0.681837</td>\n",
       "      <td>0.644332</td>\n",
       "      <td>0.687737</td>\n",
       "      <td>0.645175</td>\n",
       "      <td>0.683523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_roc_auc</th>\n",
       "      <td>0.703751</td>\n",
       "      <td>0.664138</td>\n",
       "      <td>0.696165</td>\n",
       "      <td>0.608091</td>\n",
       "      <td>0.657396</td>\n",
       "      <td>0.65866</td>\n",
       "      <td>0.683523</td>\n",
       "      <td>0.693637</td>\n",
       "      <td>0.673409</td>\n",
       "      <td>0.674673</td>\n",
       "      <td>0.63759</td>\n",
       "      <td>0.576064</td>\n",
       "      <td>0.564265</td>\n",
       "      <td>0.586599</td>\n",
       "      <td>0.560893</td>\n",
       "      <td>0.5807</td>\n",
       "      <td>0.630426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.739992</td>\n",
       "      <td>0.78129</td>\n",
       "      <td>0.725664</td>\n",
       "      <td>0.668774</td>\n",
       "      <td>0.781711</td>\n",
       "      <td>0.798567</td>\n",
       "      <td>0.741256</td>\n",
       "      <td>0.75137</td>\n",
       "      <td>0.733249</td>\n",
       "      <td>0.800253</td>\n",
       "      <td>0.668352</td>\n",
       "      <td>0.754319</td>\n",
       "      <td>0.784239</td>\n",
       "      <td>0.778761</td>\n",
       "      <td>0.734513</td>\n",
       "      <td>0.782132</td>\n",
       "      <td>0.773283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_roc_auc</th>\n",
       "      <td>0.688187</td>\n",
       "      <td>0.65581</td>\n",
       "      <td>0.655312</td>\n",
       "      <td>0.616339</td>\n",
       "      <td>0.64547</td>\n",
       "      <td>0.62766</td>\n",
       "      <td>0.658216</td>\n",
       "      <td>0.673412</td>\n",
       "      <td>0.61476</td>\n",
       "      <td>0.673604</td>\n",
       "      <td>0.618701</td>\n",
       "      <td>0.567622</td>\n",
       "      <td>0.564139</td>\n",
       "      <td>0.578046</td>\n",
       "      <td>0.560659</td>\n",
       "      <td>0.577183</td>\n",
       "      <td>0.603774</td>\n",
       "      <td>...</td>\n",
       "      <td>0.726703</td>\n",
       "      <td>0.753052</td>\n",
       "      <td>0.69112</td>\n",
       "      <td>0.626832</td>\n",
       "      <td>0.709656</td>\n",
       "      <td>0.687084</td>\n",
       "      <td>0.733179</td>\n",
       "      <td>0.698513</td>\n",
       "      <td>0.714386</td>\n",
       "      <td>0.752171</td>\n",
       "      <td>0.611667</td>\n",
       "      <td>0.670276</td>\n",
       "      <td>0.732844</td>\n",
       "      <td>0.724657</td>\n",
       "      <td>0.739288</td>\n",
       "      <td>0.723722</td>\n",
       "      <td>0.700368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_roc_auc</th>\n",
       "      <td>0.013677</td>\n",
       "      <td>0.011396</td>\n",
       "      <td>0.030516</td>\n",
       "      <td>0.018609</td>\n",
       "      <td>0.032942</td>\n",
       "      <td>0.022171</td>\n",
       "      <td>0.023872</td>\n",
       "      <td>0.018479</td>\n",
       "      <td>0.060185</td>\n",
       "      <td>0.024508</td>\n",
       "      <td>0.013402</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.023244</td>\n",
       "      <td>0.01648</td>\n",
       "      <td>0.019326</td>\n",
       "      <td>0.021642</td>\n",
       "      <td>0.027987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04864</td>\n",
       "      <td>0.028106</td>\n",
       "      <td>0.027394</td>\n",
       "      <td>0.045502</td>\n",
       "      <td>0.05901</td>\n",
       "      <td>0.091818</td>\n",
       "      <td>0.032929</td>\n",
       "      <td>0.038854</td>\n",
       "      <td>0.023112</td>\n",
       "      <td>0.062626</td>\n",
       "      <td>0.040123</td>\n",
       "      <td>0.059474</td>\n",
       "      <td>0.041806</td>\n",
       "      <td>0.057928</td>\n",
       "      <td>0.04417</td>\n",
       "      <td>0.057697</td>\n",
       "      <td>0.053988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_roc_auc</th>\n",
       "      <td>57</td>\n",
       "      <td>75</td>\n",
       "      <td>76</td>\n",
       "      <td>101</td>\n",
       "      <td>84</td>\n",
       "      <td>96</td>\n",
       "      <td>73</td>\n",
       "      <td>65</td>\n",
       "      <td>102</td>\n",
       "      <td>64</td>\n",
       "      <td>100</td>\n",
       "      <td>136</td>\n",
       "      <td>142</td>\n",
       "      <td>121</td>\n",
       "      <td>152</td>\n",
       "      <td>124</td>\n",
       "      <td>105</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>55</td>\n",
       "      <td>97</td>\n",
       "      <td>42</td>\n",
       "      <td>58</td>\n",
       "      <td>26</td>\n",
       "      <td>52</td>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "      <td>103</td>\n",
       "      <td>69</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>34</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_recall_weighted</th>\n",
       "      <td>0.708955</td>\n",
       "      <td>0.723881</td>\n",
       "      <td>0.649254</td>\n",
       "      <td>0.708955</td>\n",
       "      <td>0.649254</td>\n",
       "      <td>0.701493</td>\n",
       "      <td>0.604478</td>\n",
       "      <td>0.723881</td>\n",
       "      <td>0.574627</td>\n",
       "      <td>0.671642</td>\n",
       "      <td>0.716418</td>\n",
       "      <td>0.664179</td>\n",
       "      <td>0.61194</td>\n",
       "      <td>0.634328</td>\n",
       "      <td>0.634328</td>\n",
       "      <td>0.641791</td>\n",
       "      <td>0.671642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_recall_weighted</th>\n",
       "      <td>0.731343</td>\n",
       "      <td>0.679104</td>\n",
       "      <td>0.671642</td>\n",
       "      <td>0.671642</td>\n",
       "      <td>0.664179</td>\n",
       "      <td>0.671642</td>\n",
       "      <td>0.656716</td>\n",
       "      <td>0.679104</td>\n",
       "      <td>0.69403</td>\n",
       "      <td>0.656716</td>\n",
       "      <td>0.656716</td>\n",
       "      <td>0.679104</td>\n",
       "      <td>0.69403</td>\n",
       "      <td>0.61194</td>\n",
       "      <td>0.69403</td>\n",
       "      <td>0.686567</td>\n",
       "      <td>0.679104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_recall_weighted</th>\n",
       "      <td>0.738806</td>\n",
       "      <td>0.708955</td>\n",
       "      <td>0.641791</td>\n",
       "      <td>0.686567</td>\n",
       "      <td>0.69403</td>\n",
       "      <td>0.723881</td>\n",
       "      <td>0.723881</td>\n",
       "      <td>0.708955</td>\n",
       "      <td>0.716418</td>\n",
       "      <td>0.708955</td>\n",
       "      <td>0.69403</td>\n",
       "      <td>0.664179</td>\n",
       "      <td>0.686567</td>\n",
       "      <td>0.679104</td>\n",
       "      <td>0.69403</td>\n",
       "      <td>0.671642</td>\n",
       "      <td>0.649254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_recall_weighted</th>\n",
       "      <td>0.726368</td>\n",
       "      <td>0.70398</td>\n",
       "      <td>0.654229</td>\n",
       "      <td>0.689055</td>\n",
       "      <td>0.669154</td>\n",
       "      <td>0.699005</td>\n",
       "      <td>0.661692</td>\n",
       "      <td>0.70398</td>\n",
       "      <td>0.661692</td>\n",
       "      <td>0.679104</td>\n",
       "      <td>0.689055</td>\n",
       "      <td>0.669154</td>\n",
       "      <td>0.664179</td>\n",
       "      <td>0.641791</td>\n",
       "      <td>0.674129</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.840796</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.845771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_recall_weighted</th>\n",
       "      <td>0.012684</td>\n",
       "      <td>0.018615</td>\n",
       "      <td>0.012684</td>\n",
       "      <td>0.015334</td>\n",
       "      <td>0.018615</td>\n",
       "      <td>0.021399</td>\n",
       "      <td>0.048873</td>\n",
       "      <td>0.018615</td>\n",
       "      <td>0.062239</td>\n",
       "      <td>0.02197</td>\n",
       "      <td>0.024626</td>\n",
       "      <td>0.007036</td>\n",
       "      <td>0.037064</td>\n",
       "      <td>0.027923</td>\n",
       "      <td>0.028144</td>\n",
       "      <td>0.018615</td>\n",
       "      <td>0.012684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_recall_weighted</th>\n",
       "      <td>94</td>\n",
       "      <td>100</td>\n",
       "      <td>133</td>\n",
       "      <td>107</td>\n",
       "      <td>120</td>\n",
       "      <td>104</td>\n",
       "      <td>128</td>\n",
       "      <td>100</td>\n",
       "      <td>130</td>\n",
       "      <td>116</td>\n",
       "      <td>108</td>\n",
       "      <td>120</td>\n",
       "      <td>125</td>\n",
       "      <td>140</td>\n",
       "      <td>118</td>\n",
       "      <td>123</td>\n",
       "      <td>123</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>58</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40 rows × 162 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              133  \\\n",
       "mean_fit_time                                                            0.025344   \n",
       "std_fit_time                                                             0.000894   \n",
       "mean_score_time                                                          0.023527   \n",
       "std_score_time                                                           0.000404   \n",
       "param_clf__C                                                                   10   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__C': 10, 'clf__class_weight': 'balanced'...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                       0.380952   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.468254   \n",
       "std_test_recall                                                          0.062492   \n",
       "rank_test_recall                                                               29   \n",
       "split0_test_precision                                                     0.25641   \n",
       "split1_test_precision                                                    0.258065   \n",
       "split2_test_precision                                                    0.305556   \n",
       "mean_test_precision                                                      0.273343   \n",
       "std_test_precision                                                       0.022787   \n",
       "rank_test_precision                                                            25   \n",
       "split0_test_f1                                                           0.338983   \n",
       "split1_test_f1                                                           0.307692   \n",
       "split2_test_f1                                                           0.385965   \n",
       "mean_test_f1                                                             0.344213   \n",
       "std_test_f1                                                              0.032168   \n",
       "rank_test_f1                                                                    1   \n",
       "split0_test_roc_auc                                                      0.690351   \n",
       "split1_test_roc_auc                                                      0.670459   \n",
       "split2_test_roc_auc                                                      0.703751   \n",
       "mean_test_roc_auc                                                        0.688187   \n",
       "std_test_roc_auc                                                         0.013677   \n",
       "rank_test_roc_auc                                                              57   \n",
       "split0_test_recall_weighted                                              0.708955   \n",
       "split1_test_recall_weighted                                              0.731343   \n",
       "split2_test_recall_weighted                                              0.738806   \n",
       "mean_test_recall_weighted                                                0.726368   \n",
       "std_test_recall_weighted                                                 0.012684   \n",
       "rank_test_recall_weighted                                                      94   \n",
       "\n",
       "                                                                              132  \\\n",
       "mean_fit_time                                                            0.026636   \n",
       "std_fit_time                                                             0.000637   \n",
       "mean_score_time                                                          0.024804   \n",
       "std_score_time                                                           0.001017   \n",
       "param_clf__C                                                                   10   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__C': 10, 'clf__class_weight': 'balanced'...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.484127   \n",
       "std_test_recall                                                          0.011224   \n",
       "rank_test_recall                                                               23   \n",
       "split0_test_precision                                                     0.27027   \n",
       "split1_test_precision                                                    0.238095   \n",
       "split2_test_precision                                                    0.263158   \n",
       "mean_test_precision                                                      0.257174   \n",
       "std_test_precision                                                         0.0138   \n",
       "rank_test_precision                                                            27   \n",
       "split0_test_f1                                                           0.350877   \n",
       "split1_test_f1                                                            0.31746   \n",
       "split2_test_f1                                                           0.338983   \n",
       "mean_test_f1                                                             0.335774   \n",
       "std_test_f1                                                               0.01383   \n",
       "rank_test_f1                                                                    2   \n",
       "split0_test_roc_auc                                                      0.663596   \n",
       "split1_test_roc_auc                                                      0.639697   \n",
       "split2_test_roc_auc                                                      0.664138   \n",
       "mean_test_roc_auc                                                         0.65581   \n",
       "std_test_roc_auc                                                         0.011396   \n",
       "rank_test_roc_auc                                                              75   \n",
       "split0_test_recall_weighted                                              0.723881   \n",
       "split1_test_recall_weighted                                              0.679104   \n",
       "split2_test_recall_weighted                                              0.708955   \n",
       "mean_test_recall_weighted                                                 0.70398   \n",
       "std_test_recall_weighted                                                 0.018615   \n",
       "rank_test_recall_weighted                                                     100   \n",
       "\n",
       "                                                                              134  \\\n",
       "mean_fit_time                                                            0.023242   \n",
       "std_fit_time                                                             0.000271   \n",
       "mean_score_time                                                          0.023794   \n",
       "std_score_time                                                           0.002499   \n",
       "param_clf__C                                                                   10   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__C': 10, 'clf__class_weight': 'balanced'...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                       0.666667   \n",
       "mean_test_recall                                                         0.548413   \n",
       "std_test_recall                                                          0.097208   \n",
       "rank_test_recall                                                                2   \n",
       "split0_test_precision                                                     0.22449   \n",
       "split1_test_precision                                                    0.219512   \n",
       "split2_test_precision                                                    0.254545   \n",
       "mean_test_precision                                                      0.232849   \n",
       "std_test_precision                                                       0.015476   \n",
       "rank_test_precision                                                            42   \n",
       "split0_test_f1                                                           0.318841   \n",
       "split1_test_f1                                                           0.290323   \n",
       "split2_test_f1                                                           0.368421   \n",
       "mean_test_f1                                                             0.325861   \n",
       "std_test_f1                                                              0.032268   \n",
       "rank_test_f1                                                                    3   \n",
       "split0_test_roc_auc                                                       0.64693   \n",
       "split1_test_roc_auc                                                       0.62284   \n",
       "split2_test_roc_auc                                                      0.696165   \n",
       "mean_test_roc_auc                                                        0.655312   \n",
       "std_test_roc_auc                                                         0.030516   \n",
       "rank_test_roc_auc                                                              76   \n",
       "split0_test_recall_weighted                                              0.649254   \n",
       "split1_test_recall_weighted                                              0.671642   \n",
       "split2_test_recall_weighted                                              0.641791   \n",
       "mean_test_recall_weighted                                                0.654229   \n",
       "std_test_recall_weighted                                                 0.012684   \n",
       "rank_test_recall_weighted                                                     133   \n",
       "\n",
       "                                                                              128  \\\n",
       "mean_fit_time                                                            0.022241   \n",
       "std_fit_time                                                             0.001332   \n",
       "mean_score_time                                                          0.017368   \n",
       "std_score_time                                                           0.000685   \n",
       "param_clf__C                                                                   10   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__C': 10, 'clf__class_weight': 'balanced'...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                       0.428571   \n",
       "mean_test_recall                                                         0.484921   \n",
       "std_test_recall                                                          0.049956   \n",
       "rank_test_recall                                                               21   \n",
       "split0_test_precision                                                    0.268293   \n",
       "split1_test_precision                                                    0.232558   \n",
       "split2_test_precision                                                    0.230769   \n",
       "mean_test_precision                                                      0.243873   \n",
       "std_test_precision                                                       0.017283   \n",
       "rank_test_precision                                                            31   \n",
       "split0_test_f1                                                           0.360656   \n",
       "split1_test_f1                                                             0.3125   \n",
       "split2_test_f1                                                                0.3   \n",
       "mean_test_f1                                                             0.324385   \n",
       "std_test_f1                                                               0.02615   \n",
       "rank_test_f1                                                                    4   \n",
       "split0_test_roc_auc                                                      0.642105   \n",
       "split1_test_roc_auc                                                       0.59882   \n",
       "split2_test_roc_auc                                                      0.608091   \n",
       "mean_test_roc_auc                                                        0.616339   \n",
       "std_test_roc_auc                                                         0.018609   \n",
       "rank_test_roc_auc                                                             101   \n",
       "split0_test_recall_weighted                                              0.708955   \n",
       "split1_test_recall_weighted                                              0.671642   \n",
       "split2_test_recall_weighted                                              0.686567   \n",
       "mean_test_recall_weighted                                                0.689055   \n",
       "std_test_recall_weighted                                                 0.015334   \n",
       "rank_test_recall_weighted                                                     107   \n",
       "\n",
       "                                                                              122  \\\n",
       "mean_fit_time                                                            0.020892   \n",
       "std_fit_time                                                              0.00097   \n",
       "mean_score_time                                                          0.020337   \n",
       "std_score_time                                                           0.000494   \n",
       "param_clf__C                                                                   10   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__C': 10, 'clf__class_weight': 'balanced'...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                       0.571429   \n",
       "mean_test_recall                                                         0.499206   \n",
       "std_test_recall                                                          0.052176   \n",
       "rank_test_recall                                                               18   \n",
       "split0_test_precision                                                         0.2   \n",
       "split1_test_precision                                                    0.227273   \n",
       "split2_test_precision                                                    0.272727   \n",
       "mean_test_precision                                                      0.233333   \n",
       "std_test_precision                                                       0.029998   \n",
       "rank_test_precision                                                            39   \n",
       "split0_test_f1                                                           0.276923   \n",
       "split1_test_f1                                                           0.307692   \n",
       "split2_test_f1                                                           0.369231   \n",
       "mean_test_f1                                                             0.317949   \n",
       "std_test_f1                                                              0.038376   \n",
       "rank_test_f1                                                                    5   \n",
       "split0_test_roc_auc                                                      0.678509   \n",
       "split1_test_roc_auc                                                      0.600506   \n",
       "split2_test_roc_auc                                                      0.657396   \n",
       "mean_test_roc_auc                                                         0.64547   \n",
       "std_test_roc_auc                                                         0.032942   \n",
       "rank_test_roc_auc                                                              84   \n",
       "split0_test_recall_weighted                                              0.649254   \n",
       "split1_test_recall_weighted                                              0.664179   \n",
       "split2_test_recall_weighted                                               0.69403   \n",
       "mean_test_recall_weighted                                                0.669154   \n",
       "std_test_recall_weighted                                                 0.018615   \n",
       "rank_test_recall_weighted                                                     120   \n",
       "\n",
       "                                                                              125  \\\n",
       "mean_fit_time                                                            0.025486   \n",
       "std_fit_time                                                             0.001029   \n",
       "mean_score_time                                                           0.02406   \n",
       "std_score_time                                                           0.001164   \n",
       "param_clf__C                                                                   10   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__C': 10, 'clf__class_weight': 'balanced'...   \n",
       "split0_test_recall                                                            0.4   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.450794   \n",
       "std_test_recall                                                          0.035917   \n",
       "rank_test_recall                                                               40   \n",
       "split0_test_precision                                                    0.222222   \n",
       "split1_test_precision                                                    0.232558   \n",
       "split2_test_precision                                                    0.277778   \n",
       "mean_test_precision                                                      0.244186   \n",
       "std_test_precision                                                       0.024125   \n",
       "rank_test_precision                                                            30   \n",
       "split0_test_f1                                                           0.285714   \n",
       "split1_test_f1                                                             0.3125   \n",
       "split2_test_f1                                                           0.350877   \n",
       "mean_test_f1                                                             0.316364   \n",
       "std_test_f1                                                              0.026743   \n",
       "rank_test_f1                                                                    6   \n",
       "split0_test_roc_auc                                                      0.616228   \n",
       "split1_test_roc_auc                                                      0.608091   \n",
       "split2_test_roc_auc                                                       0.65866   \n",
       "mean_test_roc_auc                                                         0.62766   \n",
       "std_test_roc_auc                                                         0.022171   \n",
       "rank_test_roc_auc                                                              96   \n",
       "split0_test_recall_weighted                                              0.701493   \n",
       "split1_test_recall_weighted                                              0.671642   \n",
       "split2_test_recall_weighted                                              0.723881   \n",
       "mean_test_recall_weighted                                                0.699005   \n",
       "std_test_recall_weighted                                                 0.021399   \n",
       "rank_test_recall_weighted                                                     104   \n",
       "\n",
       "                                                                              111  \\\n",
       "mean_fit_time                                                            0.023287   \n",
       "std_fit_time                                                             0.001228   \n",
       "mean_score_time                                                           0.02176   \n",
       "std_score_time                                                           0.000647   \n",
       "param_clf__C                                                                   10   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__C': 10, 'clf__class_weight': 'balanced'...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.500794   \n",
       "std_test_recall                                                          0.052176   \n",
       "rank_test_recall                                                               12   \n",
       "split0_test_precision                                                         0.2   \n",
       "split1_test_precision                                                    0.209302   \n",
       "split2_test_precision                                                    0.289474   \n",
       "mean_test_precision                                                      0.232925   \n",
       "std_test_precision                                                       0.040166   \n",
       "rank_test_precision                                                            41   \n",
       "split0_test_f1                                                           0.293333   \n",
       "split1_test_f1                                                            0.28125   \n",
       "split2_test_f1                                                           0.372881   \n",
       "mean_test_f1                                                             0.315822   \n",
       "std_test_f1                                                              0.040648   \n",
       "rank_test_f1                                                                    7   \n",
       "split0_test_roc_auc                                                      0.664912   \n",
       "split1_test_roc_auc                                                      0.626212   \n",
       "split2_test_roc_auc                                                      0.683523   \n",
       "mean_test_roc_auc                                                        0.658216   \n",
       "std_test_roc_auc                                                         0.023872   \n",
       "rank_test_roc_auc                                                              73   \n",
       "split0_test_recall_weighted                                              0.604478   \n",
       "split1_test_recall_weighted                                              0.656716   \n",
       "split2_test_recall_weighted                                              0.723881   \n",
       "mean_test_recall_weighted                                                0.661692   \n",
       "std_test_recall_weighted                                                 0.048873   \n",
       "rank_test_recall_weighted                                                     128   \n",
       "\n",
       "                                                                              114  \\\n",
       "mean_fit_time                                                             0.02705   \n",
       "std_fit_time                                                             0.001677   \n",
       "mean_score_time                                                          0.025543   \n",
       "std_score_time                                                           0.001178   \n",
       "param_clf__C                                                                   10   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__C': 10, 'clf__class_weight': 'balanced'...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                       0.333333   \n",
       "mean_test_recall                                                         0.436508   \n",
       "std_test_recall                                                            0.0736   \n",
       "rank_test_recall                                                               44   \n",
       "split0_test_precision                                                     0.27027   \n",
       "split1_test_precision                                                    0.238095   \n",
       "split2_test_precision                                                     0.21875   \n",
       "mean_test_precision                                                      0.242372   \n",
       "std_test_precision                                                       0.021249   \n",
       "rank_test_precision                                                            32   \n",
       "split0_test_f1                                                           0.350877   \n",
       "split1_test_f1                                                            0.31746   \n",
       "split2_test_f1                                                           0.264151   \n",
       "mean_test_f1                                                             0.310829   \n",
       "std_test_f1                                                              0.035715   \n",
       "rank_test_f1                                                                    8   \n",
       "split0_test_roc_auc                                                      0.677632   \n",
       "split1_test_roc_auc                                                      0.648968   \n",
       "split2_test_roc_auc                                                      0.693637   \n",
       "mean_test_roc_auc                                                        0.673412   \n",
       "std_test_roc_auc                                                         0.018479   \n",
       "rank_test_roc_auc                                                              65   \n",
       "split0_test_recall_weighted                                              0.723881   \n",
       "split1_test_recall_weighted                                              0.679104   \n",
       "split2_test_recall_weighted                                              0.708955   \n",
       "mean_test_recall_weighted                                                 0.70398   \n",
       "std_test_recall_weighted                                                 0.018615   \n",
       "rank_test_recall_weighted                                                     100   \n",
       "\n",
       "                                                                              129  \\\n",
       "mean_fit_time                                                            0.023229   \n",
       "std_fit_time                                                             0.000946   \n",
       "mean_score_time                                                          0.023244   \n",
       "std_score_time                                                           0.001549   \n",
       "param_clf__C                                                                   10   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__C': 10, 'clf__class_weight': 'balanced'...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                       0.571429   \n",
       "mean_test_recall                                                         0.483333   \n",
       "std_test_recall                                                          0.062904   \n",
       "rank_test_recall                                                               26   \n",
       "split0_test_precision                                                    0.163636   \n",
       "split1_test_precision                                                    0.236842   \n",
       "split2_test_precision                                                    0.292683   \n",
       "mean_test_precision                                                      0.231054   \n",
       "std_test_precision                                                       0.052842   \n",
       "rank_test_precision                                                            45   \n",
       "split0_test_f1                                                               0.24   \n",
       "split1_test_f1                                                           0.305085   \n",
       "split2_test_f1                                                           0.387097   \n",
       "mean_test_f1                                                             0.310727   \n",
       "std_test_f1                                                              0.060184   \n",
       "rank_test_f1                                                                    9   \n",
       "split0_test_roc_auc                                                      0.532018   \n",
       "split1_test_roc_auc                                                      0.638854   \n",
       "split2_test_roc_auc                                                      0.673409   \n",
       "mean_test_roc_auc                                                         0.61476   \n",
       "std_test_roc_auc                                                         0.060185   \n",
       "rank_test_roc_auc                                                             102   \n",
       "split0_test_recall_weighted                                              0.574627   \n",
       "split1_test_recall_weighted                                               0.69403   \n",
       "split2_test_recall_weighted                                              0.716418   \n",
       "mean_test_recall_weighted                                                0.661692   \n",
       "std_test_recall_weighted                                                 0.062239   \n",
       "rank_test_recall_weighted                                                     130   \n",
       "\n",
       "                                                                              123  \\\n",
       "mean_fit_time                                                            0.025859   \n",
       "std_fit_time                                                             0.000735   \n",
       "mean_score_time                                                          0.025022   \n",
       "std_score_time                                                           0.001038   \n",
       "param_clf__C                                                                   10   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__C': 10, 'clf__class_weight': 'balanced'...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                       0.428571   \n",
       "mean_test_recall                                                         0.468254   \n",
       "std_test_recall                                                          0.029696   \n",
       "rank_test_recall                                                               29   \n",
       "split0_test_precision                                                    0.227273   \n",
       "split1_test_precision                                                    0.222222   \n",
       "split2_test_precision                                                        0.25   \n",
       "mean_test_precision                                                      0.233165   \n",
       "std_test_precision                                                       0.012081   \n",
       "rank_test_precision                                                            40   \n",
       "split0_test_f1                                                             0.3125   \n",
       "split1_test_f1                                                            0.30303   \n",
       "split2_test_f1                                                           0.315789   \n",
       "mean_test_f1                                                              0.31044   \n",
       "std_test_f1                                                              0.005409   \n",
       "rank_test_f1                                                                   10   \n",
       "split0_test_roc_auc                                                       0.70307   \n",
       "split1_test_roc_auc                                                      0.643068   \n",
       "split2_test_roc_auc                                                      0.674673   \n",
       "mean_test_roc_auc                                                        0.673604   \n",
       "std_test_roc_auc                                                         0.024508   \n",
       "rank_test_roc_auc                                                              64   \n",
       "split0_test_recall_weighted                                              0.671642   \n",
       "split1_test_recall_weighted                                              0.656716   \n",
       "split2_test_recall_weighted                                              0.708955   \n",
       "mean_test_recall_weighted                                                0.679104   \n",
       "std_test_recall_weighted                                                  0.02197   \n",
       "rank_test_recall_weighted                                                     116   \n",
       "\n",
       "                                                                              117  \\\n",
       "mean_fit_time                                                            0.022815   \n",
       "std_fit_time                                                             0.001761   \n",
       "mean_score_time                                                          0.018517   \n",
       "std_score_time                                                           0.000259   \n",
       "param_clf__C                                                                   10   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__C': 10, 'clf__class_weight': 'balanced'...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                       0.428571   \n",
       "mean_test_recall                                                         0.452381   \n",
       "std_test_recall                                                          0.033672   \n",
       "rank_test_recall                                                               39   \n",
       "split0_test_precision                                                    0.263158   \n",
       "split1_test_precision                                                    0.209302   \n",
       "split2_test_precision                                                    0.236842   \n",
       "mean_test_precision                                                      0.236434   \n",
       "std_test_precision                                                       0.021988   \n",
       "rank_test_precision                                                            36   \n",
       "split0_test_f1                                                           0.344828   \n",
       "split1_test_f1                                                            0.28125   \n",
       "split2_test_f1                                                           0.305085   \n",
       "mean_test_f1                                                             0.310387   \n",
       "std_test_f1                                                              0.026225   \n",
       "rank_test_f1                                                                   11   \n",
       "split0_test_roc_auc                                                      0.607895   \n",
       "split1_test_roc_auc                                                      0.610619   \n",
       "split2_test_roc_auc                                                       0.63759   \n",
       "mean_test_roc_auc                                                        0.618701   \n",
       "std_test_roc_auc                                                         0.013402   \n",
       "rank_test_roc_auc                                                             100   \n",
       "split0_test_recall_weighted                                              0.716418   \n",
       "split1_test_recall_weighted                                              0.656716   \n",
       "split2_test_recall_weighted                                               0.69403   \n",
       "mean_test_recall_weighted                                                0.689055   \n",
       "std_test_recall_weighted                                                 0.024626   \n",
       "rank_test_recall_weighted                                                     108   \n",
       "\n",
       "                                                                              18   \\\n",
       "mean_fit_time                                                            0.020464   \n",
       "std_fit_time                                                             0.000962   \n",
       "mean_score_time                                                          0.017126   \n",
       "std_score_time                                                           0.000205   \n",
       "param_clf__C                                                                  0.1   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__C': 0.1, 'clf__class_weight': 'balanced...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.484921   \n",
       "std_test_recall                                                          0.049956   \n",
       "rank_test_recall                                                               19   \n",
       "split0_test_precision                                                    0.234043   \n",
       "split1_test_precision                                                       0.225   \n",
       "split2_test_precision                                                    0.227273   \n",
       "mean_test_precision                                                      0.228772   \n",
       "std_test_precision                                                       0.003841   \n",
       "rank_test_precision                                                            47   \n",
       "split0_test_f1                                                           0.328358   \n",
       "split1_test_f1                                                           0.295082   \n",
       "split2_test_f1                                                           0.307692   \n",
       "mean_test_f1                                                             0.310377   \n",
       "std_test_f1                                                              0.013717   \n",
       "rank_test_f1                                                                   12   \n",
       "split0_test_roc_auc                                                      0.590351   \n",
       "split1_test_roc_auc                                                      0.536452   \n",
       "split2_test_roc_auc                                                      0.576064   \n",
       "mean_test_roc_auc                                                        0.567622   \n",
       "std_test_roc_auc                                                           0.0228   \n",
       "rank_test_roc_auc                                                             136   \n",
       "split0_test_recall_weighted                                              0.664179   \n",
       "split1_test_recall_weighted                                              0.679104   \n",
       "split2_test_recall_weighted                                              0.664179   \n",
       "mean_test_recall_weighted                                                0.669154   \n",
       "std_test_recall_weighted                                                 0.007036   \n",
       "rank_test_recall_weighted                                                     120   \n",
       "\n",
       "                                                                              24   \\\n",
       "mean_fit_time                                                            0.027063   \n",
       "std_fit_time                                                             0.001687   \n",
       "mean_score_time                                                           0.02633   \n",
       "std_score_time                                                           0.000437   \n",
       "param_clf__C                                                                  0.1   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__C': 0.1, 'clf__class_weight': 'balanced...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                       0.428571   \n",
       "mean_test_recall                                                         0.484921   \n",
       "std_test_recall                                                          0.049956   \n",
       "rank_test_recall                                                               21   \n",
       "split0_test_precision                                                    0.203704   \n",
       "split1_test_precision                                                        0.25   \n",
       "split2_test_precision                                                    0.230769   \n",
       "mean_test_precision                                                      0.228158   \n",
       "std_test_precision                                                        0.01899   \n",
       "rank_test_precision                                                            48   \n",
       "split0_test_f1                                                           0.297297   \n",
       "split1_test_f1                                                           0.327869   \n",
       "split2_test_f1                                                                0.3   \n",
       "mean_test_f1                                                             0.308389   \n",
       "std_test_f1                                                              0.013819   \n",
       "rank_test_f1                                                                   13   \n",
       "split0_test_roc_auc                                                      0.592544   \n",
       "split1_test_roc_auc                                                      0.535609   \n",
       "split2_test_roc_auc                                                      0.564265   \n",
       "mean_test_roc_auc                                                        0.564139   \n",
       "std_test_roc_auc                                                         0.023244   \n",
       "rank_test_roc_auc                                                             142   \n",
       "split0_test_recall_weighted                                               0.61194   \n",
       "split1_test_recall_weighted                                               0.69403   \n",
       "split2_test_recall_weighted                                              0.686567   \n",
       "mean_test_recall_weighted                                                0.664179   \n",
       "std_test_recall_weighted                                                 0.037064   \n",
       "rank_test_recall_weighted                                                     125   \n",
       "\n",
       "                                                                              67   \\\n",
       "mean_fit_time                                                            0.022156   \n",
       "std_fit_time                                                             0.000419   \n",
       "mean_score_time                                                          0.020831   \n",
       "std_score_time                                                           0.000671   \n",
       "param_clf__C                                                                    1   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__C': 1, 'clf__class_weight': 'balanced',...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                       0.571429   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.515873   \n",
       "std_test_recall                                                          0.040468   \n",
       "rank_test_recall                                                                8   \n",
       "split0_test_precision                                                    0.204082   \n",
       "split1_test_precision                                                    0.218182   \n",
       "split2_test_precision                                                    0.238095   \n",
       "mean_test_precision                                                       0.22012   \n",
       "std_test_precision                                                       0.013953   \n",
       "rank_test_precision                                                            58   \n",
       "split0_test_f1                                                           0.289855   \n",
       "split1_test_f1                                                           0.315789   \n",
       "split2_test_f1                                                            0.31746   \n",
       "mean_test_f1                                                             0.307702   \n",
       "std_test_f1                                                              0.012638   \n",
       "rank_test_f1                                                                   14   \n",
       "split0_test_roc_auc                                                      0.592544   \n",
       "split1_test_roc_auc                                                      0.554994   \n",
       "split2_test_roc_auc                                                      0.586599   \n",
       "mean_test_roc_auc                                                        0.578046   \n",
       "std_test_roc_auc                                                          0.01648   \n",
       "rank_test_roc_auc                                                             121   \n",
       "split0_test_recall_weighted                                              0.634328   \n",
       "split1_test_recall_weighted                                               0.61194   \n",
       "split2_test_recall_weighted                                              0.679104   \n",
       "mean_test_recall_weighted                                                0.641791   \n",
       "std_test_recall_weighted                                                 0.027923   \n",
       "rank_test_recall_weighted                                                     140   \n",
       "\n",
       "                                                                              21   \\\n",
       "mean_fit_time                                                            0.022236   \n",
       "std_fit_time                                                             0.000371   \n",
       "mean_score_time                                                          0.024156   \n",
       "std_score_time                                                           0.001527   \n",
       "param_clf__C                                                                  0.1   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__C': 0.1, 'clf__class_weight': 'balanced...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.468254   \n",
       "std_test_recall                                                          0.029696   \n",
       "rank_test_recall                                                               29   \n",
       "split0_test_precision                                                    0.204082   \n",
       "split1_test_precision                                                    0.236842   \n",
       "split2_test_precision                                                        0.25   \n",
       "mean_test_precision                                                      0.230308   \n",
       "std_test_precision                                                       0.019307   \n",
       "rank_test_precision                                                            46   \n",
       "split0_test_f1                                                           0.289855   \n",
       "split1_test_f1                                                           0.305085   \n",
       "split2_test_f1                                                           0.327869   \n",
       "mean_test_f1                                                             0.307603   \n",
       "std_test_f1                                                              0.015621   \n",
       "rank_test_f1                                                                   15   \n",
       "split0_test_roc_auc                                                      0.584211   \n",
       "split1_test_roc_auc                                                      0.536873   \n",
       "split2_test_roc_auc                                                      0.560893   \n",
       "mean_test_roc_auc                                                        0.560659   \n",
       "std_test_roc_auc                                                         0.019326   \n",
       "rank_test_roc_auc                                                             152   \n",
       "split0_test_recall_weighted                                              0.634328   \n",
       "split1_test_recall_weighted                                               0.69403   \n",
       "split2_test_recall_weighted                                               0.69403   \n",
       "mean_test_recall_weighted                                                0.674129   \n",
       "std_test_recall_weighted                                                 0.028144   \n",
       "rank_test_recall_weighted                                                     118   \n",
       "\n",
       "                                                                              54   \\\n",
       "mean_fit_time                                                            0.025043   \n",
       "std_fit_time                                                              0.00554   \n",
       "mean_score_time                                                          0.023716   \n",
       "std_score_time                                                           0.004026   \n",
       "param_clf__C                                                                    1   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__C': 1, 'clf__class_weight': 'balanced',...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                          0.46746   \n",
       "std_test_recall                                                          0.012346   \n",
       "rank_test_recall                                                               33   \n",
       "split0_test_precision                                                    0.195652   \n",
       "split1_test_precision                                                    0.243902   \n",
       "split2_test_precision                                                    0.232558   \n",
       "mean_test_precision                                                      0.224038   \n",
       "std_test_precision                                                       0.020599   \n",
       "rank_test_precision                                                            52   \n",
       "split0_test_f1                                                           0.272727   \n",
       "split1_test_f1                                                           0.322581   \n",
       "split2_test_f1                                                             0.3125   \n",
       "mean_test_f1                                                             0.302603   \n",
       "std_test_f1                                                              0.021522   \n",
       "rank_test_f1                                                                   16   \n",
       "split0_test_roc_auc                                                      0.601754   \n",
       "split1_test_roc_auc                                                      0.549094   \n",
       "split2_test_roc_auc                                                        0.5807   \n",
       "mean_test_roc_auc                                                        0.577183   \n",
       "std_test_roc_auc                                                         0.021642   \n",
       "rank_test_roc_auc                                                             124   \n",
       "split0_test_recall_weighted                                              0.641791   \n",
       "split1_test_recall_weighted                                              0.686567   \n",
       "split2_test_recall_weighted                                              0.671642   \n",
       "mean_test_recall_weighted                                                0.666667   \n",
       "std_test_recall_weighted                                                 0.018615   \n",
       "rank_test_recall_weighted                                                     123   \n",
       "\n",
       "                                                                              127  \\\n",
       "mean_fit_time                                                            0.021825   \n",
       "std_fit_time                                                             0.000961   \n",
       "mean_score_time                                                          0.018995   \n",
       "std_score_time                                                           0.001001   \n",
       "param_clf__C                                                                   10   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__C': 10, 'clf__class_weight': 'balanced'...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                          0.46746   \n",
       "std_test_recall                                                          0.012346   \n",
       "rank_test_recall                                                               33   \n",
       "split0_test_precision                                                    0.214286   \n",
       "split1_test_precision                                                    0.238095   \n",
       "split2_test_precision                                                    0.217391   \n",
       "mean_test_precision                                                      0.223257   \n",
       "std_test_precision                                                       0.010568   \n",
       "rank_test_precision                                                            54   \n",
       "split0_test_f1                                                           0.290323   \n",
       "split1_test_f1                                                            0.31746   \n",
       "split2_test_f1                                                           0.298507   \n",
       "mean_test_f1                                                             0.302097   \n",
       "std_test_f1                                                              0.011366   \n",
       "rank_test_f1                                                                   17   \n",
       "split0_test_roc_auc                                                      0.615789   \n",
       "split1_test_roc_auc                                                      0.565107   \n",
       "split2_test_roc_auc                                                      0.630426   \n",
       "mean_test_roc_auc                                                        0.603774   \n",
       "std_test_roc_auc                                                         0.027987   \n",
       "rank_test_roc_auc                                                             105   \n",
       "split0_test_recall_weighted                                              0.671642   \n",
       "split1_test_recall_weighted                                              0.679104   \n",
       "split2_test_recall_weighted                                              0.649254   \n",
       "mean_test_recall_weighted                                                0.666667   \n",
       "std_test_recall_weighted                                                 0.012684   \n",
       "rank_test_recall_weighted                                                     123   \n",
       "\n",
       "                                ...  \\\n",
       "mean_fit_time                   ...   \n",
       "std_fit_time                    ...   \n",
       "mean_score_time                 ...   \n",
       "std_score_time                  ...   \n",
       "param_clf__C                    ...   \n",
       "param_clf__class_weight         ...   \n",
       "param_smt__k_neighbors          ...   \n",
       "param_smt__sampling_strategy    ...   \n",
       "param_under__sampling_strategy  ...   \n",
       "params                          ...   \n",
       "split0_test_recall              ...   \n",
       "split1_test_recall              ...   \n",
       "split2_test_recall              ...   \n",
       "mean_test_recall                ...   \n",
       "std_test_recall                 ...   \n",
       "rank_test_recall                ...   \n",
       "split0_test_precision           ...   \n",
       "split1_test_precision           ...   \n",
       "split2_test_precision           ...   \n",
       "mean_test_precision             ...   \n",
       "std_test_precision              ...   \n",
       "rank_test_precision             ...   \n",
       "split0_test_f1                  ...   \n",
       "split1_test_f1                  ...   \n",
       "split2_test_f1                  ...   \n",
       "mean_test_f1                    ...   \n",
       "std_test_f1                     ...   \n",
       "rank_test_f1                    ...   \n",
       "split0_test_roc_auc             ...   \n",
       "split1_test_roc_auc             ...   \n",
       "split2_test_roc_auc             ...   \n",
       "mean_test_roc_auc               ...   \n",
       "std_test_roc_auc                ...   \n",
       "rank_test_roc_auc               ...   \n",
       "split0_test_recall_weighted     ...   \n",
       "split1_test_recall_weighted     ...   \n",
       "split2_test_recall_weighted     ...   \n",
       "mean_test_recall_weighted       ...   \n",
       "std_test_recall_weighted        ...   \n",
       "rank_test_recall_weighted       ...   \n",
       "\n",
       "                                                                              42   \\\n",
       "mean_fit_time                                                            0.026041   \n",
       "std_fit_time                                                             0.002695   \n",
       "mean_score_time                                                          0.022005   \n",
       "std_score_time                                                           0.000371   \n",
       "param_clf__C                                                                  0.1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__C': 0.1, 'clf__class_weight': None, 'sm...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              115   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           115   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  115   \n",
       "split0_test_roc_auc                                                      0.778509   \n",
       "split1_test_roc_auc                                                       0.66161   \n",
       "split2_test_roc_auc                                                      0.739992   \n",
       "mean_test_roc_auc                                                        0.726703   \n",
       "std_test_roc_auc                                                          0.04864   \n",
       "rank_test_roc_auc                                                              30   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       8   \n",
       "\n",
       "                                                                              30   \\\n",
       "mean_fit_time                                                            0.021706   \n",
       "std_fit_time                                                             0.000262   \n",
       "mean_score_time                                                          0.019321   \n",
       "std_score_time                                                           0.001122   \n",
       "param_clf__C                                                                  0.1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__C': 0.1, 'clf__class_weight': None, 'sm...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              115   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           115   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  115   \n",
       "split0_test_roc_auc                                                      0.763158   \n",
       "split1_test_roc_auc                                                      0.714707   \n",
       "split2_test_roc_auc                                                       0.78129   \n",
       "mean_test_roc_auc                                                        0.753052   \n",
       "std_test_roc_auc                                                         0.028106   \n",
       "rank_test_roc_auc                                                               7   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       8   \n",
       "\n",
       "                                                                              31   \\\n",
       "mean_fit_time                                                            0.022629   \n",
       "std_fit_time                                                             0.000652   \n",
       "mean_score_time                                                          0.015919   \n",
       "std_score_time                                                           0.003108   \n",
       "param_clf__C                                                                  0.1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__C': 0.1, 'clf__class_weight': None, 'sm...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              115   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           115   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  115   \n",
       "split0_test_roc_auc                                                      0.689035   \n",
       "split1_test_roc_auc                                                       0.65866   \n",
       "split2_test_roc_auc                                                      0.725664   \n",
       "mean_test_roc_auc                                                         0.69112   \n",
       "std_test_roc_auc                                                         0.027394   \n",
       "rank_test_roc_auc                                                              55   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       8   \n",
       "\n",
       "                                                                              32   \\\n",
       "mean_fit_time                                                            0.015893   \n",
       "std_fit_time                                                               0.0006   \n",
       "mean_score_time                                                          0.013993   \n",
       "std_score_time                                                            0.00329   \n",
       "param_clf__C                                                                  0.1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__C': 0.1, 'clf__class_weight': None, 'sm...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              115   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           115   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  115   \n",
       "split0_test_roc_auc                                                      0.563596   \n",
       "split1_test_roc_auc                                                      0.648125   \n",
       "split2_test_roc_auc                                                      0.668774   \n",
       "mean_test_roc_auc                                                        0.626832   \n",
       "std_test_roc_auc                                                         0.045502   \n",
       "rank_test_roc_auc                                                              97   \n",
       "split0_test_recall_weighted                                              0.835821   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.840796   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                      58   \n",
       "\n",
       "                                                                              33   \\\n",
       "mean_fit_time                                                            0.018674   \n",
       "std_fit_time                                                             0.003505   \n",
       "mean_score_time                                                          0.022419   \n",
       "std_score_time                                                            0.00617   \n",
       "param_clf__C                                                                  0.1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__C': 0.1, 'clf__class_weight': None, 'sm...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              115   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           115   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  115   \n",
       "split0_test_roc_auc                                                      0.710088   \n",
       "split1_test_roc_auc                                                      0.637168   \n",
       "split2_test_roc_auc                                                      0.781711   \n",
       "mean_test_roc_auc                                                        0.709656   \n",
       "std_test_roc_auc                                                          0.05901   \n",
       "rank_test_roc_auc                                                              42   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       8   \n",
       "\n",
       "                                                                              46   \\\n",
       "mean_fit_time                                                             0.01967   \n",
       "std_fit_time                                                             0.000126   \n",
       "mean_score_time                                                          0.017481   \n",
       "std_score_time                                                           0.000696   \n",
       "param_clf__C                                                                  0.1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__C': 0.1, 'clf__class_weight': None, 'sm...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              115   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           115   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  115   \n",
       "split0_test_roc_auc                                                      0.573684   \n",
       "split1_test_roc_auc                                                      0.689001   \n",
       "split2_test_roc_auc                                                      0.798567   \n",
       "mean_test_roc_auc                                                        0.687084   \n",
       "std_test_roc_auc                                                         0.091818   \n",
       "rank_test_roc_auc                                                              58   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       8   \n",
       "\n",
       "                                                                              53   \\\n",
       "mean_fit_time                                                            0.023284   \n",
       "std_fit_time                                                             0.000486   \n",
       "mean_score_time                                                          0.022354   \n",
       "std_score_time                                                           0.000959   \n",
       "param_clf__C                                                                  0.1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__C': 0.1, 'clf__class_weight': None, 'sm...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              115   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           115   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  115   \n",
       "split0_test_roc_auc                                                       0.76886   \n",
       "split1_test_roc_auc                                                      0.689423   \n",
       "split2_test_roc_auc                                                      0.741256   \n",
       "mean_test_roc_auc                                                        0.733179   \n",
       "std_test_roc_auc                                                         0.032929   \n",
       "rank_test_roc_auc                                                              26   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       8   \n",
       "\n",
       "                                                                              34   \\\n",
       "mean_fit_time                                                            0.023406   \n",
       "std_fit_time                                                             0.005904   \n",
       "mean_score_time                                                          0.022058   \n",
       "std_score_time                                                            0.00148   \n",
       "param_clf__C                                                                  0.1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__C': 0.1, 'clf__class_weight': None, 'sm...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              115   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           115   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  115   \n",
       "split0_test_roc_auc                                                      0.685088   \n",
       "split1_test_roc_auc                                                      0.659081   \n",
       "split2_test_roc_auc                                                       0.75137   \n",
       "mean_test_roc_auc                                                        0.698513   \n",
       "std_test_roc_auc                                                         0.038854   \n",
       "rank_test_roc_auc                                                              52   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       8   \n",
       "\n",
       "                                                                              106  \\\n",
       "mean_fit_time                                                            0.025392   \n",
       "std_fit_time                                                             0.002642   \n",
       "mean_score_time                                                           0.02618   \n",
       "std_score_time                                                           0.003024   \n",
       "param_clf__C                                                                    1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__C': 1, 'clf__class_weight': None, 'smt_...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              115   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           115   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  115   \n",
       "split0_test_roc_auc                                                       0.72807   \n",
       "split1_test_roc_auc                                                      0.681837   \n",
       "split2_test_roc_auc                                                      0.733249   \n",
       "mean_test_roc_auc                                                        0.714386   \n",
       "std_test_roc_auc                                                         0.023112   \n",
       "rank_test_roc_auc                                                              39   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       8   \n",
       "\n",
       "                                                                              105  \\\n",
       "mean_fit_time                                                            0.025357   \n",
       "std_fit_time                                                             0.002358   \n",
       "mean_score_time                                                          0.022059   \n",
       "std_score_time                                                           0.000586   \n",
       "param_clf__C                                                                    1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__C': 1, 'clf__class_weight': None, 'smt_...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              115   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           115   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  115   \n",
       "split0_test_roc_auc                                                      0.792544   \n",
       "split1_test_roc_auc                                                      0.663717   \n",
       "split2_test_roc_auc                                                      0.800253   \n",
       "mean_test_roc_auc                                                        0.752171   \n",
       "std_test_roc_auc                                                         0.062626   \n",
       "rank_test_roc_auc                                                               8   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       8   \n",
       "\n",
       "                                                                              104  \\\n",
       "mean_fit_time                                                             0.02278   \n",
       "std_fit_time                                                             0.002658   \n",
       "mean_score_time                                                          0.019854   \n",
       "std_score_time                                                           0.000199   \n",
       "param_clf__C                                                                    1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__C': 1, 'clf__class_weight': None, 'smt_...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              115   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           115   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  115   \n",
       "split0_test_roc_auc                                                      0.585526   \n",
       "split1_test_roc_auc                                                      0.581121   \n",
       "split2_test_roc_auc                                                      0.668352   \n",
       "mean_test_roc_auc                                                        0.611667   \n",
       "std_test_roc_auc                                                         0.040123   \n",
       "rank_test_roc_auc                                                             103   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       8   \n",
       "\n",
       "                                                                              103  \\\n",
       "mean_fit_time                                                            0.021172   \n",
       "std_fit_time                                                             0.000243   \n",
       "mean_score_time                                                          0.021078   \n",
       "std_score_time                                                           0.001557   \n",
       "param_clf__C                                                                    1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__C': 1, 'clf__class_weight': None, 'smt_...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              115   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           115   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  115   \n",
       "split0_test_roc_auc                                                       0.63114   \n",
       "split1_test_roc_auc                                                      0.625369   \n",
       "split2_test_roc_auc                                                      0.754319   \n",
       "mean_test_roc_auc                                                        0.670276   \n",
       "std_test_roc_auc                                                         0.059474   \n",
       "rank_test_roc_auc                                                              69   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       8   \n",
       "\n",
       "                                                                              102  \\\n",
       "mean_fit_time                                                            0.023036   \n",
       "std_fit_time                                                             0.001273   \n",
       "mean_score_time                                                          0.019764   \n",
       "std_score_time                                                           0.000223   \n",
       "param_clf__C                                                                    1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__C': 1, 'clf__class_weight': None, 'smt_...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              115   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           115   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  115   \n",
       "split0_test_roc_auc                                                      0.732456   \n",
       "split1_test_roc_auc                                                      0.681837   \n",
       "split2_test_roc_auc                                                      0.784239   \n",
       "mean_test_roc_auc                                                        0.732844   \n",
       "std_test_roc_auc                                                         0.041806   \n",
       "rank_test_roc_auc                                                              27   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       8   \n",
       "\n",
       "                                                                              100  \\\n",
       "mean_fit_time                                                            0.019765   \n",
       "std_fit_time                                                             0.000056   \n",
       "mean_score_time                                                           0.01739   \n",
       "std_score_time                                                           0.000123   \n",
       "param_clf__C                                                                    1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__C': 1, 'clf__class_weight': None, 'smt_...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              115   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           115   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  115   \n",
       "split0_test_roc_auc                                                      0.750877   \n",
       "split1_test_roc_auc                                                      0.644332   \n",
       "split2_test_roc_auc                                                      0.778761   \n",
       "mean_test_roc_auc                                                        0.724657   \n",
       "std_test_roc_auc                                                         0.057928   \n",
       "rank_test_roc_auc                                                              32   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       8   \n",
       "\n",
       "                                                                              99   \\\n",
       "mean_fit_time                                                            0.020109   \n",
       "std_fit_time                                                             0.001024   \n",
       "mean_score_time                                                          0.018541   \n",
       "std_score_time                                                           0.000946   \n",
       "param_clf__C                                                                    1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__C': 1, 'clf__class_weight': None, 'smt_...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              115   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           115   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  115   \n",
       "split0_test_roc_auc                                                      0.795614   \n",
       "split1_test_roc_auc                                                      0.687737   \n",
       "split2_test_roc_auc                                                      0.734513   \n",
       "mean_test_roc_auc                                                        0.739288   \n",
       "std_test_roc_auc                                                          0.04417   \n",
       "rank_test_roc_auc                                                              20   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       8   \n",
       "\n",
       "                                                                              48   \\\n",
       "mean_fit_time                                                            0.021999   \n",
       "std_fit_time                                                             0.000328   \n",
       "mean_score_time                                                          0.020493   \n",
       "std_score_time                                                           0.000504   \n",
       "param_clf__C                                                                  0.1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__C': 0.1, 'clf__class_weight': None, 'sm...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              115   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           115   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  115   \n",
       "split0_test_roc_auc                                                       0.74386   \n",
       "split1_test_roc_auc                                                      0.645175   \n",
       "split2_test_roc_auc                                                      0.782132   \n",
       "mean_test_roc_auc                                                        0.723722   \n",
       "std_test_roc_auc                                                         0.057697   \n",
       "rank_test_roc_auc                                                              34   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       8   \n",
       "\n",
       "                                                                              52   \n",
       "mean_fit_time                                                             0.02433  \n",
       "std_fit_time                                                               0.0013  \n",
       "mean_score_time                                                          0.021843  \n",
       "std_score_time                                                           0.001282  \n",
       "param_clf__C                                                                  0.1  \n",
       "param_clf__class_weight                                                      None  \n",
       "param_smt__k_neighbors                                                          5  \n",
       "param_smt__sampling_strategy                                                  0.4  \n",
       "param_under__sampling_strategy                                                0.6  \n",
       "params                          {'clf__C': 0.1, 'clf__class_weight': None, 'sm...  \n",
       "split0_test_recall                                                            0.0  \n",
       "split1_test_recall                                                            0.0  \n",
       "split2_test_recall                                                            0.0  \n",
       "mean_test_recall                                                              0.0  \n",
       "std_test_recall                                                               0.0  \n",
       "rank_test_recall                                                              115  \n",
       "split0_test_precision                                                         0.0  \n",
       "split1_test_precision                                                         0.0  \n",
       "split2_test_precision                                                         0.0  \n",
       "mean_test_precision                                                           0.0  \n",
       "std_test_precision                                                            0.0  \n",
       "rank_test_precision                                                           115  \n",
       "split0_test_f1                                                                0.0  \n",
       "split1_test_f1                                                                0.0  \n",
       "split2_test_f1                                                                0.0  \n",
       "mean_test_f1                                                                  0.0  \n",
       "std_test_f1                                                                   0.0  \n",
       "rank_test_f1                                                                  115  \n",
       "split0_test_roc_auc                                                      0.644298  \n",
       "split1_test_roc_auc                                                      0.683523  \n",
       "split2_test_roc_auc                                                      0.773283  \n",
       "mean_test_roc_auc                                                        0.700368  \n",
       "std_test_roc_auc                                                         0.053988  \n",
       "rank_test_roc_auc                                                              49  \n",
       "split0_test_recall_weighted                                              0.850746  \n",
       "split1_test_recall_weighted                                              0.843284  \n",
       "split2_test_recall_weighted                                              0.843284  \n",
       "mean_test_recall_weighted                                                0.845771  \n",
       "std_test_recall_weighted                                                 0.003518  \n",
       "rank_test_recall_weighted                                                       8  \n",
       "\n",
       "[40 rows x 162 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "pipeline = IMBLpipeline([\n",
    "    ('smt', SMOTE()), \n",
    "    ('under', RandomUnderSampler()), \n",
    "    ('clf', SVC())\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "            'clf__C': [0.1, 1, 10],  \n",
    "            # 'clf__kernel': ['linear', 'rbf', 'poly'],\n",
    "            'clf__class_weight': ['balanced', None],\n",
    "            'smt__sampling_strategy': [ 0.2, 0.3, 0.4],\n",
    "            'under__sampling_strategy': [0.5, 0.6, 0.7],\n",
    "            'smt__k_neighbors': [1, 3, 5]\n",
    "             }\n",
    "nfolds = 3\n",
    "scores = ['recall', 'precision', 'f1', 'roc_auc', 'recall_weighted']\n",
    "gscv_svm = GridSearchCV(pipeline, \n",
    "                    parameters, \n",
    "                    scoring=scores,\n",
    "                    cv=StratifiedKFold(n_splits=nfolds, shuffle=True),\n",
    "                    return_train_score=False, \n",
    "                    verbose=1, \n",
    "                    refit=\"f1\",\n",
    "                    n_jobs=-1)\n",
    "resultsGSCV = gscv_svm.fit(X, y)\n",
    "\n",
    "# Get the classifier name from the pipeline\n",
    "classifier_name = resultsGSCV.best_estimator_.named_steps['clf'].__class__.__name__\n",
    "    \n",
    "# save the best estimator\n",
    "save_best_estimator(gscv_svm, classifier_name, notebook_name)\n",
    "\n",
    "results = pd.DataFrame(resultsGSCV.cv_results_)\n",
    "display(results.sort_values(by=[f'rank_test_f1']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecd040b2-10bc-4183-a99a-ede567383e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.89      0.57      0.70       147\n",
      "         poz       0.20      0.62      0.30        26\n",
      "\n",
      "    accuracy                           0.58       173\n",
      "   macro avg       0.55      0.59      0.50       173\n",
      "weighted avg       0.79      0.58      0.64       173\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on test data\n",
    "svm_clf = gscv_svm.best_estimator_.steps[2][1]\n",
    "\n",
    "# # Load the best estimator from the saved pickle file (replace with acctual file name)\n",
    "# pickle_file_name = \"models/timestamp/classifier_name-notebook_name.pkl\"\n",
    "# svm_clf = load_best_estimator(pickle_file_name).steps[2][1]\n",
    "\n",
    "# Evaluation on test data\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = svm_clf.predict(X_eval)\n",
    "SVM_classification_report = classification_report(y_eval, y_pred)\n",
    "\n",
    "# Create classification report as dictionary\n",
    "SVM_report_dict = classification_report(y_eval, y_pred, output_dict=True)\n",
    "\n",
    "print(classification_report(y_eval, y_pred, target_names=[\"neg\", \"poz\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1735f33-e739-4f9a-bb3c-d87f98d21466",
   "metadata": {},
   "source": [
    "## Decision Tree Model (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf64c198-6799-4e17-838e-e52009105996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 486 candidates, totalling 1458 fits\n",
      "Best estimator saved as: models/11042023_1923/DecisionTreeClassifier-HAB_modelling_5_8-11042023_1923.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>396</th>\n",
       "      <th>399</th>\n",
       "      <th>453</th>\n",
       "      <th>391</th>\n",
       "      <th>422</th>\n",
       "      <th>397</th>\n",
       "      <th>426</th>\n",
       "      <th>334</th>\n",
       "      <th>207</th>\n",
       "      <th>189</th>\n",
       "      <th>287</th>\n",
       "      <th>345</th>\n",
       "      <th>402</th>\n",
       "      <th>445</th>\n",
       "      <th>374</th>\n",
       "      <th>301</th>\n",
       "      <th>400</th>\n",
       "      <th>...</th>\n",
       "      <th>429</th>\n",
       "      <th>266</th>\n",
       "      <th>375</th>\n",
       "      <th>162</th>\n",
       "      <th>340</th>\n",
       "      <th>154</th>\n",
       "      <th>417</th>\n",
       "      <th>342</th>\n",
       "      <th>389</th>\n",
       "      <th>230</th>\n",
       "      <th>416</th>\n",
       "      <th>363</th>\n",
       "      <th>4</th>\n",
       "      <th>440</th>\n",
       "      <th>328</th>\n",
       "      <th>360</th>\n",
       "      <th>339</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>0.020429</td>\n",
       "      <td>0.020648</td>\n",
       "      <td>0.021016</td>\n",
       "      <td>0.021013</td>\n",
       "      <td>0.02045</td>\n",
       "      <td>0.02054</td>\n",
       "      <td>0.019995</td>\n",
       "      <td>0.019534</td>\n",
       "      <td>0.020696</td>\n",
       "      <td>0.020402</td>\n",
       "      <td>0.020561</td>\n",
       "      <td>0.020159</td>\n",
       "      <td>0.021824</td>\n",
       "      <td>0.022539</td>\n",
       "      <td>0.02064</td>\n",
       "      <td>0.02078</td>\n",
       "      <td>0.021013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020868</td>\n",
       "      <td>0.020386</td>\n",
       "      <td>0.023925</td>\n",
       "      <td>0.021591</td>\n",
       "      <td>0.022992</td>\n",
       "      <td>0.020546</td>\n",
       "      <td>0.021112</td>\n",
       "      <td>0.019733</td>\n",
       "      <td>0.021035</td>\n",
       "      <td>0.020951</td>\n",
       "      <td>0.020766</td>\n",
       "      <td>0.020403</td>\n",
       "      <td>0.01508</td>\n",
       "      <td>0.020732</td>\n",
       "      <td>0.020629</td>\n",
       "      <td>0.020688</td>\n",
       "      <td>0.020451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.00047</td>\n",
       "      <td>0.00189</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001068</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.001205</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.00337</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.00146</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.002098</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>0.000229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.014775</td>\n",
       "      <td>0.013088</td>\n",
       "      <td>0.012613</td>\n",
       "      <td>0.012418</td>\n",
       "      <td>0.011842</td>\n",
       "      <td>0.012534</td>\n",
       "      <td>0.012062</td>\n",
       "      <td>0.012491</td>\n",
       "      <td>0.013441</td>\n",
       "      <td>0.014373</td>\n",
       "      <td>0.011406</td>\n",
       "      <td>0.012145</td>\n",
       "      <td>0.013018</td>\n",
       "      <td>0.012991</td>\n",
       "      <td>0.012137</td>\n",
       "      <td>0.01357</td>\n",
       "      <td>0.012203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012176</td>\n",
       "      <td>0.013284</td>\n",
       "      <td>0.013128</td>\n",
       "      <td>0.012753</td>\n",
       "      <td>0.012827</td>\n",
       "      <td>0.012099</td>\n",
       "      <td>0.012108</td>\n",
       "      <td>0.012034</td>\n",
       "      <td>0.012377</td>\n",
       "      <td>0.012216</td>\n",
       "      <td>0.012789</td>\n",
       "      <td>0.011802</td>\n",
       "      <td>0.011953</td>\n",
       "      <td>0.01212</td>\n",
       "      <td>0.012272</td>\n",
       "      <td>0.012049</td>\n",
       "      <td>0.01295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.003432</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.000696</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.002234</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.002255</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.001058</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.001233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__class_weight</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>balanced</td>\n",
       "      <td>None</td>\n",
       "      <td>balanced</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>balanced</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>balanced</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__criterion</th>\n",
       "      <td>entropy</td>\n",
       "      <td>entropy</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>entropy</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>entropy</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>entropy</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>gini</td>\n",
       "      <td>entropy</td>\n",
       "      <td>entropy</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>entropy</td>\n",
       "      <td>gini</td>\n",
       "      <td>entropy</td>\n",
       "      <td>...</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>gini</td>\n",
       "      <td>entropy</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>entropy</td>\n",
       "      <td>entropy</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>entropy</td>\n",
       "      <td>entropy</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>entropy</td>\n",
       "      <td>gini</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>entropy</td>\n",
       "      <td>entropy</td>\n",
       "      <td>entropy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__max_depth</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_smt__k_neighbors</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_smt__sampling_strategy</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_under__sampling_strategy</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_recall</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_recall</th>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_recall</th>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_recall</th>\n",
       "      <td>0.500794</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.530952</td>\n",
       "      <td>0.581746</td>\n",
       "      <td>0.435714</td>\n",
       "      <td>0.58254</td>\n",
       "      <td>0.388095</td>\n",
       "      <td>0.515079</td>\n",
       "      <td>0.727778</td>\n",
       "      <td>0.499206</td>\n",
       "      <td>0.596825</td>\n",
       "      <td>0.388095</td>\n",
       "      <td>0.401587</td>\n",
       "      <td>0.450794</td>\n",
       "      <td>0.546032</td>\n",
       "      <td>0.529365</td>\n",
       "      <td>0.498413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.336508</td>\n",
       "      <td>0.304762</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0.552381</td>\n",
       "      <td>0.384127</td>\n",
       "      <td>0.46746</td>\n",
       "      <td>0.386508</td>\n",
       "      <td>0.304762</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.303175</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.400794</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.303175</td>\n",
       "      <td>0.20873</td>\n",
       "      <td>0.257143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_recall</th>\n",
       "      <td>0.034794</td>\n",
       "      <td>0.073334</td>\n",
       "      <td>0.069198</td>\n",
       "      <td>0.075701</td>\n",
       "      <td>0.040172</td>\n",
       "      <td>0.091707</td>\n",
       "      <td>0.047896</td>\n",
       "      <td>0.07429</td>\n",
       "      <td>0.116258</td>\n",
       "      <td>0.085191</td>\n",
       "      <td>0.058364</td>\n",
       "      <td>0.047896</td>\n",
       "      <td>0.092528</td>\n",
       "      <td>0.085655</td>\n",
       "      <td>0.10326</td>\n",
       "      <td>0.148771</td>\n",
       "      <td>0.072253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199824</td>\n",
       "      <td>0.094281</td>\n",
       "      <td>0.079333</td>\n",
       "      <td>0.23419</td>\n",
       "      <td>0.174805</td>\n",
       "      <td>0.040794</td>\n",
       "      <td>0.13851</td>\n",
       "      <td>0.154939</td>\n",
       "      <td>0.20023</td>\n",
       "      <td>0.051434</td>\n",
       "      <td>0.226389</td>\n",
       "      <td>0.132499</td>\n",
       "      <td>0.158034</td>\n",
       "      <td>0.140635</td>\n",
       "      <td>0.226389</td>\n",
       "      <td>0.056889</td>\n",
       "      <td>0.123443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_recall</th>\n",
       "      <td>230</td>\n",
       "      <td>163</td>\n",
       "      <td>200</td>\n",
       "      <td>118</td>\n",
       "      <td>336</td>\n",
       "      <td>116</td>\n",
       "      <td>395</td>\n",
       "      <td>219</td>\n",
       "      <td>5</td>\n",
       "      <td>235</td>\n",
       "      <td>104</td>\n",
       "      <td>395</td>\n",
       "      <td>386</td>\n",
       "      <td>318</td>\n",
       "      <td>182</td>\n",
       "      <td>204</td>\n",
       "      <td>237</td>\n",
       "      <td>...</td>\n",
       "      <td>454</td>\n",
       "      <td>469</td>\n",
       "      <td>461</td>\n",
       "      <td>152</td>\n",
       "      <td>409</td>\n",
       "      <td>285</td>\n",
       "      <td>404</td>\n",
       "      <td>468</td>\n",
       "      <td>273</td>\n",
       "      <td>310</td>\n",
       "      <td>472</td>\n",
       "      <td>476</td>\n",
       "      <td>390</td>\n",
       "      <td>392</td>\n",
       "      <td>472</td>\n",
       "      <td>486</td>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_precision</th>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.246377</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_precision</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.37037</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.37931</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.28125</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.232558</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.254902</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_precision</th>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.390244</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.351351</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.163265</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.264706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_precision</th>\n",
       "      <td>0.474731</td>\n",
       "      <td>0.432299</td>\n",
       "      <td>0.447695</td>\n",
       "      <td>0.395339</td>\n",
       "      <td>0.504453</td>\n",
       "      <td>0.380401</td>\n",
       "      <td>0.564815</td>\n",
       "      <td>0.427381</td>\n",
       "      <td>0.333419</td>\n",
       "      <td>0.422261</td>\n",
       "      <td>0.356176</td>\n",
       "      <td>0.530117</td>\n",
       "      <td>0.557692</td>\n",
       "      <td>0.486869</td>\n",
       "      <td>0.408369</td>\n",
       "      <td>0.426608</td>\n",
       "      <td>0.453535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.392222</td>\n",
       "      <td>0.299487</td>\n",
       "      <td>0.277864</td>\n",
       "      <td>0.220244</td>\n",
       "      <td>0.255102</td>\n",
       "      <td>0.215645</td>\n",
       "      <td>0.236881</td>\n",
       "      <td>0.306481</td>\n",
       "      <td>0.202309</td>\n",
       "      <td>0.196044</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>0.314992</td>\n",
       "      <td>0.207158</td>\n",
       "      <td>0.252791</td>\n",
       "      <td>0.440523</td>\n",
       "      <td>0.384259</td>\n",
       "      <td>0.377124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_precision</th>\n",
       "      <td>0.092765</td>\n",
       "      <td>0.105789</td>\n",
       "      <td>0.108392</td>\n",
       "      <td>0.138826</td>\n",
       "      <td>0.133111</td>\n",
       "      <td>0.043113</td>\n",
       "      <td>0.132895</td>\n",
       "      <td>0.122763</td>\n",
       "      <td>0.062499</td>\n",
       "      <td>0.104872</td>\n",
       "      <td>0.02256</td>\n",
       "      <td>0.044822</td>\n",
       "      <td>0.139561</td>\n",
       "      <td>0.172156</td>\n",
       "      <td>0.117025</td>\n",
       "      <td>0.12048</td>\n",
       "      <td>0.194039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194104</td>\n",
       "      <td>0.053076</td>\n",
       "      <td>0.03451</td>\n",
       "      <td>0.049133</td>\n",
       "      <td>0.015403</td>\n",
       "      <td>0.053583</td>\n",
       "      <td>0.02483</td>\n",
       "      <td>0.102247</td>\n",
       "      <td>0.046341</td>\n",
       "      <td>0.028412</td>\n",
       "      <td>0.166781</td>\n",
       "      <td>0.134969</td>\n",
       "      <td>0.045728</td>\n",
       "      <td>0.09502</td>\n",
       "      <td>0.170527</td>\n",
       "      <td>0.045831</td>\n",
       "      <td>0.157599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_precision</th>\n",
       "      <td>27</td>\n",
       "      <td>58</td>\n",
       "      <td>43</td>\n",
       "      <td>104</td>\n",
       "      <td>14</td>\n",
       "      <td>129</td>\n",
       "      <td>3</td>\n",
       "      <td>63</td>\n",
       "      <td>196</td>\n",
       "      <td>71</td>\n",
       "      <td>161</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>88</td>\n",
       "      <td>64</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>110</td>\n",
       "      <td>276</td>\n",
       "      <td>356</td>\n",
       "      <td>477</td>\n",
       "      <td>423</td>\n",
       "      <td>479</td>\n",
       "      <td>465</td>\n",
       "      <td>257</td>\n",
       "      <td>484</td>\n",
       "      <td>486</td>\n",
       "      <td>59</td>\n",
       "      <td>239</td>\n",
       "      <td>483</td>\n",
       "      <td>433</td>\n",
       "      <td>46</td>\n",
       "      <td>121</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_f1</th>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.440678</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.382022</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.45283</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.26087</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.337079</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.228571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_f1</th>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.40625</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>0.390244</td>\n",
       "      <td>0.40625</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.393443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.366197</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.371429</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.360656</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.351351</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_f1</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.38806</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.42623</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>...</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.255319</td>\n",
       "      <td>0.27027</td>\n",
       "      <td>0.232558</td>\n",
       "      <td>0.27451</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.27451</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.327273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_f1</th>\n",
       "      <td>0.483135</td>\n",
       "      <td>0.47207</td>\n",
       "      <td>0.46875</td>\n",
       "      <td>0.461489</td>\n",
       "      <td>0.460079</td>\n",
       "      <td>0.458727</td>\n",
       "      <td>0.457585</td>\n",
       "      <td>0.449559</td>\n",
       "      <td>0.447532</td>\n",
       "      <td>0.44716</td>\n",
       "      <td>0.443443</td>\n",
       "      <td>0.443408</td>\n",
       "      <td>0.44303</td>\n",
       "      <td>0.442475</td>\n",
       "      <td>0.441705</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>0.438993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296702</td>\n",
       "      <td>0.296349</td>\n",
       "      <td>0.292431</td>\n",
       "      <td>0.292402</td>\n",
       "      <td>0.291503</td>\n",
       "      <td>0.290059</td>\n",
       "      <td>0.286997</td>\n",
       "      <td>0.277213</td>\n",
       "      <td>0.275488</td>\n",
       "      <td>0.27295</td>\n",
       "      <td>0.272112</td>\n",
       "      <td>0.270248</td>\n",
       "      <td>0.269226</td>\n",
       "      <td>0.265772</td>\n",
       "      <td>0.264363</td>\n",
       "      <td>0.262882</td>\n",
       "      <td>0.262204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_f1</th>\n",
       "      <td>0.059581</td>\n",
       "      <td>0.055451</td>\n",
       "      <td>0.044194</td>\n",
       "      <td>0.111497</td>\n",
       "      <td>0.062017</td>\n",
       "      <td>0.056409</td>\n",
       "      <td>0.074304</td>\n",
       "      <td>0.046359</td>\n",
       "      <td>0.054792</td>\n",
       "      <td>0.062094</td>\n",
       "      <td>0.006715</td>\n",
       "      <td>0.015244</td>\n",
       "      <td>0.017787</td>\n",
       "      <td>0.053578</td>\n",
       "      <td>0.026674</td>\n",
       "      <td>0.068303</td>\n",
       "      <td>0.054841</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061721</td>\n",
       "      <td>0.067349</td>\n",
       "      <td>0.038181</td>\n",
       "      <td>0.064753</td>\n",
       "      <td>0.059546</td>\n",
       "      <td>0.04806</td>\n",
       "      <td>0.054877</td>\n",
       "      <td>0.067586</td>\n",
       "      <td>0.063855</td>\n",
       "      <td>0.034433</td>\n",
       "      <td>0.094657</td>\n",
       "      <td>0.063387</td>\n",
       "      <td>0.069312</td>\n",
       "      <td>0.054491</td>\n",
       "      <td>0.076553</td>\n",
       "      <td>0.038227</td>\n",
       "      <td>0.046019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_f1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>470</td>\n",
       "      <td>471</td>\n",
       "      <td>472</td>\n",
       "      <td>473</td>\n",
       "      <td>474</td>\n",
       "      <td>475</td>\n",
       "      <td>476</td>\n",
       "      <td>477</td>\n",
       "      <td>478</td>\n",
       "      <td>479</td>\n",
       "      <td>480</td>\n",
       "      <td>481</td>\n",
       "      <td>482</td>\n",
       "      <td>483</td>\n",
       "      <td>484</td>\n",
       "      <td>485</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_roc_auc</th>\n",
       "      <td>0.77807</td>\n",
       "      <td>0.758991</td>\n",
       "      <td>0.728947</td>\n",
       "      <td>0.762281</td>\n",
       "      <td>0.741228</td>\n",
       "      <td>0.796272</td>\n",
       "      <td>0.747368</td>\n",
       "      <td>0.714035</td>\n",
       "      <td>0.672149</td>\n",
       "      <td>0.749342</td>\n",
       "      <td>0.74057</td>\n",
       "      <td>0.677632</td>\n",
       "      <td>0.741667</td>\n",
       "      <td>0.787061</td>\n",
       "      <td>0.756798</td>\n",
       "      <td>0.688816</td>\n",
       "      <td>0.695833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.747807</td>\n",
       "      <td>0.677632</td>\n",
       "      <td>0.632456</td>\n",
       "      <td>0.645614</td>\n",
       "      <td>0.691228</td>\n",
       "      <td>0.487061</td>\n",
       "      <td>0.655702</td>\n",
       "      <td>0.57807</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.585526</td>\n",
       "      <td>0.558772</td>\n",
       "      <td>0.725439</td>\n",
       "      <td>0.622368</td>\n",
       "      <td>0.501316</td>\n",
       "      <td>0.590789</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.691228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_roc_auc</th>\n",
       "      <td>0.768647</td>\n",
       "      <td>0.702697</td>\n",
       "      <td>0.739781</td>\n",
       "      <td>0.698062</td>\n",
       "      <td>0.695954</td>\n",
       "      <td>0.656131</td>\n",
       "      <td>0.699747</td>\n",
       "      <td>0.660135</td>\n",
       "      <td>0.683734</td>\n",
       "      <td>0.798567</td>\n",
       "      <td>0.67952</td>\n",
       "      <td>0.735777</td>\n",
       "      <td>0.702276</td>\n",
       "      <td>0.693426</td>\n",
       "      <td>0.738517</td>\n",
       "      <td>0.699536</td>\n",
       "      <td>0.734092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.686262</td>\n",
       "      <td>0.690265</td>\n",
       "      <td>0.698272</td>\n",
       "      <td>0.672988</td>\n",
       "      <td>0.681837</td>\n",
       "      <td>0.665824</td>\n",
       "      <td>0.66772</td>\n",
       "      <td>0.694269</td>\n",
       "      <td>0.565529</td>\n",
       "      <td>0.667299</td>\n",
       "      <td>0.692373</td>\n",
       "      <td>0.598188</td>\n",
       "      <td>0.655921</td>\n",
       "      <td>0.67067</td>\n",
       "      <td>0.684155</td>\n",
       "      <td>0.650442</td>\n",
       "      <td>0.719343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_roc_auc</th>\n",
       "      <td>0.742309</td>\n",
       "      <td>0.788453</td>\n",
       "      <td>0.762958</td>\n",
       "      <td>0.705225</td>\n",
       "      <td>0.731353</td>\n",
       "      <td>0.702276</td>\n",
       "      <td>0.698904</td>\n",
       "      <td>0.70965</td>\n",
       "      <td>0.82048</td>\n",
       "      <td>0.723557</td>\n",
       "      <td>0.741256</td>\n",
       "      <td>0.70354</td>\n",
       "      <td>0.774968</td>\n",
       "      <td>0.731563</td>\n",
       "      <td>0.786768</td>\n",
       "      <td>0.714707</td>\n",
       "      <td>0.704172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.613991</td>\n",
       "      <td>0.655289</td>\n",
       "      <td>0.720396</td>\n",
       "      <td>0.631479</td>\n",
       "      <td>0.645807</td>\n",
       "      <td>0.624315</td>\n",
       "      <td>0.649178</td>\n",
       "      <td>0.62579</td>\n",
       "      <td>0.578803</td>\n",
       "      <td>0.514749</td>\n",
       "      <td>0.640329</td>\n",
       "      <td>0.636115</td>\n",
       "      <td>0.650653</td>\n",
       "      <td>0.668563</td>\n",
       "      <td>0.632533</td>\n",
       "      <td>0.667299</td>\n",
       "      <td>0.678677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_roc_auc</th>\n",
       "      <td>0.763009</td>\n",
       "      <td>0.750047</td>\n",
       "      <td>0.743896</td>\n",
       "      <td>0.721856</td>\n",
       "      <td>0.722845</td>\n",
       "      <td>0.718226</td>\n",
       "      <td>0.71534</td>\n",
       "      <td>0.694607</td>\n",
       "      <td>0.725454</td>\n",
       "      <td>0.757155</td>\n",
       "      <td>0.720449</td>\n",
       "      <td>0.70565</td>\n",
       "      <td>0.739637</td>\n",
       "      <td>0.73735</td>\n",
       "      <td>0.760694</td>\n",
       "      <td>0.70102</td>\n",
       "      <td>0.711366</td>\n",
       "      <td>...</td>\n",
       "      <td>0.682687</td>\n",
       "      <td>0.674395</td>\n",
       "      <td>0.683708</td>\n",
       "      <td>0.650027</td>\n",
       "      <td>0.672957</td>\n",
       "      <td>0.5924</td>\n",
       "      <td>0.657533</td>\n",
       "      <td>0.63271</td>\n",
       "      <td>0.606444</td>\n",
       "      <td>0.589191</td>\n",
       "      <td>0.630491</td>\n",
       "      <td>0.653247</td>\n",
       "      <td>0.642981</td>\n",
       "      <td>0.613516</td>\n",
       "      <td>0.635826</td>\n",
       "      <td>0.626747</td>\n",
       "      <td>0.696416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_roc_auc</th>\n",
       "      <td>0.015134</td>\n",
       "      <td>0.035577</td>\n",
       "      <td>0.014186</td>\n",
       "      <td>0.028734</td>\n",
       "      <td>0.019437</td>\n",
       "      <td>0.058313</td>\n",
       "      <td>0.02265</td>\n",
       "      <td>0.024441</td>\n",
       "      <td>0.06736</td>\n",
       "      <td>0.031117</td>\n",
       "      <td>0.028942</td>\n",
       "      <td>0.023785</td>\n",
       "      <td>0.029711</td>\n",
       "      <td>0.038445</td>\n",
       "      <td>0.01989</td>\n",
       "      <td>0.010622</td>\n",
       "      <td>0.016426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054689</td>\n",
       "      <td>0.014461</td>\n",
       "      <td>0.037349</td>\n",
       "      <td>0.017231</td>\n",
       "      <td>0.019577</td>\n",
       "      <td>0.076389</td>\n",
       "      <td>0.00768</td>\n",
       "      <td>0.04769</td>\n",
       "      <td>0.048778</td>\n",
       "      <td>0.062332</td>\n",
       "      <td>0.054984</td>\n",
       "      <td>0.053344</td>\n",
       "      <td>0.014733</td>\n",
       "      <td>0.079342</td>\n",
       "      <td>0.038187</td>\n",
       "      <td>0.045948</td>\n",
       "      <td>0.017002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_roc_auc</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>48</td>\n",
       "      <td>44</td>\n",
       "      <td>59</td>\n",
       "      <td>72</td>\n",
       "      <td>170</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>52</td>\n",
       "      <td>104</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>83</td>\n",
       "      <td>...</td>\n",
       "      <td>254</td>\n",
       "      <td>310</td>\n",
       "      <td>251</td>\n",
       "      <td>422</td>\n",
       "      <td>318</td>\n",
       "      <td>484</td>\n",
       "      <td>395</td>\n",
       "      <td>454</td>\n",
       "      <td>480</td>\n",
       "      <td>485</td>\n",
       "      <td>458</td>\n",
       "      <td>413</td>\n",
       "      <td>437</td>\n",
       "      <td>478</td>\n",
       "      <td>449</td>\n",
       "      <td>466</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_recall_weighted</th>\n",
       "      <td>0.858209</td>\n",
       "      <td>0.753731</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.88806</td>\n",
       "      <td>0.820896</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.873134</td>\n",
       "      <td>0.589552</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.783582</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.88806</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.88806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.813433</td>\n",
       "      <td>0.746269</td>\n",
       "      <td>0.328358</td>\n",
       "      <td>0.798507</td>\n",
       "      <td>0.537313</td>\n",
       "      <td>0.701493</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.559701</td>\n",
       "      <td>0.61194</td>\n",
       "      <td>0.858209</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.664179</td>\n",
       "      <td>0.432836</td>\n",
       "      <td>0.858209</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.798507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_recall_weighted</th>\n",
       "      <td>0.858209</td>\n",
       "      <td>0.798507</td>\n",
       "      <td>0.716418</td>\n",
       "      <td>0.753731</td>\n",
       "      <td>0.820896</td>\n",
       "      <td>0.791045</td>\n",
       "      <td>0.828358</td>\n",
       "      <td>0.783582</td>\n",
       "      <td>0.776119</td>\n",
       "      <td>0.791045</td>\n",
       "      <td>0.791045</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.813433</td>\n",
       "      <td>0.716418</td>\n",
       "      <td>0.858209</td>\n",
       "      <td>0.723881</td>\n",
       "      <td>...</td>\n",
       "      <td>0.664179</td>\n",
       "      <td>0.791045</td>\n",
       "      <td>0.746269</td>\n",
       "      <td>0.686567</td>\n",
       "      <td>0.671642</td>\n",
       "      <td>0.738806</td>\n",
       "      <td>0.686567</td>\n",
       "      <td>0.708955</td>\n",
       "      <td>0.723881</td>\n",
       "      <td>0.671642</td>\n",
       "      <td>0.708955</td>\n",
       "      <td>0.708955</td>\n",
       "      <td>0.641791</td>\n",
       "      <td>0.708955</td>\n",
       "      <td>0.656716</td>\n",
       "      <td>0.798507</td>\n",
       "      <td>0.850746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_recall_weighted</th>\n",
       "      <td>0.776119</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.69403</td>\n",
       "      <td>0.80597</td>\n",
       "      <td>0.753731</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.738806</td>\n",
       "      <td>0.776119</td>\n",
       "      <td>0.761194</td>\n",
       "      <td>0.731343</td>\n",
       "      <td>0.858209</td>\n",
       "      <td>0.813433</td>\n",
       "      <td>0.753731</td>\n",
       "      <td>0.761194</td>\n",
       "      <td>0.641791</td>\n",
       "      <td>0.761194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.783582</td>\n",
       "      <td>0.738806</td>\n",
       "      <td>0.798507</td>\n",
       "      <td>0.753731</td>\n",
       "      <td>0.723881</td>\n",
       "      <td>0.626866</td>\n",
       "      <td>0.761194</td>\n",
       "      <td>0.753731</td>\n",
       "      <td>0.61194</td>\n",
       "      <td>0.597015</td>\n",
       "      <td>0.813433</td>\n",
       "      <td>0.738806</td>\n",
       "      <td>0.723881</td>\n",
       "      <td>0.820896</td>\n",
       "      <td>0.828358</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.723881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_recall_weighted</th>\n",
       "      <td>0.830846</td>\n",
       "      <td>0.80597</td>\n",
       "      <td>0.80597</td>\n",
       "      <td>0.776119</td>\n",
       "      <td>0.838308</td>\n",
       "      <td>0.788557</td>\n",
       "      <td>0.855721</td>\n",
       "      <td>0.798507</td>\n",
       "      <td>0.71393</td>\n",
       "      <td>0.80597</td>\n",
       "      <td>0.768657</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.818408</td>\n",
       "      <td>0.781095</td>\n",
       "      <td>0.781095</td>\n",
       "      <td>0.791045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.771144</td>\n",
       "      <td>0.781095</td>\n",
       "      <td>0.763682</td>\n",
       "      <td>0.589552</td>\n",
       "      <td>0.731343</td>\n",
       "      <td>0.634328</td>\n",
       "      <td>0.716418</td>\n",
       "      <td>0.768657</td>\n",
       "      <td>0.631841</td>\n",
       "      <td>0.626866</td>\n",
       "      <td>0.793532</td>\n",
       "      <td>0.766169</td>\n",
       "      <td>0.676617</td>\n",
       "      <td>0.654229</td>\n",
       "      <td>0.781095</td>\n",
       "      <td>0.823383</td>\n",
       "      <td>0.791045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_recall_weighted</th>\n",
       "      <td>0.038697</td>\n",
       "      <td>0.046003</td>\n",
       "      <td>0.064485</td>\n",
       "      <td>0.077794</td>\n",
       "      <td>0.035703</td>\n",
       "      <td>0.027476</td>\n",
       "      <td>0.028796</td>\n",
       "      <td>0.055846</td>\n",
       "      <td>0.087949</td>\n",
       "      <td>0.043939</td>\n",
       "      <td>0.02656</td>\n",
       "      <td>0.006093</td>\n",
       "      <td>0.027476</td>\n",
       "      <td>0.054952</td>\n",
       "      <td>0.062536</td>\n",
       "      <td>0.098691</td>\n",
       "      <td>0.070271</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082728</td>\n",
       "      <td>0.031268</td>\n",
       "      <td>0.024626</td>\n",
       "      <td>0.186716</td>\n",
       "      <td>0.052061</td>\n",
       "      <td>0.082428</td>\n",
       "      <td>0.032242</td>\n",
       "      <td>0.055846</td>\n",
       "      <td>0.068487</td>\n",
       "      <td>0.032242</td>\n",
       "      <td>0.062536</td>\n",
       "      <td>0.061034</td>\n",
       "      <td>0.034648</td>\n",
       "      <td>0.163082</td>\n",
       "      <td>0.088789</td>\n",
       "      <td>0.01759</td>\n",
       "      <td>0.052061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_recall_weighted</th>\n",
       "      <td>25</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>143</td>\n",
       "      <td>17</td>\n",
       "      <td>111</td>\n",
       "      <td>2</td>\n",
       "      <td>82</td>\n",
       "      <td>293</td>\n",
       "      <td>67</td>\n",
       "      <td>163</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>45</td>\n",
       "      <td>127</td>\n",
       "      <td>127</td>\n",
       "      <td>101</td>\n",
       "      <td>...</td>\n",
       "      <td>155</td>\n",
       "      <td>127</td>\n",
       "      <td>176</td>\n",
       "      <td>475</td>\n",
       "      <td>252</td>\n",
       "      <td>443</td>\n",
       "      <td>284</td>\n",
       "      <td>163</td>\n",
       "      <td>447</td>\n",
       "      <td>454</td>\n",
       "      <td>94</td>\n",
       "      <td>170</td>\n",
       "      <td>370</td>\n",
       "      <td>418</td>\n",
       "      <td>127</td>\n",
       "      <td>34</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41 rows × 486 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              396  \\\n",
       "mean_fit_time                                                            0.020429   \n",
       "std_fit_time                                                             0.000319   \n",
       "mean_score_time                                                          0.014775   \n",
       "std_score_time                                                           0.003432   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__max_depth                                                            4   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.500794   \n",
       "std_test_recall                                                          0.034794   \n",
       "rank_test_recall                                                              230   \n",
       "split0_test_precision                                                     0.52381   \n",
       "split1_test_precision                                                    0.555556   \n",
       "split2_test_precision                                                    0.344828   \n",
       "mean_test_precision                                                      0.474731   \n",
       "std_test_precision                                                       0.092765   \n",
       "rank_test_precision                                                            27   \n",
       "split0_test_f1                                                           0.536585   \n",
       "split1_test_f1                                                           0.512821   \n",
       "split2_test_f1                                                                0.4   \n",
       "mean_test_f1                                                             0.483135   \n",
       "std_test_f1                                                              0.059581   \n",
       "rank_test_f1                                                                    1   \n",
       "split0_test_roc_auc                                                       0.77807   \n",
       "split1_test_roc_auc                                                      0.768647   \n",
       "split2_test_roc_auc                                                      0.742309   \n",
       "mean_test_roc_auc                                                        0.763009   \n",
       "std_test_roc_auc                                                         0.015134   \n",
       "rank_test_roc_auc                                                               1   \n",
       "split0_test_recall_weighted                                              0.858209   \n",
       "split1_test_recall_weighted                                              0.858209   \n",
       "split2_test_recall_weighted                                              0.776119   \n",
       "mean_test_recall_weighted                                                0.830846   \n",
       "std_test_recall_weighted                                                 0.038697   \n",
       "rank_test_recall_weighted                                                      25   \n",
       "\n",
       "                                                                              399  \\\n",
       "mean_fit_time                                                            0.020648   \n",
       "std_fit_time                                                             0.000073   \n",
       "mean_score_time                                                          0.013088   \n",
       "std_score_time                                                           0.001013   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__max_depth                                                            4   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.65   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                             0.55   \n",
       "std_test_recall                                                          0.073334   \n",
       "rank_test_recall                                                              163   \n",
       "split0_test_precision                                                    0.333333   \n",
       "split1_test_precision                                                    0.384615   \n",
       "split2_test_precision                                                    0.578947   \n",
       "mean_test_precision                                                      0.432299   \n",
       "std_test_precision                                                       0.105789   \n",
       "rank_test_precision                                                            58   \n",
       "split0_test_f1                                                           0.440678   \n",
       "split1_test_f1                                                           0.425532   \n",
       "split2_test_f1                                                               0.55   \n",
       "mean_test_f1                                                              0.47207   \n",
       "std_test_f1                                                              0.055451   \n",
       "rank_test_f1                                                                    2   \n",
       "split0_test_roc_auc                                                      0.758991   \n",
       "split1_test_roc_auc                                                      0.702697   \n",
       "split2_test_roc_auc                                                      0.788453   \n",
       "mean_test_roc_auc                                                        0.750047   \n",
       "std_test_roc_auc                                                         0.035577   \n",
       "rank_test_roc_auc                                                               5   \n",
       "split0_test_recall_weighted                                              0.753731   \n",
       "split1_test_recall_weighted                                              0.798507   \n",
       "split2_test_recall_weighted                                              0.865672   \n",
       "mean_test_recall_weighted                                                 0.80597   \n",
       "std_test_recall_weighted                                                 0.046003   \n",
       "rank_test_recall_weighted                                                      67   \n",
       "\n",
       "                                                                              453  \\\n",
       "mean_fit_time                                                            0.021016   \n",
       "std_fit_time                                                             0.000371   \n",
       "mean_score_time                                                          0.012613   \n",
       "std_score_time                                                           0.000696   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__max_depth                                                            3   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                       0.619048   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.530952   \n",
       "std_test_recall                                                          0.069198   \n",
       "rank_test_recall                                                              200   \n",
       "split0_test_precision                                                      0.5625   \n",
       "split1_test_precision                                                    0.302326   \n",
       "split2_test_precision                                                    0.478261   \n",
       "mean_test_precision                                                      0.447695   \n",
       "std_test_precision                                                       0.108392   \n",
       "rank_test_precision                                                            43   \n",
       "split0_test_f1                                                                0.5   \n",
       "split1_test_f1                                                            0.40625   \n",
       "split2_test_f1                                                                0.5   \n",
       "mean_test_f1                                                              0.46875   \n",
       "std_test_f1                                                              0.044194   \n",
       "rank_test_f1                                                                    3   \n",
       "split0_test_roc_auc                                                      0.728947   \n",
       "split1_test_roc_auc                                                      0.739781   \n",
       "split2_test_roc_auc                                                      0.762958   \n",
       "mean_test_roc_auc                                                        0.743896   \n",
       "std_test_roc_auc                                                         0.014186   \n",
       "rank_test_roc_auc                                                               6   \n",
       "split0_test_recall_weighted                                              0.865672   \n",
       "split1_test_recall_weighted                                              0.716418   \n",
       "split2_test_recall_weighted                                              0.835821   \n",
       "mean_test_recall_weighted                                                 0.80597   \n",
       "std_test_recall_weighted                                                 0.064485   \n",
       "rank_test_recall_weighted                                                      67   \n",
       "\n",
       "                                                                              391  \\\n",
       "mean_fit_time                                                            0.021013   \n",
       "std_fit_time                                                             0.000377   \n",
       "mean_score_time                                                          0.012418   \n",
       "std_score_time                                                           0.000533   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__max_depth                                                            4   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.65   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                       0.619048   \n",
       "mean_test_recall                                                         0.581746   \n",
       "std_test_recall                                                          0.075701   \n",
       "rank_test_recall                                                              118   \n",
       "split0_test_precision                                                    0.590909   \n",
       "split1_test_precision                                                      0.3125   \n",
       "split2_test_precision                                                    0.282609   \n",
       "mean_test_precision                                                      0.395339   \n",
       "std_test_precision                                                       0.138826   \n",
       "rank_test_precision                                                           104   \n",
       "split0_test_f1                                                           0.619048   \n",
       "split1_test_f1                                                           0.377358   \n",
       "split2_test_f1                                                            0.38806   \n",
       "mean_test_f1                                                             0.461489   \n",
       "std_test_f1                                                              0.111497   \n",
       "rank_test_f1                                                                    4   \n",
       "split0_test_roc_auc                                                      0.762281   \n",
       "split1_test_roc_auc                                                      0.698062   \n",
       "split2_test_roc_auc                                                      0.705225   \n",
       "mean_test_roc_auc                                                        0.721856   \n",
       "std_test_roc_auc                                                         0.028734   \n",
       "rank_test_roc_auc                                                              48   \n",
       "split0_test_recall_weighted                                              0.880597   \n",
       "split1_test_recall_weighted                                              0.753731   \n",
       "split2_test_recall_weighted                                               0.69403   \n",
       "mean_test_recall_weighted                                                0.776119   \n",
       "std_test_recall_weighted                                                 0.077794   \n",
       "rank_test_recall_weighted                                                     143   \n",
       "\n",
       "                                                                              422  \\\n",
       "mean_fit_time                                                             0.02045   \n",
       "std_fit_time                                                             0.000074   \n",
       "mean_score_time                                                          0.011842   \n",
       "std_score_time                                                           0.000521   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__max_depth                                                            2   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                       0.380952   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.435714   \n",
       "std_test_recall                                                          0.040172   \n",
       "rank_test_recall                                                              336   \n",
       "split0_test_precision                                                    0.692308   \n",
       "split1_test_precision                                                    0.421053   \n",
       "split2_test_precision                                                         0.4   \n",
       "mean_test_precision                                                      0.504453   \n",
       "std_test_precision                                                       0.133111   \n",
       "rank_test_precision                                                            14   \n",
       "split0_test_f1                                                           0.545455   \n",
       "split1_test_f1                                                                0.4   \n",
       "split2_test_f1                                                           0.434783   \n",
       "mean_test_f1                                                             0.460079   \n",
       "std_test_f1                                                              0.062017   \n",
       "rank_test_f1                                                                    5   \n",
       "split0_test_roc_auc                                                      0.741228   \n",
       "split1_test_roc_auc                                                      0.695954   \n",
       "split2_test_roc_auc                                                      0.731353   \n",
       "mean_test_roc_auc                                                        0.722845   \n",
       "std_test_roc_auc                                                         0.019437   \n",
       "rank_test_roc_auc                                                              44   \n",
       "split0_test_recall_weighted                                               0.88806   \n",
       "split1_test_recall_weighted                                              0.820896   \n",
       "split2_test_recall_weighted                                               0.80597   \n",
       "mean_test_recall_weighted                                                0.838308   \n",
       "std_test_recall_weighted                                                 0.035703   \n",
       "rank_test_recall_weighted                                                      17   \n",
       "\n",
       "                                                                              397  \\\n",
       "mean_fit_time                                                             0.02054   \n",
       "std_fit_time                                                             0.000442   \n",
       "mean_score_time                                                          0.012534   \n",
       "std_score_time                                                           0.000587   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__max_depth                                                            4   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.7   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                       0.571429   \n",
       "mean_test_recall                                                          0.58254   \n",
       "std_test_recall                                                          0.091707   \n",
       "rank_test_recall                                                              116   \n",
       "split0_test_precision                                                      0.4375   \n",
       "split1_test_precision                                                     0.37037   \n",
       "split2_test_precision                                                    0.333333   \n",
       "mean_test_precision                                                      0.380401   \n",
       "std_test_precision                                                       0.043113   \n",
       "rank_test_precision                                                           129   \n",
       "split0_test_f1                                                           0.538462   \n",
       "split1_test_f1                                                           0.416667   \n",
       "split2_test_f1                                                           0.421053   \n",
       "mean_test_f1                                                             0.458727   \n",
       "std_test_f1                                                              0.056409   \n",
       "rank_test_f1                                                                    6   \n",
       "split0_test_roc_auc                                                      0.796272   \n",
       "split1_test_roc_auc                                                      0.656131   \n",
       "split2_test_roc_auc                                                      0.702276   \n",
       "mean_test_roc_auc                                                        0.718226   \n",
       "std_test_roc_auc                                                         0.058313   \n",
       "rank_test_roc_auc                                                              59   \n",
       "split0_test_recall_weighted                                              0.820896   \n",
       "split1_test_recall_weighted                                              0.791045   \n",
       "split2_test_recall_weighted                                              0.753731   \n",
       "mean_test_recall_weighted                                                0.788557   \n",
       "std_test_recall_weighted                                                 0.027476   \n",
       "rank_test_recall_weighted                                                     111   \n",
       "\n",
       "                                                                              426  \\\n",
       "mean_fit_time                                                            0.019995   \n",
       "std_fit_time                                                             0.000536   \n",
       "mean_score_time                                                          0.012062   \n",
       "std_score_time                                                           0.000236   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__max_depth                                                            2   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                       0.380952   \n",
       "split2_test_recall                                                       0.333333   \n",
       "mean_test_recall                                                         0.388095   \n",
       "std_test_recall                                                          0.047896   \n",
       "rank_test_recall                                                              395   \n",
       "split0_test_precision                                                        0.75   \n",
       "split1_test_precision                                                    0.444444   \n",
       "split2_test_precision                                                         0.5   \n",
       "mean_test_precision                                                      0.564815   \n",
       "std_test_precision                                                       0.132895   \n",
       "rank_test_precision                                                             3   \n",
       "split0_test_f1                                                             0.5625   \n",
       "split1_test_f1                                                           0.410256   \n",
       "split2_test_f1                                                                0.4   \n",
       "mean_test_f1                                                             0.457585   \n",
       "std_test_f1                                                              0.074304   \n",
       "rank_test_f1                                                                    7   \n",
       "split0_test_roc_auc                                                      0.747368   \n",
       "split1_test_roc_auc                                                      0.699747   \n",
       "split2_test_roc_auc                                                      0.698904   \n",
       "mean_test_roc_auc                                                         0.71534   \n",
       "std_test_roc_auc                                                          0.02265   \n",
       "rank_test_roc_auc                                                              72   \n",
       "split0_test_recall_weighted                                              0.895522   \n",
       "split1_test_recall_weighted                                              0.828358   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.855721   \n",
       "std_test_recall_weighted                                                 0.028796   \n",
       "rank_test_recall_weighted                                                       2   \n",
       "\n",
       "                                                                              334  \\\n",
       "mean_fit_time                                                            0.019534   \n",
       "std_fit_time                                                             0.000132   \n",
       "mean_score_time                                                          0.012491   \n",
       "std_score_time                                                           0.000504   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__max_depth                                                            2   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                       0.619048   \n",
       "mean_test_recall                                                         0.515079   \n",
       "std_test_recall                                                           0.07429   \n",
       "rank_test_recall                                                              219   \n",
       "split0_test_precision                                                         0.6   \n",
       "split1_test_precision                                                    0.357143   \n",
       "split2_test_precision                                                       0.325   \n",
       "mean_test_precision                                                      0.427381   \n",
       "std_test_precision                                                       0.122763   \n",
       "rank_test_precision                                                            63   \n",
       "split0_test_f1                                                           0.514286   \n",
       "split1_test_f1                                                           0.408163   \n",
       "split2_test_f1                                                            0.42623   \n",
       "mean_test_f1                                                             0.449559   \n",
       "std_test_f1                                                              0.046359   \n",
       "rank_test_f1                                                                    8   \n",
       "split0_test_roc_auc                                                      0.714035   \n",
       "split1_test_roc_auc                                                      0.660135   \n",
       "split2_test_roc_auc                                                       0.70965   \n",
       "mean_test_roc_auc                                                        0.694607   \n",
       "std_test_roc_auc                                                         0.024441   \n",
       "rank_test_roc_auc                                                             170   \n",
       "split0_test_recall_weighted                                              0.873134   \n",
       "split1_test_recall_weighted                                              0.783582   \n",
       "split2_test_recall_weighted                                              0.738806   \n",
       "mean_test_recall_weighted                                                0.798507   \n",
       "std_test_recall_weighted                                                 0.055846   \n",
       "rank_test_recall_weighted                                                      82   \n",
       "\n",
       "                                                                              207  \\\n",
       "mean_fit_time                                                            0.020696   \n",
       "std_fit_time                                                             0.000484   \n",
       "mean_score_time                                                          0.013441   \n",
       "std_score_time                                                           0.000633   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__max_depth                                                            3   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                           0.85   \n",
       "split1_test_recall                                                       0.571429   \n",
       "split2_test_recall                                                       0.761905   \n",
       "mean_test_recall                                                         0.727778   \n",
       "std_test_recall                                                          0.116258   \n",
       "rank_test_recall                                                                5   \n",
       "split0_test_precision                                                    0.246377   \n",
       "split1_test_precision                                                    0.363636   \n",
       "split2_test_precision                                                    0.390244   \n",
       "mean_test_precision                                                      0.333419   \n",
       "std_test_precision                                                       0.062499   \n",
       "rank_test_precision                                                           196   \n",
       "split0_test_f1                                                           0.382022   \n",
       "split1_test_f1                                                           0.444444   \n",
       "split2_test_f1                                                           0.516129   \n",
       "mean_test_f1                                                             0.447532   \n",
       "std_test_f1                                                              0.054792   \n",
       "rank_test_f1                                                                    9   \n",
       "split0_test_roc_auc                                                      0.672149   \n",
       "split1_test_roc_auc                                                      0.683734   \n",
       "split2_test_roc_auc                                                       0.82048   \n",
       "mean_test_roc_auc                                                        0.725454   \n",
       "std_test_roc_auc                                                          0.06736   \n",
       "rank_test_roc_auc                                                              33   \n",
       "split0_test_recall_weighted                                              0.589552   \n",
       "split1_test_recall_weighted                                              0.776119   \n",
       "split2_test_recall_weighted                                              0.776119   \n",
       "mean_test_recall_weighted                                                 0.71393   \n",
       "std_test_recall_weighted                                                 0.087949   \n",
       "rank_test_recall_weighted                                                     293   \n",
       "\n",
       "                                                                              189  \\\n",
       "mean_fit_time                                                            0.020402   \n",
       "std_fit_time                                                             0.000227   \n",
       "mean_score_time                                                          0.014373   \n",
       "std_score_time                                                           0.002234   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__max_depth                                                            3   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                       0.619048   \n",
       "split2_test_recall                                                       0.428571   \n",
       "mean_test_recall                                                         0.499206   \n",
       "std_test_recall                                                          0.085191   \n",
       "rank_test_recall                                                              235   \n",
       "split0_test_precision                                                      0.5625   \n",
       "split1_test_precision                                                    0.393939   \n",
       "split2_test_precision                                                    0.310345   \n",
       "mean_test_precision                                                      0.422261   \n",
       "std_test_precision                                                       0.104872   \n",
       "rank_test_precision                                                            71   \n",
       "split0_test_f1                                                                0.5   \n",
       "split1_test_f1                                                           0.481481   \n",
       "split2_test_f1                                                               0.36   \n",
       "mean_test_f1                                                              0.44716   \n",
       "std_test_f1                                                              0.062094   \n",
       "rank_test_f1                                                                   10   \n",
       "split0_test_roc_auc                                                      0.749342   \n",
       "split1_test_roc_auc                                                      0.798567   \n",
       "split2_test_roc_auc                                                      0.723557   \n",
       "mean_test_roc_auc                                                        0.757155   \n",
       "std_test_roc_auc                                                         0.031117   \n",
       "rank_test_roc_auc                                                               4   \n",
       "split0_test_recall_weighted                                              0.865672   \n",
       "split1_test_recall_weighted                                              0.791045   \n",
       "split2_test_recall_weighted                                              0.761194   \n",
       "mean_test_recall_weighted                                                 0.80597   \n",
       "std_test_recall_weighted                                                 0.043939   \n",
       "rank_test_recall_weighted                                                      67   \n",
       "\n",
       "                                                                              287  \\\n",
       "mean_fit_time                                                            0.020561   \n",
       "std_fit_time                                                             0.000178   \n",
       "mean_score_time                                                          0.011406   \n",
       "std_score_time                                                           0.001098   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__max_depth                                                            3   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.6   \n",
       "split1_test_recall                                                        0.52381   \n",
       "split2_test_recall                                                       0.666667   \n",
       "mean_test_recall                                                         0.596825   \n",
       "std_test_recall                                                          0.058364   \n",
       "rank_test_recall                                                              104   \n",
       "split0_test_precision                                                    0.363636   \n",
       "split1_test_precision                                                     0.37931   \n",
       "split2_test_precision                                                    0.325581   \n",
       "mean_test_precision                                                      0.356176   \n",
       "std_test_precision                                                        0.02256   \n",
       "rank_test_precision                                                           161   \n",
       "split0_test_f1                                                            0.45283   \n",
       "split1_test_f1                                                               0.44   \n",
       "split2_test_f1                                                             0.4375   \n",
       "mean_test_f1                                                             0.443443   \n",
       "std_test_f1                                                              0.006715   \n",
       "rank_test_f1                                                                   11   \n",
       "split0_test_roc_auc                                                       0.74057   \n",
       "split1_test_roc_auc                                                       0.67952   \n",
       "split2_test_roc_auc                                                      0.741256   \n",
       "mean_test_roc_auc                                                        0.720449   \n",
       "std_test_roc_auc                                                         0.028942   \n",
       "rank_test_roc_auc                                                              52   \n",
       "split0_test_recall_weighted                                              0.783582   \n",
       "split1_test_recall_weighted                                              0.791045   \n",
       "split2_test_recall_weighted                                              0.731343   \n",
       "mean_test_recall_weighted                                                0.768657   \n",
       "std_test_recall_weighted                                                  0.02656   \n",
       "rank_test_recall_weighted                                                     163   \n",
       "\n",
       "                                                                              345  \\\n",
       "mean_fit_time                                                            0.020159   \n",
       "std_fit_time                                                             0.000107   \n",
       "mean_score_time                                                          0.012145   \n",
       "std_score_time                                                           0.000193   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__max_depth                                                            2   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                       0.380952   \n",
       "split2_test_recall                                                       0.333333   \n",
       "mean_test_recall                                                         0.388095   \n",
       "std_test_recall                                                          0.047896   \n",
       "rank_test_recall                                                              395   \n",
       "split0_test_precision                                                    0.473684   \n",
       "split1_test_precision                                                    0.533333   \n",
       "split2_test_precision                                                    0.583333   \n",
       "mean_test_precision                                                      0.530117   \n",
       "std_test_precision                                                       0.044822   \n",
       "rank_test_precision                                                             8   \n",
       "split0_test_f1                                                           0.461538   \n",
       "split1_test_f1                                                           0.444444   \n",
       "split2_test_f1                                                           0.424242   \n",
       "mean_test_f1                                                             0.443408   \n",
       "std_test_f1                                                              0.015244   \n",
       "rank_test_f1                                                                   12   \n",
       "split0_test_roc_auc                                                      0.677632   \n",
       "split1_test_roc_auc                                                      0.735777   \n",
       "split2_test_roc_auc                                                       0.70354   \n",
       "mean_test_roc_auc                                                         0.70565   \n",
       "std_test_roc_auc                                                         0.023785   \n",
       "rank_test_roc_auc                                                             104   \n",
       "split0_test_recall_weighted                                              0.843284   \n",
       "split1_test_recall_weighted                                              0.850746   \n",
       "split2_test_recall_weighted                                              0.858209   \n",
       "mean_test_recall_weighted                                                0.850746   \n",
       "std_test_recall_weighted                                                 0.006093   \n",
       "rank_test_recall_weighted                                                       4   \n",
       "\n",
       "                                                                              402  \\\n",
       "mean_fit_time                                                            0.021824   \n",
       "std_fit_time                                                              0.00047   \n",
       "mean_score_time                                                          0.013018   \n",
       "std_score_time                                                           0.000712   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__max_depth                                                            4   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.3   \n",
       "split1_test_recall                                                       0.380952   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.401587   \n",
       "std_test_recall                                                          0.092528   \n",
       "rank_test_recall                                                              386   \n",
       "split0_test_precision                                                        0.75   \n",
       "split1_test_precision                                                         0.5   \n",
       "split2_test_precision                                                    0.423077   \n",
       "mean_test_precision                                                      0.557692   \n",
       "std_test_precision                                                       0.139561   \n",
       "rank_test_precision                                                             5   \n",
       "split0_test_f1                                                           0.428571   \n",
       "split1_test_f1                                                           0.432432   \n",
       "split2_test_f1                                                           0.468085   \n",
       "mean_test_f1                                                              0.44303   \n",
       "std_test_f1                                                              0.017787   \n",
       "rank_test_f1                                                                   13   \n",
       "split0_test_roc_auc                                                      0.741667   \n",
       "split1_test_roc_auc                                                      0.702276   \n",
       "split2_test_roc_auc                                                      0.774968   \n",
       "mean_test_roc_auc                                                        0.739637   \n",
       "std_test_roc_auc                                                         0.029711   \n",
       "rank_test_roc_auc                                                               8   \n",
       "split0_test_recall_weighted                                              0.880597   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.813433   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.027476   \n",
       "rank_test_recall_weighted                                                       7   \n",
       "\n",
       "                                                                              445  \\\n",
       "mean_fit_time                                                            0.022539   \n",
       "std_fit_time                                                              0.00189   \n",
       "mean_score_time                                                          0.012991   \n",
       "std_score_time                                                           0.001169   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__max_depth                                                            3   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.4   \n",
       "split1_test_recall                                                       0.380952   \n",
       "split2_test_recall                                                       0.571429   \n",
       "mean_test_recall                                                         0.450794   \n",
       "std_test_recall                                                          0.085655   \n",
       "rank_test_recall                                                              318   \n",
       "split0_test_precision                                                    0.727273   \n",
       "split1_test_precision                                                         0.4   \n",
       "split2_test_precision                                                    0.333333   \n",
       "mean_test_precision                                                      0.486869   \n",
       "std_test_precision                                                       0.172156   \n",
       "rank_test_precision                                                            24   \n",
       "split0_test_f1                                                           0.516129   \n",
       "split1_test_f1                                                           0.390244   \n",
       "split2_test_f1                                                           0.421053   \n",
       "mean_test_f1                                                             0.442475   \n",
       "std_test_f1                                                              0.053578   \n",
       "rank_test_f1                                                                   14   \n",
       "split0_test_roc_auc                                                      0.787061   \n",
       "split1_test_roc_auc                                                      0.693426   \n",
       "split2_test_roc_auc                                                      0.731563   \n",
       "mean_test_roc_auc                                                         0.73735   \n",
       "std_test_roc_auc                                                         0.038445   \n",
       "rank_test_roc_auc                                                              13   \n",
       "split0_test_recall_weighted                                               0.88806   \n",
       "split1_test_recall_weighted                                              0.813433   \n",
       "split2_test_recall_weighted                                              0.753731   \n",
       "mean_test_recall_weighted                                                0.818408   \n",
       "std_test_recall_weighted                                                 0.054952   \n",
       "rank_test_recall_weighted                                                      45   \n",
       "\n",
       "                                                                              374  \\\n",
       "mean_fit_time                                                             0.02064   \n",
       "std_fit_time                                                             0.000294   \n",
       "mean_score_time                                                          0.012137   \n",
       "std_score_time                                                           0.000175   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__max_depth                                                            3   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.4   \n",
       "split1_test_recall                                                       0.619048   \n",
       "split2_test_recall                                                       0.619048   \n",
       "mean_test_recall                                                         0.546032   \n",
       "std_test_recall                                                           0.10326   \n",
       "rank_test_recall                                                              182   \n",
       "split0_test_precision                                                    0.571429   \n",
       "split1_test_precision                                                    0.302326   \n",
       "split2_test_precision                                                    0.351351   \n",
       "mean_test_precision                                                      0.408369   \n",
       "std_test_precision                                                       0.117025   \n",
       "rank_test_precision                                                            88   \n",
       "split0_test_f1                                                           0.470588   \n",
       "split1_test_f1                                                            0.40625   \n",
       "split2_test_f1                                                           0.448276   \n",
       "mean_test_f1                                                             0.441705   \n",
       "std_test_f1                                                              0.026674   \n",
       "rank_test_f1                                                                   15   \n",
       "split0_test_roc_auc                                                      0.756798   \n",
       "split1_test_roc_auc                                                      0.738517   \n",
       "split2_test_roc_auc                                                      0.786768   \n",
       "mean_test_roc_auc                                                        0.760694   \n",
       "std_test_roc_auc                                                          0.01989   \n",
       "rank_test_roc_auc                                                               2   \n",
       "split0_test_recall_weighted                                              0.865672   \n",
       "split1_test_recall_weighted                                              0.716418   \n",
       "split2_test_recall_weighted                                              0.761194   \n",
       "mean_test_recall_weighted                                                0.781095   \n",
       "std_test_recall_weighted                                                 0.062536   \n",
       "rank_test_recall_weighted                                                     127   \n",
       "\n",
       "                                                                              301  \\\n",
       "mean_fit_time                                                             0.02078   \n",
       "std_fit_time                                                             0.000609   \n",
       "mean_score_time                                                           0.01357   \n",
       "std_score_time                                                           0.002255   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__max_depth                                                            4   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.35   \n",
       "split1_test_recall                                                        0.52381   \n",
       "split2_test_recall                                                       0.714286   \n",
       "mean_test_recall                                                         0.529365   \n",
       "std_test_recall                                                          0.148771   \n",
       "rank_test_recall                                                              204   \n",
       "split0_test_precision                                                    0.466667   \n",
       "split1_test_precision                                                        0.55   \n",
       "split2_test_precision                                                    0.263158   \n",
       "mean_test_precision                                                      0.426608   \n",
       "std_test_precision                                                        0.12048   \n",
       "rank_test_precision                                                            64   \n",
       "split0_test_f1                                                                0.4   \n",
       "split1_test_f1                                                           0.536585   \n",
       "split2_test_f1                                                           0.384615   \n",
       "mean_test_f1                                                               0.4404   \n",
       "std_test_f1                                                              0.068303   \n",
       "rank_test_f1                                                                   16   \n",
       "split0_test_roc_auc                                                      0.688816   \n",
       "split1_test_roc_auc                                                      0.699536   \n",
       "split2_test_roc_auc                                                      0.714707   \n",
       "mean_test_roc_auc                                                         0.70102   \n",
       "std_test_roc_auc                                                         0.010622   \n",
       "rank_test_roc_auc                                                             129   \n",
       "split0_test_recall_weighted                                              0.843284   \n",
       "split1_test_recall_weighted                                              0.858209   \n",
       "split2_test_recall_weighted                                              0.641791   \n",
       "mean_test_recall_weighted                                                0.781095   \n",
       "std_test_recall_weighted                                                 0.098691   \n",
       "rank_test_recall_weighted                                                     127   \n",
       "\n",
       "                                                                              400  \\\n",
       "mean_fit_time                                                            0.021013   \n",
       "std_fit_time                                                             0.000513   \n",
       "mean_score_time                                                          0.012203   \n",
       "std_score_time                                                           0.000339   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__max_depth                                                            4   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.4   \n",
       "split1_test_recall                                                       0.571429   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.498413   \n",
       "std_test_recall                                                          0.072253   \n",
       "rank_test_recall                                                              237   \n",
       "split0_test_precision                                                    0.727273   \n",
       "split1_test_precision                                                         0.3   \n",
       "split2_test_precision                                                    0.333333   \n",
       "mean_test_precision                                                      0.453535   \n",
       "std_test_precision                                                       0.194039   \n",
       "rank_test_precision                                                            41   \n",
       "split0_test_f1                                                           0.516129   \n",
       "split1_test_f1                                                           0.393443   \n",
       "split2_test_f1                                                           0.407407   \n",
       "mean_test_f1                                                             0.438993   \n",
       "std_test_f1                                                              0.054841   \n",
       "rank_test_f1                                                                   17   \n",
       "split0_test_roc_auc                                                      0.695833   \n",
       "split1_test_roc_auc                                                      0.734092   \n",
       "split2_test_roc_auc                                                      0.704172   \n",
       "mean_test_roc_auc                                                        0.711366   \n",
       "std_test_roc_auc                                                         0.016426   \n",
       "rank_test_roc_auc                                                              83   \n",
       "split0_test_recall_weighted                                               0.88806   \n",
       "split1_test_recall_weighted                                              0.723881   \n",
       "split2_test_recall_weighted                                              0.761194   \n",
       "mean_test_recall_weighted                                                0.791045   \n",
       "std_test_recall_weighted                                                 0.070271   \n",
       "rank_test_recall_weighted                                                     101   \n",
       "\n",
       "                                ...  \\\n",
       "mean_fit_time                   ...   \n",
       "std_fit_time                    ...   \n",
       "mean_score_time                 ...   \n",
       "std_score_time                  ...   \n",
       "param_clf__class_weight         ...   \n",
       "param_clf__criterion            ...   \n",
       "param_clf__max_depth            ...   \n",
       "param_smt__k_neighbors          ...   \n",
       "param_smt__sampling_strategy    ...   \n",
       "param_under__sampling_strategy  ...   \n",
       "params                          ...   \n",
       "split0_test_recall              ...   \n",
       "split1_test_recall              ...   \n",
       "split2_test_recall              ...   \n",
       "mean_test_recall                ...   \n",
       "std_test_recall                 ...   \n",
       "rank_test_recall                ...   \n",
       "split0_test_precision           ...   \n",
       "split1_test_precision           ...   \n",
       "split2_test_precision           ...   \n",
       "mean_test_precision             ...   \n",
       "std_test_precision              ...   \n",
       "rank_test_precision             ...   \n",
       "split0_test_f1                  ...   \n",
       "split1_test_f1                  ...   \n",
       "split2_test_f1                  ...   \n",
       "mean_test_f1                    ...   \n",
       "std_test_f1                     ...   \n",
       "rank_test_f1                    ...   \n",
       "split0_test_roc_auc             ...   \n",
       "split1_test_roc_auc             ...   \n",
       "split2_test_roc_auc             ...   \n",
       "mean_test_roc_auc               ...   \n",
       "std_test_roc_auc                ...   \n",
       "rank_test_roc_auc               ...   \n",
       "split0_test_recall_weighted     ...   \n",
       "split1_test_recall_weighted     ...   \n",
       "split2_test_recall_weighted     ...   \n",
       "mean_test_recall_weighted       ...   \n",
       "std_test_recall_weighted        ...   \n",
       "rank_test_recall_weighted       ...   \n",
       "\n",
       "                                                                              429  \\\n",
       "mean_fit_time                                                            0.020868   \n",
       "std_fit_time                                                             0.001068   \n",
       "mean_score_time                                                          0.012176   \n",
       "std_score_time                                                           0.000229   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__max_depth                                                            2   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.2   \n",
       "split1_test_recall                                                       0.619048   \n",
       "split2_test_recall                                                       0.190476   \n",
       "mean_test_recall                                                         0.336508   \n",
       "std_test_recall                                                          0.199824   \n",
       "rank_test_recall                                                              454   \n",
       "split0_test_precision                                                    0.666667   \n",
       "split1_test_precision                                                        0.26   \n",
       "split2_test_precision                                                        0.25   \n",
       "mean_test_precision                                                      0.392222   \n",
       "std_test_precision                                                       0.194104   \n",
       "rank_test_precision                                                           110   \n",
       "split0_test_f1                                                           0.307692   \n",
       "split1_test_f1                                                           0.366197   \n",
       "split2_test_f1                                                           0.216216   \n",
       "mean_test_f1                                                             0.296702   \n",
       "std_test_f1                                                              0.061721   \n",
       "rank_test_f1                                                                  470   \n",
       "split0_test_roc_auc                                                      0.747807   \n",
       "split1_test_roc_auc                                                      0.686262   \n",
       "split2_test_roc_auc                                                      0.613991   \n",
       "mean_test_roc_auc                                                        0.682687   \n",
       "std_test_roc_auc                                                         0.054689   \n",
       "rank_test_roc_auc                                                             254   \n",
       "split0_test_recall_weighted                                              0.865672   \n",
       "split1_test_recall_weighted                                              0.664179   \n",
       "split2_test_recall_weighted                                              0.783582   \n",
       "mean_test_recall_weighted                                                0.771144   \n",
       "std_test_recall_weighted                                                 0.082728   \n",
       "rank_test_recall_weighted                                                     155   \n",
       "\n",
       "                                                                              266  \\\n",
       "mean_fit_time                                                            0.020386   \n",
       "std_fit_time                                                             0.000461   \n",
       "mean_score_time                                                          0.013284   \n",
       "std_score_time                                                           0.000759   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__max_depth                                                            2   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.2   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                       0.285714   \n",
       "mean_test_recall                                                         0.304762   \n",
       "std_test_recall                                                          0.094281   \n",
       "rank_test_recall                                                              469   \n",
       "split0_test_precision                                                    0.307692   \n",
       "split1_test_precision                                                        0.36   \n",
       "split2_test_precision                                                    0.230769   \n",
       "mean_test_precision                                                      0.299487   \n",
       "std_test_precision                                                       0.053076   \n",
       "rank_test_precision                                                           276   \n",
       "split0_test_f1                                                           0.242424   \n",
       "split1_test_f1                                                           0.391304   \n",
       "split2_test_f1                                                           0.255319   \n",
       "mean_test_f1                                                             0.296349   \n",
       "std_test_f1                                                              0.067349   \n",
       "rank_test_f1                                                                  471   \n",
       "split0_test_roc_auc                                                      0.677632   \n",
       "split1_test_roc_auc                                                      0.690265   \n",
       "split2_test_roc_auc                                                      0.655289   \n",
       "mean_test_roc_auc                                                        0.674395   \n",
       "std_test_roc_auc                                                         0.014461   \n",
       "rank_test_roc_auc                                                             310   \n",
       "split0_test_recall_weighted                                              0.813433   \n",
       "split1_test_recall_weighted                                              0.791045   \n",
       "split2_test_recall_weighted                                              0.738806   \n",
       "mean_test_recall_weighted                                                0.781095   \n",
       "std_test_recall_weighted                                                 0.031268   \n",
       "rank_test_recall_weighted                                                     127   \n",
       "\n",
       "                                                                              375  \\\n",
       "mean_fit_time                                                            0.023925   \n",
       "std_fit_time                                                             0.001205   \n",
       "mean_score_time                                                          0.013128   \n",
       "std_score_time                                                           0.000642   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__max_depth                                                            3   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.3   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                       0.238095   \n",
       "mean_test_recall                                                         0.322222   \n",
       "std_test_recall                                                          0.079333   \n",
       "rank_test_recall                                                              461   \n",
       "split0_test_precision                                                    0.230769   \n",
       "split1_test_precision                                                    0.290323   \n",
       "split2_test_precision                                                      0.3125   \n",
       "mean_test_precision                                                      0.277864   \n",
       "std_test_precision                                                        0.03451   \n",
       "rank_test_precision                                                           356   \n",
       "split0_test_f1                                                            0.26087   \n",
       "split1_test_f1                                                           0.346154   \n",
       "split2_test_f1                                                            0.27027   \n",
       "mean_test_f1                                                             0.292431   \n",
       "std_test_f1                                                              0.038181   \n",
       "rank_test_f1                                                                  472   \n",
       "split0_test_roc_auc                                                      0.632456   \n",
       "split1_test_roc_auc                                                      0.698272   \n",
       "split2_test_roc_auc                                                      0.720396   \n",
       "mean_test_roc_auc                                                        0.683708   \n",
       "std_test_roc_auc                                                         0.037349   \n",
       "rank_test_roc_auc                                                             251   \n",
       "split0_test_recall_weighted                                              0.746269   \n",
       "split1_test_recall_weighted                                              0.746269   \n",
       "split2_test_recall_weighted                                              0.798507   \n",
       "mean_test_recall_weighted                                                0.763682   \n",
       "std_test_recall_weighted                                                 0.024626   \n",
       "rank_test_recall_weighted                                                     176   \n",
       "\n",
       "                                                                              162  \\\n",
       "mean_fit_time                                                            0.021591   \n",
       "std_fit_time                                                             0.001733   \n",
       "mean_score_time                                                          0.012753   \n",
       "std_score_time                                                           0.000638   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__max_depth                                                            2   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                            0.8   \n",
       "split1_test_recall                                                       0.619048   \n",
       "split2_test_recall                                                       0.238095   \n",
       "mean_test_recall                                                         0.552381   \n",
       "std_test_recall                                                           0.23419   \n",
       "rank_test_recall                                                              152   \n",
       "split0_test_precision                                                    0.156863   \n",
       "split1_test_precision                                                    0.276596   \n",
       "split2_test_precision                                                    0.227273   \n",
       "mean_test_precision                                                      0.220244   \n",
       "std_test_precision                                                       0.049133   \n",
       "rank_test_precision                                                           477   \n",
       "split0_test_f1                                                           0.262295   \n",
       "split1_test_f1                                                           0.382353   \n",
       "split2_test_f1                                                           0.232558   \n",
       "mean_test_f1                                                             0.292402   \n",
       "std_test_f1                                                              0.064753   \n",
       "rank_test_f1                                                                  473   \n",
       "split0_test_roc_auc                                                      0.645614   \n",
       "split1_test_roc_auc                                                      0.672988   \n",
       "split2_test_roc_auc                                                      0.631479   \n",
       "mean_test_roc_auc                                                        0.650027   \n",
       "std_test_roc_auc                                                         0.017231   \n",
       "rank_test_roc_auc                                                             422   \n",
       "split0_test_recall_weighted                                              0.328358   \n",
       "split1_test_recall_weighted                                              0.686567   \n",
       "split2_test_recall_weighted                                              0.753731   \n",
       "mean_test_recall_weighted                                                0.589552   \n",
       "std_test_recall_weighted                                                 0.186716   \n",
       "rank_test_recall_weighted                                                     475   \n",
       "\n",
       "                                                                              340  \\\n",
       "mean_fit_time                                                            0.022992   \n",
       "std_fit_time                                                              0.00337   \n",
       "mean_score_time                                                          0.012827   \n",
       "std_score_time                                                           0.000407   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__max_depth                                                            2   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.2   \n",
       "split1_test_recall                                                       0.619048   \n",
       "split2_test_recall                                                       0.333333   \n",
       "mean_test_recall                                                         0.384127   \n",
       "std_test_recall                                                          0.174805   \n",
       "rank_test_recall                                                              409   \n",
       "split0_test_precision                                                    0.266667   \n",
       "split1_test_precision                                                    0.265306   \n",
       "split2_test_precision                                                    0.233333   \n",
       "mean_test_precision                                                      0.255102   \n",
       "std_test_precision                                                       0.015403   \n",
       "rank_test_precision                                                           423   \n",
       "split0_test_f1                                                           0.228571   \n",
       "split1_test_f1                                                           0.371429   \n",
       "split2_test_f1                                                            0.27451   \n",
       "mean_test_f1                                                             0.291503   \n",
       "std_test_f1                                                              0.059546   \n",
       "rank_test_f1                                                                  474   \n",
       "split0_test_roc_auc                                                      0.691228   \n",
       "split1_test_roc_auc                                                      0.681837   \n",
       "split2_test_roc_auc                                                      0.645807   \n",
       "mean_test_roc_auc                                                        0.672957   \n",
       "std_test_roc_auc                                                         0.019577   \n",
       "rank_test_roc_auc                                                             318   \n",
       "split0_test_recall_weighted                                              0.798507   \n",
       "split1_test_recall_weighted                                              0.671642   \n",
       "split2_test_recall_weighted                                              0.723881   \n",
       "mean_test_recall_weighted                                                0.731343   \n",
       "std_test_recall_weighted                                                 0.052061   \n",
       "rank_test_recall_weighted                                                     252   \n",
       "\n",
       "                                                                              154  \\\n",
       "mean_fit_time                                                            0.020546   \n",
       "std_fit_time                                                             0.000461   \n",
       "mean_score_time                                                          0.012099   \n",
       "std_score_time                                                           0.000213   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__max_depth                                                            4   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                          0.46746   \n",
       "std_test_recall                                                          0.040794   \n",
       "rank_test_recall                                                              285   \n",
       "split0_test_precision                                                        0.15   \n",
       "split1_test_precision                                                     0.28125   \n",
       "split2_test_precision                                                    0.215686   \n",
       "mean_test_precision                                                      0.215645   \n",
       "std_test_precision                                                       0.053583   \n",
       "rank_test_precision                                                           479   \n",
       "split0_test_f1                                                              0.225   \n",
       "split1_test_f1                                                           0.339623   \n",
       "split2_test_f1                                                           0.305556   \n",
       "mean_test_f1                                                             0.290059   \n",
       "std_test_f1                                                               0.04806   \n",
       "rank_test_f1                                                                  475   \n",
       "split0_test_roc_auc                                                      0.487061   \n",
       "split1_test_roc_auc                                                      0.665824   \n",
       "split2_test_roc_auc                                                      0.624315   \n",
       "mean_test_roc_auc                                                          0.5924   \n",
       "std_test_roc_auc                                                         0.076389   \n",
       "rank_test_roc_auc                                                             484   \n",
       "split0_test_recall_weighted                                              0.537313   \n",
       "split1_test_recall_weighted                                              0.738806   \n",
       "split2_test_recall_weighted                                              0.626866   \n",
       "mean_test_recall_weighted                                                0.634328   \n",
       "std_test_recall_weighted                                                 0.082428   \n",
       "rank_test_recall_weighted                                                     443   \n",
       "\n",
       "                                                                              417  \\\n",
       "mean_fit_time                                                            0.021112   \n",
       "std_fit_time                                                             0.000851   \n",
       "mean_score_time                                                          0.012108   \n",
       "std_score_time                                                           0.000049   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__max_depth                                                            2   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.35   \n",
       "split1_test_recall                                                       0.571429   \n",
       "split2_test_recall                                                       0.238095   \n",
       "mean_test_recall                                                         0.386508   \n",
       "std_test_recall                                                           0.13851   \n",
       "rank_test_recall                                                              404   \n",
       "split0_test_precision                                                    0.205882   \n",
       "split1_test_precision                                                    0.266667   \n",
       "split2_test_precision                                                    0.238095   \n",
       "mean_test_precision                                                      0.236881   \n",
       "std_test_precision                                                        0.02483   \n",
       "rank_test_precision                                                           465   \n",
       "split0_test_f1                                                           0.259259   \n",
       "split1_test_f1                                                           0.363636   \n",
       "split2_test_f1                                                           0.238095   \n",
       "mean_test_f1                                                             0.286997   \n",
       "std_test_f1                                                              0.054877   \n",
       "rank_test_f1                                                                  476   \n",
       "split0_test_roc_auc                                                      0.655702   \n",
       "split1_test_roc_auc                                                       0.66772   \n",
       "split2_test_roc_auc                                                      0.649178   \n",
       "mean_test_roc_auc                                                        0.657533   \n",
       "std_test_roc_auc                                                          0.00768   \n",
       "rank_test_roc_auc                                                             395   \n",
       "split0_test_recall_weighted                                              0.701493   \n",
       "split1_test_recall_weighted                                              0.686567   \n",
       "split2_test_recall_weighted                                              0.761194   \n",
       "mean_test_recall_weighted                                                0.716418   \n",
       "std_test_recall_weighted                                                 0.032242   \n",
       "rank_test_recall_weighted                                                     284   \n",
       "\n",
       "                                                                              342  \\\n",
       "mean_fit_time                                                            0.019733   \n",
       "std_fit_time                                                             0.000062   \n",
       "mean_score_time                                                          0.012034   \n",
       "std_score_time                                                            0.00012   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__max_depth                                                            2   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.2   \n",
       "split1_test_recall                                                        0.52381   \n",
       "split2_test_recall                                                       0.190476   \n",
       "mean_test_recall                                                         0.304762   \n",
       "std_test_recall                                                          0.154939   \n",
       "rank_test_recall                                                              468   \n",
       "split0_test_precision                                                    0.444444   \n",
       "split1_test_precision                                                       0.275   \n",
       "split2_test_precision                                                         0.2   \n",
       "mean_test_precision                                                      0.306481   \n",
       "std_test_precision                                                       0.102247   \n",
       "rank_test_precision                                                           257   \n",
       "split0_test_f1                                                           0.275862   \n",
       "split1_test_f1                                                           0.360656   \n",
       "split2_test_f1                                                           0.195122   \n",
       "mean_test_f1                                                             0.277213   \n",
       "std_test_f1                                                              0.067586   \n",
       "rank_test_f1                                                                  477   \n",
       "split0_test_roc_auc                                                       0.57807   \n",
       "split1_test_roc_auc                                                      0.694269   \n",
       "split2_test_roc_auc                                                       0.62579   \n",
       "mean_test_roc_auc                                                         0.63271   \n",
       "std_test_roc_auc                                                          0.04769   \n",
       "rank_test_roc_auc                                                             454   \n",
       "split0_test_recall_weighted                                              0.843284   \n",
       "split1_test_recall_weighted                                              0.708955   \n",
       "split2_test_recall_weighted                                              0.753731   \n",
       "mean_test_recall_weighted                                                0.768657   \n",
       "std_test_recall_weighted                                                 0.055846   \n",
       "rank_test_recall_weighted                                                     163   \n",
       "\n",
       "                                                                              389  \\\n",
       "mean_fit_time                                                            0.021035   \n",
       "std_fit_time                                                             0.000702   \n",
       "mean_score_time                                                          0.012377   \n",
       "std_score_time                                                           0.000329   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__max_depth                                                            4   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.75   \n",
       "split1_test_recall                                                       0.380952   \n",
       "split2_test_recall                                                       0.285714   \n",
       "mean_test_recall                                                         0.472222   \n",
       "std_test_recall                                                           0.20023   \n",
       "rank_test_recall                                                              273   \n",
       "split0_test_precision                                                    0.217391   \n",
       "split1_test_precision                                                        0.25   \n",
       "split2_test_precision                                                    0.139535   \n",
       "mean_test_precision                                                      0.202309   \n",
       "std_test_precision                                                       0.046341   \n",
       "rank_test_precision                                                           484   \n",
       "split0_test_f1                                                           0.337079   \n",
       "split1_test_f1                                                           0.301887   \n",
       "split2_test_f1                                                             0.1875   \n",
       "mean_test_f1                                                             0.275488   \n",
       "std_test_f1                                                              0.063855   \n",
       "rank_test_f1                                                                  478   \n",
       "split0_test_roc_auc                                                         0.675   \n",
       "split1_test_roc_auc                                                      0.565529   \n",
       "split2_test_roc_auc                                                      0.578803   \n",
       "mean_test_roc_auc                                                        0.606444   \n",
       "std_test_roc_auc                                                         0.048778   \n",
       "rank_test_roc_auc                                                             480   \n",
       "split0_test_recall_weighted                                              0.559701   \n",
       "split1_test_recall_weighted                                              0.723881   \n",
       "split2_test_recall_weighted                                               0.61194   \n",
       "mean_test_recall_weighted                                                0.631841   \n",
       "std_test_recall_weighted                                                 0.068487   \n",
       "rank_test_recall_weighted                                                     447   \n",
       "\n",
       "                                                                              230  \\\n",
       "mean_fit_time                                                            0.020951   \n",
       "std_fit_time                                                             0.000215   \n",
       "mean_score_time                                                          0.012216   \n",
       "std_score_time                                                           0.000043   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__max_depth                                                            4   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                       0.380952   \n",
       "mean_test_recall                                                         0.452381   \n",
       "std_test_recall                                                          0.051434   \n",
       "rank_test_recall                                                              310   \n",
       "split0_test_precision                                                    0.192308   \n",
       "split1_test_precision                                                    0.232558   \n",
       "split2_test_precision                                                    0.163265   \n",
       "mean_test_precision                                                      0.196044   \n",
       "std_test_precision                                                       0.028412   \n",
       "rank_test_precision                                                           486   \n",
       "split0_test_f1                                                           0.277778   \n",
       "split1_test_f1                                                             0.3125   \n",
       "split2_test_f1                                                           0.228571   \n",
       "mean_test_f1                                                              0.27295   \n",
       "std_test_f1                                                              0.034433   \n",
       "rank_test_f1                                                                  479   \n",
       "split0_test_roc_auc                                                      0.585526   \n",
       "split1_test_roc_auc                                                      0.667299   \n",
       "split2_test_roc_auc                                                      0.514749   \n",
       "mean_test_roc_auc                                                        0.589191   \n",
       "std_test_roc_auc                                                         0.062332   \n",
       "rank_test_roc_auc                                                             485   \n",
       "split0_test_recall_weighted                                               0.61194   \n",
       "split1_test_recall_weighted                                              0.671642   \n",
       "split2_test_recall_weighted                                              0.597015   \n",
       "mean_test_recall_weighted                                                0.626866   \n",
       "std_test_recall_weighted                                                 0.032242   \n",
       "rank_test_recall_weighted                                                     454   \n",
       "\n",
       "                                                                              416  \\\n",
       "mean_fit_time                                                            0.020766   \n",
       "std_fit_time                                                              0.00146   \n",
       "mean_score_time                                                          0.012789   \n",
       "std_score_time                                                           0.001058   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__max_depth                                                            2   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.1   \n",
       "split1_test_recall                                                       0.619048   \n",
       "split2_test_recall                                                       0.190476   \n",
       "mean_test_recall                                                         0.303175   \n",
       "std_test_recall                                                          0.226389   \n",
       "rank_test_recall                                                              472   \n",
       "split0_test_precision                                                    0.666667   \n",
       "split1_test_precision                                                    0.295455   \n",
       "split2_test_precision                                                    0.333333   \n",
       "mean_test_precision                                                      0.431818   \n",
       "std_test_precision                                                       0.166781   \n",
       "rank_test_precision                                                            59   \n",
       "split0_test_f1                                                           0.173913   \n",
       "split1_test_f1                                                                0.4   \n",
       "split2_test_f1                                                           0.242424   \n",
       "mean_test_f1                                                             0.272112   \n",
       "std_test_f1                                                              0.094657   \n",
       "rank_test_f1                                                                  480   \n",
       "split0_test_roc_auc                                                      0.558772   \n",
       "split1_test_roc_auc                                                      0.692373   \n",
       "split2_test_roc_auc                                                      0.640329   \n",
       "mean_test_roc_auc                                                        0.630491   \n",
       "std_test_roc_auc                                                         0.054984   \n",
       "rank_test_roc_auc                                                             458   \n",
       "split0_test_recall_weighted                                              0.858209   \n",
       "split1_test_recall_weighted                                              0.708955   \n",
       "split2_test_recall_weighted                                              0.813433   \n",
       "mean_test_recall_weighted                                                0.793532   \n",
       "std_test_recall_weighted                                                 0.062536   \n",
       "rank_test_recall_weighted                                                      94   \n",
       "\n",
       "                                                                              363  \\\n",
       "mean_fit_time                                                            0.020403   \n",
       "std_fit_time                                                             0.000035   \n",
       "mean_score_time                                                          0.011802   \n",
       "std_score_time                                                           0.000177   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__max_depth                                                            3   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.2   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                       0.190476   \n",
       "mean_test_recall                                                         0.288889   \n",
       "std_test_recall                                                          0.132499   \n",
       "rank_test_recall                                                              476   \n",
       "split0_test_precision                                                         0.5   \n",
       "split1_test_precision                                                    0.263158   \n",
       "split2_test_precision                                                    0.181818   \n",
       "mean_test_precision                                                      0.314992   \n",
       "std_test_precision                                                       0.134969   \n",
       "rank_test_precision                                                           239   \n",
       "split0_test_f1                                                           0.285714   \n",
       "split1_test_f1                                                           0.338983   \n",
       "split2_test_f1                                                           0.186047   \n",
       "mean_test_f1                                                             0.270248   \n",
       "std_test_f1                                                              0.063387   \n",
       "rank_test_f1                                                                  481   \n",
       "split0_test_roc_auc                                                      0.725439   \n",
       "split1_test_roc_auc                                                      0.598188   \n",
       "split2_test_roc_auc                                                      0.636115   \n",
       "mean_test_roc_auc                                                        0.653247   \n",
       "std_test_roc_auc                                                         0.053344   \n",
       "rank_test_roc_auc                                                             413   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.708955   \n",
       "split2_test_recall_weighted                                              0.738806   \n",
       "mean_test_recall_weighted                                                0.766169   \n",
       "std_test_recall_weighted                                                 0.061034   \n",
       "rank_test_recall_weighted                                                     170   \n",
       "\n",
       "                                                                              4    \\\n",
       "mean_fit_time                                                             0.01508   \n",
       "std_fit_time                                                             0.002098   \n",
       "mean_score_time                                                          0.011953   \n",
       "std_score_time                                                           0.000942   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__max_depth                                                            2   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                           0.25   \n",
       "split1_test_recall                                                       0.619048   \n",
       "split2_test_recall                                                       0.333333   \n",
       "mean_test_recall                                                         0.400794   \n",
       "std_test_recall                                                          0.158034   \n",
       "rank_test_recall                                                              390   \n",
       "split0_test_precision                                                    0.142857   \n",
       "split1_test_precision                                                    0.245283   \n",
       "split2_test_precision                                                    0.233333   \n",
       "mean_test_precision                                                      0.207158   \n",
       "std_test_precision                                                       0.045728   \n",
       "rank_test_precision                                                           483   \n",
       "split0_test_f1                                                           0.181818   \n",
       "split1_test_f1                                                           0.351351   \n",
       "split2_test_f1                                                            0.27451   \n",
       "mean_test_f1                                                             0.269226   \n",
       "std_test_f1                                                              0.069312   \n",
       "rank_test_f1                                                                  482   \n",
       "split0_test_roc_auc                                                      0.622368   \n",
       "split1_test_roc_auc                                                      0.655921   \n",
       "split2_test_roc_auc                                                      0.650653   \n",
       "mean_test_roc_auc                                                        0.642981   \n",
       "std_test_roc_auc                                                         0.014733   \n",
       "rank_test_roc_auc                                                             437   \n",
       "split0_test_recall_weighted                                              0.664179   \n",
       "split1_test_recall_weighted                                              0.641791   \n",
       "split2_test_recall_weighted                                              0.723881   \n",
       "mean_test_recall_weighted                                                0.676617   \n",
       "std_test_recall_weighted                                                 0.034648   \n",
       "rank_test_recall_weighted                                                     370   \n",
       "\n",
       "                                                                              440  \\\n",
       "mean_fit_time                                                            0.020732   \n",
       "std_fit_time                                                             0.000227   \n",
       "mean_score_time                                                           0.01212   \n",
       "std_score_time                                                           0.000144   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__max_depth                                                            3   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                       0.190476   \n",
       "mean_test_recall                                                         0.388889   \n",
       "std_test_recall                                                          0.140635   \n",
       "rank_test_recall                                                              392   \n",
       "split0_test_precision                                                    0.131579   \n",
       "split1_test_precision                                                    0.263158   \n",
       "split2_test_precision                                                    0.363636   \n",
       "mean_test_precision                                                      0.252791   \n",
       "std_test_precision                                                        0.09502   \n",
       "rank_test_precision                                                           433   \n",
       "split0_test_f1                                                           0.208333   \n",
       "split1_test_f1                                                           0.338983   \n",
       "split2_test_f1                                                               0.25   \n",
       "mean_test_f1                                                             0.265772   \n",
       "std_test_f1                                                              0.054491   \n",
       "rank_test_f1                                                                  483   \n",
       "split0_test_roc_auc                                                      0.501316   \n",
       "split1_test_roc_auc                                                       0.67067   \n",
       "split2_test_roc_auc                                                      0.668563   \n",
       "mean_test_roc_auc                                                        0.613516   \n",
       "std_test_roc_auc                                                         0.079342   \n",
       "rank_test_roc_auc                                                             478   \n",
       "split0_test_recall_weighted                                              0.432836   \n",
       "split1_test_recall_weighted                                              0.708955   \n",
       "split2_test_recall_weighted                                              0.820896   \n",
       "mean_test_recall_weighted                                                0.654229   \n",
       "std_test_recall_weighted                                                 0.163082   \n",
       "rank_test_recall_weighted                                                     418   \n",
       "\n",
       "                                                                              328  \\\n",
       "mean_fit_time                                                            0.020629   \n",
       "std_fit_time                                                             0.000936   \n",
       "mean_score_time                                                          0.012272   \n",
       "std_score_time                                                           0.000072   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__max_depth                                                            2   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.1   \n",
       "split1_test_recall                                                       0.619048   \n",
       "split2_test_recall                                                       0.190476   \n",
       "mean_test_recall                                                         0.303175   \n",
       "std_test_recall                                                          0.226389   \n",
       "rank_test_recall                                                              472   \n",
       "split0_test_precision                                                    0.666667   \n",
       "split1_test_precision                                                    0.254902   \n",
       "split2_test_precision                                                         0.4   \n",
       "mean_test_precision                                                      0.440523   \n",
       "std_test_precision                                                       0.170527   \n",
       "rank_test_precision                                                            46   \n",
       "split0_test_f1                                                           0.173913   \n",
       "split1_test_f1                                                           0.361111   \n",
       "split2_test_f1                                                           0.258065   \n",
       "mean_test_f1                                                             0.264363   \n",
       "std_test_f1                                                              0.076553   \n",
       "rank_test_f1                                                                  484   \n",
       "split0_test_roc_auc                                                      0.590789   \n",
       "split1_test_roc_auc                                                      0.684155   \n",
       "split2_test_roc_auc                                                      0.632533   \n",
       "mean_test_roc_auc                                                        0.635826   \n",
       "std_test_roc_auc                                                         0.038187   \n",
       "rank_test_roc_auc                                                             449   \n",
       "split0_test_recall_weighted                                              0.858209   \n",
       "split1_test_recall_weighted                                              0.656716   \n",
       "split2_test_recall_weighted                                              0.828358   \n",
       "mean_test_recall_weighted                                                0.781095   \n",
       "std_test_recall_weighted                                                 0.088789   \n",
       "rank_test_recall_weighted                                                     127   \n",
       "\n",
       "                                                                              360  \\\n",
       "mean_fit_time                                                            0.020688   \n",
       "std_fit_time                                                             0.000728   \n",
       "mean_score_time                                                          0.012049   \n",
       "std_score_time                                                           0.000757   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__max_depth                                                            3   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.15   \n",
       "split1_test_recall                                                       0.285714   \n",
       "split2_test_recall                                                       0.190476   \n",
       "mean_test_recall                                                          0.20873   \n",
       "std_test_recall                                                          0.056889   \n",
       "rank_test_recall                                                              486   \n",
       "split0_test_precision                                                       0.375   \n",
       "split1_test_precision                                                    0.333333   \n",
       "split2_test_precision                                                    0.444444   \n",
       "mean_test_precision                                                      0.384259   \n",
       "std_test_precision                                                       0.045831   \n",
       "rank_test_precision                                                           121   \n",
       "split0_test_f1                                                           0.214286   \n",
       "split1_test_f1                                                           0.307692   \n",
       "split2_test_f1                                                           0.266667   \n",
       "mean_test_f1                                                             0.262882   \n",
       "std_test_f1                                                              0.038227   \n",
       "rank_test_f1                                                                  485   \n",
       "split0_test_roc_auc                                                        0.5625   \n",
       "split1_test_roc_auc                                                      0.650442   \n",
       "split2_test_roc_auc                                                      0.667299   \n",
       "mean_test_roc_auc                                                        0.626747   \n",
       "std_test_roc_auc                                                         0.045948   \n",
       "rank_test_roc_auc                                                             466   \n",
       "split0_test_recall_weighted                                              0.835821   \n",
       "split1_test_recall_weighted                                              0.798507   \n",
       "split2_test_recall_weighted                                              0.835821   \n",
       "mean_test_recall_weighted                                                0.823383   \n",
       "std_test_recall_weighted                                                  0.01759   \n",
       "rank_test_recall_weighted                                                      34   \n",
       "\n",
       "                                                                              339  \n",
       "mean_fit_time                                                            0.020451  \n",
       "std_fit_time                                                             0.000229  \n",
       "mean_score_time                                                           0.01295  \n",
       "std_score_time                                                           0.001233  \n",
       "param_clf__class_weight                                                      None  \n",
       "param_clf__criterion                                                      entropy  \n",
       "param_clf__max_depth                                                            2  \n",
       "param_smt__k_neighbors                                                          3  \n",
       "param_smt__sampling_strategy                                                  0.4  \n",
       "param_under__sampling_strategy                                                0.5  \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...  \n",
       "split0_test_recall                                                            0.2  \n",
       "split1_test_recall                                                       0.142857  \n",
       "split2_test_recall                                                       0.428571  \n",
       "mean_test_recall                                                         0.257143  \n",
       "std_test_recall                                                          0.123443  \n",
       "rank_test_recall                                                              479  \n",
       "split0_test_precision                                                    0.266667  \n",
       "split1_test_precision                                                         0.6  \n",
       "split2_test_precision                                                    0.264706  \n",
       "mean_test_precision                                                      0.377124  \n",
       "std_test_precision                                                       0.157599  \n",
       "rank_test_precision                                                           132  \n",
       "split0_test_f1                                                           0.228571  \n",
       "split1_test_f1                                                           0.230769  \n",
       "split2_test_f1                                                           0.327273  \n",
       "mean_test_f1                                                             0.262204  \n",
       "std_test_f1                                                              0.046019  \n",
       "rank_test_f1                                                                  486  \n",
       "split0_test_roc_auc                                                      0.691228  \n",
       "split1_test_roc_auc                                                      0.719343  \n",
       "split2_test_roc_auc                                                      0.678677  \n",
       "mean_test_roc_auc                                                        0.696416  \n",
       "std_test_roc_auc                                                         0.017002  \n",
       "rank_test_roc_auc                                                             157  \n",
       "split0_test_recall_weighted                                              0.798507  \n",
       "split1_test_recall_weighted                                              0.850746  \n",
       "split2_test_recall_weighted                                              0.723881  \n",
       "mean_test_recall_weighted                                                0.791045  \n",
       "std_test_recall_weighted                                                 0.052061  \n",
       "rank_test_recall_weighted                                                     101  \n",
       "\n",
       "[41 rows x 486 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "pipeline = IMBLpipeline([\n",
    "    ('smt', SMOTE()), \n",
    "    ('under', RandomUnderSampler()), \n",
    "    ('clf', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "            'clf__max_depth': [2,3,4],\n",
    "            'clf__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "               'clf__class_weight': ['balanced', None],\n",
    "               'smt__sampling_strategy': [ 0.2, 0.3, 0.4],\n",
    "               'under__sampling_strategy': [0.5, 0.6, 0.7],\n",
    "               'smt__k_neighbors': [1, 3, 5]\n",
    "             }\n",
    "nfolds = 3\n",
    "scores = ['recall', 'precision', 'f1', 'roc_auc', 'recall_weighted']\n",
    "gscv_dt = GridSearchCV(pipeline, \n",
    "                    parameters, \n",
    "                    scoring=scores,\n",
    "                    cv=StratifiedKFold(n_splits=nfolds, shuffle=True),\n",
    "                    return_train_score=False, \n",
    "                    verbose=1, \n",
    "                    refit=\"f1\",\n",
    "                    n_jobs=-1)\n",
    "resultsGSCV = gscv_dt.fit(X, y)\n",
    "\n",
    "# Get the classifier name from the pipeline\n",
    "classifier_name = resultsGSCV.best_estimator_.named_steps['clf'].__class__.__name__\n",
    "    \n",
    "# save the best estimator\n",
    "save_best_estimator(gscv_dt, classifier_name, notebook_name)\n",
    "\n",
    "results = pd.DataFrame(resultsGSCV.cv_results_)\n",
    "display(results.sort_values(by=[f'rank_test_f1']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "395678b1-57e4-480f-980d-4106ab700c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.90      0.76      0.82       147\n",
      "         poz       0.27      0.50      0.35        26\n",
      "\n",
      "    accuracy                           0.72       173\n",
      "   macro avg       0.58      0.63      0.58       173\n",
      "weighted avg       0.80      0.72      0.75       173\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on test data\n",
    "dt_clf = gscv_dt.best_estimator_.steps[2][1]\n",
    "\n",
    "# # Load the best estimator from the saved pickle file (replace with acctual file name)\n",
    "# pickle_file_name = \"models/timestamp/classifier_name-notebook_name.pkl\"\n",
    "# dt_clf = load_best_estimator(pickle_file_name).steps[2][1]\n",
    "\n",
    "y_pred = dt_clf.predict(X_eval)\n",
    "DT_classification_report = classification_report(y_eval, y_pred)\n",
    "\n",
    "# Create classification report as dictionary\n",
    "DT_report_dict = classification_report(y_eval, y_pred, output_dict=True)\n",
    "\n",
    "print(classification_report(y_eval, y_pred, target_names=[\"neg\", \"poz\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5024ff2c-4a0c-41da-91cb-8c87eea28b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization saved as: models/11042023_1923/DT_visualisation.svg\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:ns2=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"775.5\" height=\"532.5\" viewBox=\"0.0 0.0 774.75 532.5\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1.5 1.5) rotate(0) translate(4 351)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-351 512.5,-351 512.5,4 -4,4\" />\n",
       "<g id=\"clust10\" class=\"cluster\">\n",
       "<title>cluster_legend</title>\n",
       "</g>\n",
       "\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>node3</title>\n",
       "<text text-anchor=\"start\" x=\"49.5\" y=\"-138.4\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#444443\">salinity@37.67</text>\n",
       "</g>\n",
       "\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>leaf4</title>\n",
       "<polygon fill=\"none\" stroke=\"#444443\" stroke-width=\"0\" points=\"67,-83 0,-83 0,0 67,0 67,-83\" />\n",
       "<svg width=\"59px\" height=\"75px\" viewBox=\"0 0 78.519997 99.516235\" version=\"1.1\" preserveAspectRatio=\"xMinYMin meet\" x=\"4.5\" y=\"-78.5\">\n",
       " <metadata>\n",
       "  <rdf:RDF>\n",
       "   <ns2:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\" />\n",
       "    <dc:date>2023-04-11T19:23:53.260364</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <ns2:Agent>\n",
       "      <dc:title>Matplotlib v3.5.2, https://matplotlib.org/</dc:title>\n",
       "     </ns2:Agent>\n",
       "    </dc:creator>\n",
       "   </ns2:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 99.516235  L 78.519997 99.516235  L 78.519997 0  L 0 0  L 0 99.516235  z \" style=\"fill: none\" />\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 78.519997 39.259998  C 78.519997 35.381929 77.945389 31.525173 76.814914 27.81553  C 75.684439 24.105887 74.010524 20.584134 71.847769 17.365142  C 69.685014 14.146149 67.057191 11.265301 64.050014 8.816564  C 61.042838 6.367828 57.689363 4.378119 54.098972 2.912337  C 50.508581 1.446556 46.720739 0.520814 42.858999 0.165308  C 38.997259 -0.190198 35.104068 0.028438 31.306413 0.814087  C 27.508759 1.599735 23.848385 2.943759 20.444685 4.802319  C 17.040985 6.66088 13.931372 9.013548 11.217275 11.783584  C 8.503178 14.55362 6.214431 17.710577 4.425687 21.15148  C 2.636943 24.592384 1.367864 28.279412 0.659845 32.092302  C -0.048174 35.905193 -0.18735 39.802033 0.246855 43.655718  C 0.681061 47.509404 1.683876 51.277573 3.22259 54.837317  C 4.761304 58.397062 6.819005 61.709253 9.328574 64.665853  C 11.838142 67.622454 14.771994 70.190965 18.034434 72.287608  C 21.296873 74.384252 24.852039 75.98598 28.583969 77.040549  C 32.3159 78.095118 36.183574 78.590935 40.060837 78.511829  L 39.259997 39.259998  L 78.519997 39.259998  z \" style=\"fill: #fefebb; stroke: #444443; stroke-width: 0.5; stroke-linejoin: miter\" />\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 40.060837 78.511829  C 50.328668 78.302339 60.1152 74.07207 67.302717 66.736414  C 74.490234 59.400759 78.519995 49.529973 78.519997 39.260005  L 39.259997 39.259998  L 40.060837 78.511829  z \" style=\"fill: #a1dab4; stroke: #444443; stroke-width: 0.5; stroke-linejoin: miter\" />\n",
       "   </g>\n",
       "   <g id=\"text_1\">\n",
       "    \n",
       "    <g style=\"fill: #444443\" transform=\"translate(26.622029 88.102829)scale(0.09 -0.09)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-6e\" d=\"M 422 0  L 422 3319  L 928 3319  L 928 2847  Q 1294 3394 1984 3394  Q 2284 3394 2536 3286  Q 2788 3178 2913 3003  Q 3038 2828 3088 2588  Q 3119 2431 3119 2041  L 3119 0  L 2556 0  L 2556 2019  Q 2556 2363 2490 2533  Q 2425 2703 2258 2804  Q 2091 2906 1866 2906  Q 1506 2906 1245 2678  Q 984 2450 984 1813  L 984 0  L 422 0  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-3d\" d=\"M 3381 2694  L 356 2694  L 356 3219  L 3381 3219  L 3381 2694  z M 3381 1303  L 356 1303  L 356 1828  L 3381 1828  L 3381 1303  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-31\" d=\"M 2384 0  L 1822 0  L 1822 3584  Q 1619 3391 1289 3197  Q 959 3003 697 2906  L 697 3450  Q 1169 3672 1522 3987  Q 1875 4303 2022 4600  L 2384 4600  L 2384 0  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-36\" d=\"M 3184 3459  L 2625 3416  Q 2550 3747 2413 3897  Q 2184 4138 1850 4138  Q 1581 4138 1378 3988  Q 1113 3794 959 3422  Q 806 3050 800 2363  Q 1003 2672 1297 2822  Q 1591 2972 1913 2972  Q 2475 2972 2870 2558  Q 3266 2144 3266 1488  Q 3266 1056 3080 686  Q 2894 316 2569 119  Q 2244 -78 1831 -78  Q 1128 -78 684 439  Q 241 956 241 2144  Q 241 3472 731 4075  Q 1159 4600 1884 4600  Q 2425 4600 2770 4297  Q 3116 3994 3184 3459  z M 888 1484  Q 888 1194 1011 928  Q 1134 663 1356 523  Q 1578 384 1822 384  Q 2178 384 2434 671  Q 2691 959 2691 1453  Q 2691 1928 2437 2201  Q 2184 2475 1800 2475  Q 1419 2475 1153 2201  Q 888 1928 888 1484  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-32\" d=\"M 3222 541  L 3222 0  L 194 0  Q 188 203 259 391  Q 375 700 629 1000  Q 884 1300 1366 1694  Q 2113 2306 2375 2664  Q 2638 3022 2638 3341  Q 2638 3675 2398 3904  Q 2159 4134 1775 4134  Q 1369 4134 1125 3890  Q 881 3647 878 3216  L 300 3275  Q 359 3922 746 4261  Q 1134 4600 1788 4600  Q 2447 4600 2831 4234  Q 3216 3869 3216 3328  Q 3216 3053 3103 2787  Q 2991 2522 2730 2228  Q 2469 1934 1863 1422  Q 1356 997 1212 845  Q 1069 694 975 541  L 3222 541  z \" transform=\"scale(0.015625)\" />\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-6e\" />\n",
       "     <use xlink:href=\"#ArialMT-3d\" x=\"55.615234\" />\n",
       "     <use xlink:href=\"#ArialMT-31\" x=\"114.013672\" />\n",
       "     <use xlink:href=\"#ArialMT-36\" x=\"169.628906\" />\n",
       "     <use xlink:href=\"#ArialMT-32\" x=\"225.244141\" />\n",
       "    </g>\n",
       "    \n",
       "    <g style=\"fill: #444443\" transform=\"translate(31.752732 97.622017)scale(0.09 -0.09)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-65\" d=\"M 2694 1069  L 3275 997  Q 3138 488 2766 206  Q 2394 -75 1816 -75  Q 1088 -75 661 373  Q 234 822 234 1631  Q 234 2469 665 2931  Q 1097 3394 1784 3394  Q 2450 3394 2872 2941  Q 3294 2488 3294 1666  Q 3294 1616 3291 1516  L 816 1516  Q 847 969 1125 678  Q 1403 388 1819 388  Q 2128 388 2347 550  Q 2566 713 2694 1069  z M 847 1978  L 2700 1978  Q 2663 2397 2488 2606  Q 2219 2931 1791 2931  Q 1403 2931 1139 2672  Q 875 2413 847 1978  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-67\" d=\"M 319 -275  L 866 -356  Q 900 -609 1056 -725  Q 1266 -881 1628 -881  Q 2019 -881 2231 -725  Q 2444 -569 2519 -288  Q 2563 -116 2559 434  Q 2191 0 1641 0  Q 956 0 581 494  Q 206 988 206 1678  Q 206 2153 378 2554  Q 550 2956 876 3175  Q 1203 3394 1644 3394  Q 2231 3394 2613 2919  L 2613 3319  L 3131 3319  L 3131 450  Q 3131 -325 2973 -648  Q 2816 -972 2473 -1159  Q 2131 -1347 1631 -1347  Q 1038 -1347 672 -1080  Q 306 -813 319 -275  z M 784 1719  Q 784 1066 1043 766  Q 1303 466 1694 466  Q 2081 466 2343 764  Q 2606 1063 2606 1700  Q 2606 2309 2336 2618  Q 2066 2928 1684 2928  Q 1309 2928 1046 2623  Q 784 2319 784 1719  z \" transform=\"scale(0.015625)\" />\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-6e\" />\n",
       "     <use xlink:href=\"#ArialMT-65\" x=\"55.615234\" />\n",
       "     <use xlink:href=\"#ArialMT-67\" x=\"111.230469\" />\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       "</svg></g>\n",
       "\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>node3-&gt;leaf4</title>\n",
       "<path fill=\"none\" stroke=\"#444443\" stroke-width=\"0.3\" d=\"M86.33,-134.95C80.99,-125.61 69.74,-105.93 58.99,-87.11\" />\n",
       "<polygon fill=\"#444443\" stroke=\"#444443\" stroke-width=\"0.3\" points=\"60.05,-86.15 56.85,-83.37 57.62,-87.53 60.05,-86.15\" />\n",
       "</g>\n",
       "\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>leaf5</title>\n",
       "<polygon fill=\"none\" stroke=\"#444443\" stroke-width=\"0\" points=\"136.5,-77 80.5,-77 80.5,-6 136.5,-6 136.5,-77\" />\n",
       "<svg width=\"48px\" height=\"63px\" viewBox=\"0 0 64.628 85.068557\" version=\"1.1\" preserveAspectRatio=\"xMinYMin meet\" x=\"84.5\" y=\"-72.5\">\n",
       " <metadata>\n",
       "  <rdf:RDF>\n",
       "   <ns2:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\" />\n",
       "    <dc:date>2023-04-11T19:23:53.297937</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <ns2:Agent>\n",
       "      <dc:title>Matplotlib v3.5.2, https://matplotlib.org/</dc:title>\n",
       "     </ns2:Agent>\n",
       "    </dc:creator>\n",
       "   </ns2:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 85.068557  L 64.628 85.068557  L 64.628 0  L 0 0  L 0 85.068557  z \" style=\"fill: none\" />\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 64.628 32.314  C 64.628 28.070564 63.792136 23.868391 62.168243 19.947968  C 60.544351 16.027544 58.164011 12.465114 55.163449 9.464551  C 52.162886 6.463989 48.600456 4.083649 44.680032 2.459757  C 40.759609 0.835864 36.557436 -0 32.314 -0  C 28.070564 -0 23.868391 0.835864 19.947968 2.459757  C 16.027544 4.083649 12.465114 6.463989 9.464551 9.464551  C 6.463989 12.465114 4.083649 16.027544 2.459757 19.947968  C 0.835864 23.868391 0 28.070564 0 32.314  C 0 36.557436 0.835864 40.759609 2.459757 44.680032  C 4.083649 48.600456 6.463989 52.162886 9.464551 55.163449  C 12.465114 58.164011 16.027544 60.544351 19.947968 62.168243  C 23.868391 63.792136 28.070564 64.628 32.314 64.628  C 36.557436 64.628 40.759609 63.792136 44.680032 62.168243  C 48.600456 60.544351 52.162886 58.164011 55.163449 55.163449  C 58.164011 52.162886 60.544351 48.600456 62.168243 44.680032  C 63.792136 40.759609 64.628 36.557436 64.628 32.314  M 32.314 32.314  M 64.628 32.314  z \" style=\"fill: #fefebb; stroke: #444443; stroke-width: 0.5; stroke-linejoin: miter\" />\n",
       "   </g>\n",
       "   <g id=\"text_1\">\n",
       "    \n",
       "    <g style=\"fill: #444443\" transform=\"translate(22.178453 73.655151)scale(0.09 -0.09)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-6e\" d=\"M 422 0  L 422 3319  L 928 3319  L 928 2847  Q 1294 3394 1984 3394  Q 2284 3394 2536 3286  Q 2788 3178 2913 3003  Q 3038 2828 3088 2588  Q 3119 2431 3119 2041  L 3119 0  L 2556 0  L 2556 2019  Q 2556 2363 2490 2533  Q 2425 2703 2258 2804  Q 2091 2906 1866 2906  Q 1506 2906 1245 2678  Q 984 2450 984 1813  L 984 0  L 422 0  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-3d\" d=\"M 3381 2694  L 356 2694  L 356 3219  L 3381 3219  L 3381 2694  z M 3381 1303  L 356 1303  L 356 1828  L 3381 1828  L 3381 1303  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-34\" d=\"M 2069 0  L 2069 1097  L 81 1097  L 81 1613  L 2172 4581  L 2631 4581  L 2631 1613  L 3250 1613  L 3250 1097  L 2631 1097  L 2631 0  L 2069 0  z M 2069 1613  L 2069 3678  L 634 1613  L 2069 1613  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-36\" d=\"M 3184 3459  L 2625 3416  Q 2550 3747 2413 3897  Q 2184 4138 1850 4138  Q 1581 4138 1378 3988  Q 1113 3794 959 3422  Q 806 3050 800 2363  Q 1003 2672 1297 2822  Q 1591 2972 1913 2972  Q 2475 2972 2870 2558  Q 3266 2144 3266 1488  Q 3266 1056 3080 686  Q 2894 316 2569 119  Q 2244 -78 1831 -78  Q 1128 -78 684 439  Q 241 956 241 2144  Q 241 3472 731 4075  Q 1159 4600 1884 4600  Q 2425 4600 2770 4297  Q 3116 3994 3184 3459  z M 888 1484  Q 888 1194 1011 928  Q 1134 663 1356 523  Q 1578 384 1822 384  Q 2178 384 2434 671  Q 2691 959 2691 1453  Q 2691 1928 2437 2201  Q 2184 2475 1800 2475  Q 1419 2475 1153 2201  Q 888 1928 888 1484  z \" transform=\"scale(0.015625)\" />\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-6e\" />\n",
       "     <use xlink:href=\"#ArialMT-3d\" x=\"55.615234\" />\n",
       "     <use xlink:href=\"#ArialMT-34\" x=\"114.013672\" />\n",
       "     <use xlink:href=\"#ArialMT-36\" x=\"169.628906\" />\n",
       "    </g>\n",
       "    \n",
       "    <g style=\"fill: #444443\" transform=\"translate(24.806734 83.174339)scale(0.09 -0.09)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-65\" d=\"M 2694 1069  L 3275 997  Q 3138 488 2766 206  Q 2394 -75 1816 -75  Q 1088 -75 661 373  Q 234 822 234 1631  Q 234 2469 665 2931  Q 1097 3394 1784 3394  Q 2450 3394 2872 2941  Q 3294 2488 3294 1666  Q 3294 1616 3291 1516  L 816 1516  Q 847 969 1125 678  Q 1403 388 1819 388  Q 2128 388 2347 550  Q 2566 713 2694 1069  z M 847 1978  L 2700 1978  Q 2663 2397 2488 2606  Q 2219 2931 1791 2931  Q 1403 2931 1139 2672  Q 875 2413 847 1978  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-67\" d=\"M 319 -275  L 866 -356  Q 900 -609 1056 -725  Q 1266 -881 1628 -881  Q 2019 -881 2231 -725  Q 2444 -569 2519 -288  Q 2563 -116 2559 434  Q 2191 0 1641 0  Q 956 0 581 494  Q 206 988 206 1678  Q 206 2153 378 2554  Q 550 2956 876 3175  Q 1203 3394 1644 3394  Q 2231 3394 2613 2919  L 2613 3319  L 3131 3319  L 3131 450  Q 3131 -325 2973 -648  Q 2816 -972 2473 -1159  Q 2131 -1347 1631 -1347  Q 1038 -1347 672 -1080  Q 306 -813 319 -275  z M 784 1719  Q 784 1066 1043 766  Q 1303 466 1694 466  Q 2081 466 2343 764  Q 2606 1063 2606 1700  Q 2606 2309 2336 2618  Q 2066 2928 1684 2928  Q 1309 2928 1046 2623  Q 784 2319 784 1719  z \" transform=\"scale(0.015625)\" />\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-6e\" />\n",
       "     <use xlink:href=\"#ArialMT-65\" x=\"55.615234\" />\n",
       "     <use xlink:href=\"#ArialMT-67\" x=\"111.230469\" />\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       "</svg></g>\n",
       "\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>node3-&gt;leaf5</title>\n",
       "<path fill=\"none\" stroke=\"#444443\" stroke-width=\"0.3\" d=\"M90.58,-134.95C92.58,-124.63 97.03,-101.65 101,-81.19\" />\n",
       "<polygon fill=\"#444443\" stroke=\"#444443\" stroke-width=\"0.3\" points=\"102.4,-81.33 101.78,-77.14 99.65,-80.8 102.4,-81.33\" />\n",
       "</g>\n",
       "\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>leaf6</title>\n",
       "<polygon fill=\"none\" stroke=\"#444443\" stroke-width=\"0\" points=\"210,-183 143,-183 143,-100 210,-100 210,-183\" />\n",
       "<svg width=\"59px\" height=\"75px\" viewBox=\"0 0 78.52 99.516238\" version=\"1.1\" preserveAspectRatio=\"xMinYMin meet\" x=\"147.5\" y=\"-178.5\">\n",
       " <metadata>\n",
       "  <rdf:RDF>\n",
       "   <ns2:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\" />\n",
       "    <dc:date>2023-04-11T19:23:53.335755</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <ns2:Agent>\n",
       "      <dc:title>Matplotlib v3.5.2, https://matplotlib.org/</dc:title>\n",
       "     </ns2:Agent>\n",
       "    </dc:creator>\n",
       "   </ns2:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M -0 99.516238  L 78.52 99.516238  L 78.52 0  L -0 0  L -0 99.516238  z \" style=\"fill: none\" />\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 78.52 39.26  C 78.52 34.104424 77.504464 28.99898 75.53151 24.235848  C 73.558557 19.472717 70.666555 15.14453 67.021012 11.498988  C 63.37547 7.853445 59.047283 4.961443 54.284152 2.98849  C 49.52102 1.015536 44.415576 0 39.26 0  C 34.104424 0 28.99898 1.015536 24.235848 2.98849  C 19.472717 4.961443 15.14453 7.853445 11.498988 11.498988  C 7.853445 15.14453 4.961443 19.472717 2.98849 24.235848  C 1.015536 28.99898 0 34.104424 0 39.26  C 0 44.415576 1.015536 49.52102 2.98849 54.284152  C 4.961443 59.047283 7.853445 63.37547 11.498988 67.021012  C 15.14453 70.666555 19.472717 73.558557 24.235848 75.53151  C 28.99898 77.504464 34.104424 78.52 39.26 78.52  C 44.415576 78.52 49.52102 77.504464 54.284152 75.53151  C 59.047283 73.558557 63.37547 70.666555 67.021012 67.021012  C 70.666555 63.37547 73.558557 59.047283 75.53151 54.284152  C 77.504464 49.52102 78.52 44.415576 78.52 39.26  M 39.26 39.26  M 78.52 39.26  z \" style=\"fill: #fefebb; stroke: #444443; stroke-width: 0.5; stroke-linejoin: miter\" />\n",
       "   </g>\n",
       "   <g id=\"text_1\">\n",
       "    \n",
       "    <g style=\"fill: #444443\" transform=\"translate(29.124453 88.102831)scale(0.09 -0.09)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-6e\" d=\"M 422 0  L 422 3319  L 928 3319  L 928 2847  Q 1294 3394 1984 3394  Q 2284 3394 2536 3286  Q 2788 3178 2913 3003  Q 3038 2828 3088 2588  Q 3119 2431 3119 2041  L 3119 0  L 2556 0  L 2556 2019  Q 2556 2363 2490 2533  Q 2425 2703 2258 2804  Q 2091 2906 1866 2906  Q 1506 2906 1245 2678  Q 984 2450 984 1813  L 984 0  L 422 0  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-3d\" d=\"M 3381 2694  L 356 2694  L 356 3219  L 3381 3219  L 3381 2694  z M 3381 1303  L 356 1303  L 356 1828  L 3381 1828  L 3381 1303  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-37\" d=\"M 303 3981  L 303 4522  L 3269 4522  L 3269 4084  Q 2831 3619 2401 2847  Q 1972 2075 1738 1259  Q 1569 684 1522 0  L 944 0  Q 953 541 1156 1306  Q 1359 2072 1739 2783  Q 2119 3494 2547 3981  L 303 3981  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-31\" d=\"M 2384 0  L 1822 0  L 1822 3584  Q 1619 3391 1289 3197  Q 959 3003 697 2906  L 697 3450  Q 1169 3672 1522 3987  Q 1875 4303 2022 4600  L 2384 4600  L 2384 0  z \" transform=\"scale(0.015625)\" />\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-6e\" />\n",
       "     <use xlink:href=\"#ArialMT-3d\" x=\"55.615234\" />\n",
       "     <use xlink:href=\"#ArialMT-37\" x=\"114.013672\" />\n",
       "     <use xlink:href=\"#ArialMT-31\" x=\"169.628906\" />\n",
       "    </g>\n",
       "    \n",
       "    <g style=\"fill: #444443\" transform=\"translate(31.752734 97.622019)scale(0.09 -0.09)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-65\" d=\"M 2694 1069  L 3275 997  Q 3138 488 2766 206  Q 2394 -75 1816 -75  Q 1088 -75 661 373  Q 234 822 234 1631  Q 234 2469 665 2931  Q 1097 3394 1784 3394  Q 2450 3394 2872 2941  Q 3294 2488 3294 1666  Q 3294 1616 3291 1516  L 816 1516  Q 847 969 1125 678  Q 1403 388 1819 388  Q 2128 388 2347 550  Q 2566 713 2694 1069  z M 847 1978  L 2700 1978  Q 2663 2397 2488 2606  Q 2219 2931 1791 2931  Q 1403 2931 1139 2672  Q 875 2413 847 1978  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-67\" d=\"M 319 -275  L 866 -356  Q 900 -609 1056 -725  Q 1266 -881 1628 -881  Q 2019 -881 2231 -725  Q 2444 -569 2519 -288  Q 2563 -116 2559 434  Q 2191 0 1641 0  Q 956 0 581 494  Q 206 988 206 1678  Q 206 2153 378 2554  Q 550 2956 876 3175  Q 1203 3394 1644 3394  Q 2231 3394 2613 2919  L 2613 3319  L 3131 3319  L 3131 450  Q 3131 -325 2973 -648  Q 2816 -972 2473 -1159  Q 2131 -1347 1631 -1347  Q 1038 -1347 672 -1080  Q 306 -813 319 -275  z M 784 1719  Q 784 1066 1043 766  Q 1303 466 1694 466  Q 2081 466 2343 764  Q 2606 1063 2606 1700  Q 2606 2309 2336 2618  Q 2066 2928 1684 2928  Q 1309 2928 1046 2623  Q 784 2319 784 1719  z \" transform=\"scale(0.015625)\" />\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-6e\" />\n",
       "     <use xlink:href=\"#ArialMT-65\" x=\"55.615234\" />\n",
       "     <use xlink:href=\"#ArialMT-67\" x=\"111.230469\" />\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       "</svg></g>\n",
       "\n",
       "\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>node2</title>\n",
       "<text text-anchor=\"start\" x=\"180.5\" y=\"-213.9\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#444443\">T@24.39</text>\n",
       "</g>\n",
       "\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>node2-&gt;node3</title>\n",
       "<path fill=\"none\" stroke=\"#444443\" stroke-width=\"0.3\" d=\"M196.78,-210.48C176.38,-197.55 125.46,-165.28 101.86,-150.33\" />\n",
       "<polygon fill=\"#444443\" stroke=\"#444443\" stroke-width=\"0.3\" points=\"102.45,-149.05 98.32,-148.09 100.95,-151.41 102.45,-149.05\" />\n",
       "</g>\n",
       "\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>node7</title>\n",
       "<text text-anchor=\"start\" x=\"256\" y=\"-213.9\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#444443\">Chl-a@0.39</text>\n",
       "</g>\n",
       "\n",
       "\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>node2-&gt;leaf6</title>\n",
       "<path fill=\"none\" stroke=\"#444443\" stroke-width=\"0.3\" d=\"M203.22,-210.23C201.12,-204.9 197.78,-196.44 194.17,-187.28\" />\n",
       "<polygon fill=\"#444443\" stroke=\"#444443\" stroke-width=\"0.3\" points=\"195.38,-186.53 192.61,-183.32 192.77,-187.56 195.38,-186.53\" />\n",
       "</g>\n",
       "\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>node9</title>\n",
       "<text text-anchor=\"start\" x=\"266\" y=\"-138.4\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#444443\">DIN@9.86</text>\n",
       "</g>\n",
       "\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>leaf10</title>\n",
       "<polygon fill=\"none\" stroke=\"#444443\" stroke-width=\"0\" points=\"299,-75.5 246,-75.5 246,-7.5 299,-7.5 299,-75.5\" />\n",
       "<svg width=\"45px\" height=\"60px\" viewBox=\"0 0 61.003989 81.194109\" version=\"1.1\" preserveAspectRatio=\"xMinYMin meet\" x=\"250.5\" y=\"-71.5\">\n",
       " <metadata>\n",
       "  <rdf:RDF>\n",
       "   <ns2:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\" />\n",
       "    <dc:date>2023-04-11T19:23:53.412176</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <ns2:Agent>\n",
       "      <dc:title>Matplotlib v3.5.2, https://matplotlib.org/</dc:title>\n",
       "     </ns2:Agent>\n",
       "    </dc:creator>\n",
       "   </ns2:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 81.194109  L 61.003989 81.194109  L 61.003989 0  L 0 0  L 0 81.194109  z \" style=\"fill: none\" />\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 61.003989 30.501981  C 61.003989 22.746412 58.046043 15.275492 52.736991 9.621925  C 47.427938 3.968359 40.157492 0.547146 32.417227 0.060169  C 24.676961 -0.426807 17.035053 2.056199 11.059284 6.999785  C 5.083515 11.94337 1.212538 18.98465 0.240507 26.679064  L 30.501989 30.501981  L 61.003989 30.501981  z \" style=\"fill: #fefebb; stroke: #444443; stroke-width: 0.5; stroke-linejoin: miter\" />\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 0.240507 26.679064  C -0.28173 30.812994 0.048615 35.010427 1.211111 39.011765  C 2.373606 43.013103 4.343783 46.734121 6.999792 49.944685  C 9.655801 53.155249 12.941735 55.787778 16.654368 57.67946  C 20.367001 59.571142 24.428185 60.682156 28.586749 60.943792  C 32.745313 61.205427 36.91372 60.612175 40.834169 59.200727  C 44.754618 57.789279 48.344585 55.589346 51.382042 52.736985  C 54.419498 49.884624 56.840508 46.439877 58.495339 42.61579  C 60.15017 38.791704 61.003988 34.668772 61.003989 30.501986  L 30.501989 30.501981  L 0.240507 26.679064  z \" style=\"fill: #a1dab4; stroke: #444443; stroke-width: 0.5; stroke-linejoin: miter\" />\n",
       "   </g>\n",
       "   <g id=\"text_1\">\n",
       "    \n",
       "    <g style=\"fill: #444443\" transform=\"translate(20.366442 69.886172)scale(0.09 -0.09)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-6e\" d=\"M 422 0  L 422 3319  L 928 3319  L 928 2847  Q 1294 3394 1984 3394  Q 2284 3394 2536 3286  Q 2788 3178 2913 3003  Q 3038 2828 3088 2588  Q 3119 2431 3119 2041  L 3119 0  L 2556 0  L 2556 2019  Q 2556 2363 2490 2533  Q 2425 2703 2258 2804  Q 2091 2906 1866 2906  Q 1506 2906 1245 2678  Q 984 2450 984 1813  L 984 0  L 422 0  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-3d\" d=\"M 3381 2694  L 356 2694  L 356 3219  L 3381 3219  L 3381 2694  z M 3381 1303  L 356 1303  L 356 1828  L 3381 1828  L 3381 1303  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-34\" d=\"M 2069 0  L 2069 1097  L 81 1097  L 81 1613  L 2172 4581  L 2631 4581  L 2631 1613  L 3250 1613  L 3250 1097  L 2631 1097  L 2631 0  L 2069 0  z M 2069 1613  L 2069 3678  L 634 1613  L 2069 1613  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-33\" d=\"M 269 1209  L 831 1284  Q 928 806 1161 595  Q 1394 384 1728 384  Q 2125 384 2398 659  Q 2672 934 2672 1341  Q 2672 1728 2419 1979  Q 2166 2231 1775 2231  Q 1616 2231 1378 2169  L 1441 2663  Q 1497 2656 1531 2656  Q 1891 2656 2178 2843  Q 2466 3031 2466 3422  Q 2466 3731 2256 3934  Q 2047 4138 1716 4138  Q 1388 4138 1169 3931  Q 950 3725 888 3313  L 325 3413  Q 428 3978 793 4289  Q 1159 4600 1703 4600  Q 2078 4600 2393 4439  Q 2709 4278 2876 4000  Q 3044 3722 3044 3409  Q 3044 3113 2884 2869  Q 2725 2625 2413 2481  Q 2819 2388 3044 2092  Q 3269 1797 3269 1353  Q 3269 753 2831 336  Q 2394 -81 1725 -81  Q 1122 -81 723 278  Q 325 638 269 1209  z \" transform=\"scale(0.015625)\" />\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-6e\" />\n",
       "     <use xlink:href=\"#ArialMT-3d\" x=\"55.615234\" />\n",
       "     <use xlink:href=\"#ArialMT-34\" x=\"114.013672\" />\n",
       "     <use xlink:href=\"#ArialMT-33\" x=\"169.628906\" />\n",
       "    </g>\n",
       "    \n",
       "    <g style=\"fill: #444443\" transform=\"translate(23.247146 79.405359)scale(0.09 -0.09)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-70\" d=\"M 422 -1272  L 422 3319  L 934 3319  L 934 2888  Q 1116 3141 1344 3267  Q 1572 3394 1897 3394  Q 2322 3394 2647 3175  Q 2972 2956 3137 2557  Q 3303 2159 3303 1684  Q 3303 1175 3120 767  Q 2938 359 2589 142  Q 2241 -75 1856 -75  Q 1575 -75 1351 44  Q 1128 163 984 344  L 984 -1272  L 422 -1272  z M 931 1641  Q 931 1000 1190 694  Q 1450 388 1819 388  Q 2194 388 2461 705  Q 2728 1022 2728 1688  Q 2728 2322 2467 2637  Q 2206 2953 1844 2953  Q 1484 2953 1207 2617  Q 931 2281 931 1641  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-6f\" d=\"M 213 1659  Q 213 2581 725 3025  Q 1153 3394 1769 3394  Q 2453 3394 2887 2945  Q 3322 2497 3322 1706  Q 3322 1066 3130 698  Q 2938 331 2570 128  Q 2203 -75 1769 -75  Q 1072 -75 642 372  Q 213 819 213 1659  z M 791 1659  Q 791 1022 1069 705  Q 1347 388 1769 388  Q 2188 388 2466 706  Q 2744 1025 2744 1678  Q 2744 2294 2464 2611  Q 2184 2928 1769 2928  Q 1347 2928 1069 2612  Q 791 2297 791 1659  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-7a\" d=\"M 125 0  L 125 456  L 2238 2881  Q 1878 2863 1603 2863  L 250 2863  L 250 3319  L 2963 3319  L 2963 2947  L 1166 841  L 819 456  Q 1197 484 1528 484  L 3063 484  L 3063 0  L 125 0  z \" transform=\"scale(0.015625)\" />\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-70\" />\n",
       "     <use xlink:href=\"#ArialMT-6f\" x=\"55.615234\" />\n",
       "     <use xlink:href=\"#ArialMT-7a\" x=\"111.230469\" />\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       "</svg></g>\n",
       "\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>node9-&gt;leaf10</title>\n",
       "<path fill=\"none\" stroke=\"#444443\" stroke-width=\"0.3\" d=\"M293.25,-134.95C290.89,-124.4 285.55,-100.63 280.88,-79.84\" />\n",
       "<polygon fill=\"#444443\" stroke=\"#444443\" stroke-width=\"0.3\" points=\"282.2,-79.32 279.96,-75.73 279.47,-79.94 282.2,-79.32\" />\n",
       "</g>\n",
       "\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>leaf11</title>\n",
       "<polygon fill=\"none\" stroke=\"#444443\" stroke-width=\"0\" points=\"340,-62 313,-62 313,-21 340,-21 340,-62\" />\n",
       "<svg width=\"19px\" height=\"33px\" viewBox=\"0 0 25.972 44.760849\" version=\"1.1\" preserveAspectRatio=\"xMinYMin meet\" x=\"317.5\" y=\"-57.5\">\n",
       " <metadata>\n",
       "  <rdf:RDF>\n",
       "   <ns2:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\" />\n",
       "    <dc:date>2023-04-11T19:23:53.450736</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <ns2:Agent>\n",
       "      <dc:title>Matplotlib v3.5.2, https://matplotlib.org/</dc:title>\n",
       "     </ns2:Agent>\n",
       "    </dc:creator>\n",
       "   </ns2:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 44.760849  L 25.972 44.760849  L 25.972 0  L 0 0  L 0 44.760849  z \" style=\"fill: none\" />\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 25.972 12.986  C 25.972 11.280694 25.636092 9.59197 24.9835 8.016473  C 24.330907 6.440976 23.374322 5.009345 22.168489 3.803511  C 20.962655 2.597678 19.531024 1.641093 17.955527 0.9885  C 16.38003 0.335908 14.691306 0 12.986 0  C 11.280694 0 9.59197 0.335908 8.016473 0.9885  C 6.440976 1.641093 5.009345 2.597678 3.803511 3.803511  C 2.597678 5.009345 1.641093 6.440976 0.9885 8.016473  C 0.335908 9.59197 -0 11.280694 -0 12.986  C -0 14.691306 0.335908 16.38003 0.9885 17.955527  C 1.641093 19.531024 2.597678 20.962655 3.803511 22.168489  C 5.009345 23.374322 6.440976 24.330907 8.016473 24.9835  C 9.59197 25.636092 11.280694 25.972 12.986 25.972  C 14.691306 25.972 16.38003 25.636092 17.955527 24.9835  C 19.531024 24.330907 20.962655 23.374322 22.168489 22.168489  C 23.374322 20.962655 24.330907 19.531024 24.9835 17.955527  C 25.636092 16.38003 25.972 14.691306 25.972 12.986  M 12.986 12.986  M 25.972 12.986  z \" style=\"fill: #a1dab4; stroke: #444443; stroke-width: 0.5; stroke-linejoin: miter\" />\n",
       "   </g>\n",
       "   <g id=\"text_1\">\n",
       "    \n",
       "    <g style=\"fill: #444443\" transform=\"translate(2.850453 33.452911)scale(0.09 -0.09)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-6e\" d=\"M 422 0  L 422 3319  L 928 3319  L 928 2847  Q 1294 3394 1984 3394  Q 2284 3394 2536 3286  Q 2788 3178 2913 3003  Q 3038 2828 3088 2588  Q 3119 2431 3119 2041  L 3119 0  L 2556 0  L 2556 2019  Q 2556 2363 2490 2533  Q 2425 2703 2258 2804  Q 2091 2906 1866 2906  Q 1506 2906 1245 2678  Q 984 2450 984 1813  L 984 0  L 422 0  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-3d\" d=\"M 3381 2694  L 356 2694  L 356 3219  L 3381 3219  L 3381 2694  z M 3381 1303  L 356 1303  L 356 1828  L 3381 1828  L 3381 1303  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-31\" d=\"M 2384 0  L 1822 0  L 1822 3584  Q 1619 3391 1289 3197  Q 959 3003 697 2906  L 697 3450  Q 1169 3672 1522 3987  Q 1875 4303 2022 4600  L 2384 4600  L 2384 0  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-34\" d=\"M 2069 0  L 2069 1097  L 81 1097  L 81 1613  L 2172 4581  L 2631 4581  L 2631 1613  L 3250 1613  L 3250 1097  L 2631 1097  L 2631 0  L 2069 0  z M 2069 1613  L 2069 3678  L 634 1613  L 2069 1613  z \" transform=\"scale(0.015625)\" />\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-6e\" />\n",
       "     <use xlink:href=\"#ArialMT-3d\" x=\"55.615234\" />\n",
       "     <use xlink:href=\"#ArialMT-31\" x=\"114.013672\" />\n",
       "     <use xlink:href=\"#ArialMT-34\" x=\"169.628906\" />\n",
       "    </g>\n",
       "    \n",
       "    <g style=\"fill: #444443\" transform=\"translate(5.731156 42.972099)scale(0.09 -0.09)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-70\" d=\"M 422 -1272  L 422 3319  L 934 3319  L 934 2888  Q 1116 3141 1344 3267  Q 1572 3394 1897 3394  Q 2322 3394 2647 3175  Q 2972 2956 3137 2557  Q 3303 2159 3303 1684  Q 3303 1175 3120 767  Q 2938 359 2589 142  Q 2241 -75 1856 -75  Q 1575 -75 1351 44  Q 1128 163 984 344  L 984 -1272  L 422 -1272  z M 931 1641  Q 931 1000 1190 694  Q 1450 388 1819 388  Q 2194 388 2461 705  Q 2728 1022 2728 1688  Q 2728 2322 2467 2637  Q 2206 2953 1844 2953  Q 1484 2953 1207 2617  Q 931 2281 931 1641  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-6f\" d=\"M 213 1659  Q 213 2581 725 3025  Q 1153 3394 1769 3394  Q 2453 3394 2887 2945  Q 3322 2497 3322 1706  Q 3322 1066 3130 698  Q 2938 331 2570 128  Q 2203 -75 1769 -75  Q 1072 -75 642 372  Q 213 819 213 1659  z M 791 1659  Q 791 1022 1069 705  Q 1347 388 1769 388  Q 2188 388 2466 706  Q 2744 1025 2744 1678  Q 2744 2294 2464 2611  Q 2184 2928 1769 2928  Q 1347 2928 1069 2612  Q 791 2297 791 1659  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-7a\" d=\"M 125 0  L 125 456  L 2238 2881  Q 1878 2863 1603 2863  L 250 2863  L 250 3319  L 2963 3319  L 2963 2947  L 1166 841  L 819 456  Q 1197 484 1528 484  L 3063 484  L 3063 0  L 125 0  z \" transform=\"scale(0.015625)\" />\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-70\" />\n",
       "     <use xlink:href=\"#ArialMT-6f\" x=\"55.615234\" />\n",
       "     <use xlink:href=\"#ArialMT-7a\" x=\"111.230469\" />\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       "</svg></g>\n",
       "\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>node9-&gt;leaf11</title>\n",
       "<path fill=\"none\" stroke=\"#444443\" stroke-width=\"0.3\" d=\"M296.31,-134.95C300.56,-121.94 311.37,-88.83 318.83,-65.99\" />\n",
       "<polygon fill=\"#444443\" stroke=\"#444443\" stroke-width=\"0.3\" points=\"320.18,-66.37 320.09,-62.13 317.52,-65.5 320.18,-66.37\" />\n",
       "</g>\n",
       "\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>node7-&gt;node9</title>\n",
       "<path fill=\"none\" stroke=\"#444443\" stroke-width=\"0.3\" d=\"M288.97,-210.23C289.99,-197.72 292.42,-167.96 293.69,-152.38\" />\n",
       "<polygon fill=\"#444443\" stroke=\"#444443\" stroke-width=\"0.3\" points=\"295.12,-152.14 294.05,-148.04 292.33,-151.91 295.12,-152.14\" />\n",
       "</g>\n",
       "\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>leaf8</title>\n",
       "<polygon fill=\"none\" stroke=\"#444443\" stroke-width=\"0\" points=\"252,-166.5 217,-166.5 217,-116.5 252,-116.5 252,-166.5\" />\n",
       "<svg width=\"27px\" height=\"42px\" viewBox=\"0 0 36.844 56.173197\" version=\"1.1\" preserveAspectRatio=\"xMinYMin meet\" x=\"221.5\" y=\"-162.5\">\n",
       " <metadata>\n",
       "  <rdf:RDF>\n",
       "   <ns2:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\" />\n",
       "    <dc:date>2023-04-11T19:23:53.374048</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <ns2:Agent>\n",
       "      <dc:title>Matplotlib v3.5.2, https://matplotlib.org/</dc:title>\n",
       "     </ns2:Agent>\n",
       "    </dc:creator>\n",
       "   </ns2:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 56.173197  L 36.844 56.173197  L 36.844 0  L 0 0  L 0 56.173197  z \" style=\"fill: none\" />\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 36.844 18.422  C 36.844 16.002845 36.367479 13.607213 35.441709 11.372206  C 34.515938 9.137198 33.158922 7.10628 31.448321 5.395679  C 29.73772 3.685078 27.706802 2.328062 25.471794 1.402291  C 23.236787 0.476521 20.841155 0 18.422 0  C 16.002845 0 13.607213 0.476521 11.372206 1.402291  C 9.137198 2.328062 7.10628 3.685078 5.395679 5.395679  C 3.685078 7.10628 2.328062 9.137198 1.402291 11.372206  C 0.476521 13.607213 0 16.002845 0 18.422  C 0 20.841155 0.476521 23.236787 1.402291 25.471794  C 2.328062 27.706802 3.685078 29.73772 5.395679 31.448321  C 7.10628 33.158922 9.137198 34.515938 11.372206 35.441709  C 13.607213 36.367479 16.002845 36.844 18.422 36.844  C 20.841155 36.844 23.236787 36.367479 25.471794 35.441709  C 27.706802 34.515938 29.73772 33.158922 31.448321 31.448321  C 33.158922 29.73772 34.515938 27.706802 35.441709 25.471794  C 36.367479 23.236787 36.844 20.841155 36.844 18.422  M 18.422 18.422  M 36.844 18.422  z \" style=\"fill: #fefebb; stroke: #444443; stroke-width: 0.5; stroke-linejoin: miter\" />\n",
       "   </g>\n",
       "   <g id=\"text_1\">\n",
       "    \n",
       "    <g style=\"fill: #444443\" transform=\"translate(8.286453 44.759791)scale(0.09 -0.09)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-6e\" d=\"M 422 0  L 422 3319  L 928 3319  L 928 2847  Q 1294 3394 1984 3394  Q 2284 3394 2536 3286  Q 2788 3178 2913 3003  Q 3038 2828 3088 2588  Q 3119 2431 3119 2041  L 3119 0  L 2556 0  L 2556 2019  Q 2556 2363 2490 2533  Q 2425 2703 2258 2804  Q 2091 2906 1866 2906  Q 1506 2906 1245 2678  Q 984 2450 984 1813  L 984 0  L 422 0  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-3d\" d=\"M 3381 2694  L 356 2694  L 356 3219  L 3381 3219  L 3381 2694  z M 3381 1303  L 356 1303  L 356 1828  L 3381 1828  L 3381 1303  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-32\" d=\"M 3222 541  L 3222 0  L 194 0  Q 188 203 259 391  Q 375 700 629 1000  Q 884 1300 1366 1694  Q 2113 2306 2375 2664  Q 2638 3022 2638 3341  Q 2638 3675 2398 3904  Q 2159 4134 1775 4134  Q 1369 4134 1125 3890  Q 881 3647 878 3216  L 300 3275  Q 359 3922 746 4261  Q 1134 4600 1788 4600  Q 2447 4600 2831 4234  Q 3216 3869 3216 3328  Q 3216 3053 3103 2787  Q 2991 2522 2730 2228  Q 2469 1934 1863 1422  Q 1356 997 1212 845  Q 1069 694 975 541  L 3222 541  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-33\" d=\"M 269 1209  L 831 1284  Q 928 806 1161 595  Q 1394 384 1728 384  Q 2125 384 2398 659  Q 2672 934 2672 1341  Q 2672 1728 2419 1979  Q 2166 2231 1775 2231  Q 1616 2231 1378 2169  L 1441 2663  Q 1497 2656 1531 2656  Q 1891 2656 2178 2843  Q 2466 3031 2466 3422  Q 2466 3731 2256 3934  Q 2047 4138 1716 4138  Q 1388 4138 1169 3931  Q 950 3725 888 3313  L 325 3413  Q 428 3978 793 4289  Q 1159 4600 1703 4600  Q 2078 4600 2393 4439  Q 2709 4278 2876 4000  Q 3044 3722 3044 3409  Q 3044 3113 2884 2869  Q 2725 2625 2413 2481  Q 2819 2388 3044 2092  Q 3269 1797 3269 1353  Q 3269 753 2831 336  Q 2394 -81 1725 -81  Q 1122 -81 723 278  Q 325 638 269 1209  z \" transform=\"scale(0.015625)\" />\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-6e\" />\n",
       "     <use xlink:href=\"#ArialMT-3d\" x=\"55.615234\" />\n",
       "     <use xlink:href=\"#ArialMT-32\" x=\"114.013672\" />\n",
       "     <use xlink:href=\"#ArialMT-33\" x=\"169.628906\" />\n",
       "    </g>\n",
       "    \n",
       "    <g style=\"fill: #444443\" transform=\"translate(10.914734 54.278979)scale(0.09 -0.09)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-65\" d=\"M 2694 1069  L 3275 997  Q 3138 488 2766 206  Q 2394 -75 1816 -75  Q 1088 -75 661 373  Q 234 822 234 1631  Q 234 2469 665 2931  Q 1097 3394 1784 3394  Q 2450 3394 2872 2941  Q 3294 2488 3294 1666  Q 3294 1616 3291 1516  L 816 1516  Q 847 969 1125 678  Q 1403 388 1819 388  Q 2128 388 2347 550  Q 2566 713 2694 1069  z M 847 1978  L 2700 1978  Q 2663 2397 2488 2606  Q 2219 2931 1791 2931  Q 1403 2931 1139 2672  Q 875 2413 847 1978  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-67\" d=\"M 319 -275  L 866 -356  Q 900 -609 1056 -725  Q 1266 -881 1628 -881  Q 2019 -881 2231 -725  Q 2444 -569 2519 -288  Q 2563 -116 2559 434  Q 2191 0 1641 0  Q 956 0 581 494  Q 206 988 206 1678  Q 206 2153 378 2554  Q 550 2956 876 3175  Q 1203 3394 1644 3394  Q 2231 3394 2613 2919  L 2613 3319  L 3131 3319  L 3131 450  Q 3131 -325 2973 -648  Q 2816 -972 2473 -1159  Q 2131 -1347 1631 -1347  Q 1038 -1347 672 -1080  Q 306 -813 319 -275  z M 784 1719  Q 784 1066 1043 766  Q 1303 466 1694 466  Q 2081 466 2343 764  Q 2606 1063 2606 1700  Q 2606 2309 2336 2618  Q 2066 2928 1684 2928  Q 1309 2928 1046 2623  Q 784 2319 784 1719  z \" transform=\"scale(0.015625)\" />\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-6e\" />\n",
       "     <use xlink:href=\"#ArialMT-65\" x=\"55.615234\" />\n",
       "     <use xlink:href=\"#ArialMT-67\" x=\"111.230469\" />\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       "</svg></g>\n",
       "\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>node7-&gt;leaf8</title>\n",
       "<path fill=\"none\" stroke=\"#444443\" stroke-width=\"0.3\" d=\"M284.26,-210.23C277.88,-201.55 265.4,-184.56 254.53,-169.77\" />\n",
       "<polygon fill=\"#444443\" stroke=\"#444443\" stroke-width=\"0.3\" points=\"255.59,-168.85 252.1,-166.45 253.34,-170.5 255.59,-168.85\" />\n",
       "</g>\n",
       "\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>node1</title>\n",
       "<text text-anchor=\"start\" x=\"275\" y=\"-254.4\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#444443\">DIN@3.79</text>\n",
       "</g>\n",
       "\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>node1-&gt;node2</title>\n",
       "<path fill=\"none\" stroke=\"#444443\" stroke-width=\"0.3\" d=\"M289.46,-250.98C272.14,-244.18 242.66,-232.6 223.64,-225.13\" />\n",
       "<polygon fill=\"#444443\" stroke=\"#444443\" stroke-width=\"0.3\" points=\"223.75,-223.67 219.51,-223.51 222.73,-226.27 223.75,-223.67\" />\n",
       "</g>\n",
       "\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>node1-&gt;node7</title>\n",
       "<path fill=\"none\" stroke=\"#444443\" stroke-width=\"0.3\" d=\"M301.29,-250.82C298.89,-244.68 295.04,-234.79 292.19,-227.47\" />\n",
       "<polygon fill=\"#444443\" stroke=\"#444443\" stroke-width=\"0.3\" points=\"293.48,-226.92 290.72,-223.7 290.87,-227.94 293.48,-226.92\" />\n",
       "</g>\n",
       "\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>node12</title>\n",
       "<text text-anchor=\"start\" x=\"352\" y=\"-254.4\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#444443\">DIN@5.58</text>\n",
       "</g>\n",
       "\n",
       "\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>node14</title>\n",
       "<text text-anchor=\"start\" x=\"330.5\" y=\"-138.4\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#444443\">Dinophysis tripos@5.00</text>\n",
       "</g>\n",
       "\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>leaf15</title>\n",
       "<polygon fill=\"none\" stroke=\"#444443\" stroke-width=\"0\" points=\"389,-60.5 366,-60.5 366,-22.5 389,-22.5 389,-60.5\" />\n",
       "<svg width=\"15px\" height=\"30px\" viewBox=\"0 0 21.14 39.841037\" version=\"1.1\" preserveAspectRatio=\"xMinYMin meet\" x=\"370.5\" y=\"-56.5\">\n",
       " <metadata>\n",
       "  <rdf:RDF>\n",
       "   <ns2:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\" />\n",
       "    <dc:date>2023-04-11T19:23:53.485116</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <ns2:Agent>\n",
       "      <dc:title>Matplotlib v3.5.2, https://matplotlib.org/</dc:title>\n",
       "     </ns2:Agent>\n",
       "    </dc:creator>\n",
       "   </ns2:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 39.841037  L 21.14 39.841037  L 21.14 0  L 0 0  L 0 39.841037  z \" style=\"fill: none\" />\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 21.14 10.57  C 21.14 8.714716 20.651531 6.891723 19.723888 5.285  C 18.796246 3.678276 17.461723 2.343753 15.855 1.416111  C 14.248276 0.488469 12.425284 -0 10.57 0  C 8.714715 0 6.891723 0.488469 5.284999 1.416112  C 3.678276 2.343754 2.343753 3.678277 1.416111 5.285001  C 0.488469 6.891724 -0 8.714717 0 10.570001  C 0 12.425285 0.48847 14.248278 1.416112 15.855001  C 2.343754 17.461724 3.678278 18.796247 5.285001 19.723889  L 10.57 10.57  L 21.14 10.57  z \" style=\"fill: #fefebb; stroke: #444443; stroke-width: 0.5; stroke-linejoin: miter\" />\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 5.285001 19.723889  C 6.891725 20.651531 8.714717 21.14 10.570001 21.14  C 12.425286 21.14 14.248278 20.65153 15.855001 19.723888  C 17.461725 18.796245 18.796247 17.461722 19.723889 15.854998  C 20.651531 14.248275 21.14 12.425282 21.14 10.569998  L 10.57 10.57  L 5.285001 19.723889  z \" style=\"fill: #a1dab4; stroke: #444443; stroke-width: 0.5; stroke-linejoin: miter\" />\n",
       "   </g>\n",
       "   <g id=\"text_1\">\n",
       "    \n",
       "    <g style=\"fill: #444443\" transform=\"translate(0.434453 28.427631)scale(0.09 -0.09)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-6e\" d=\"M 422 0  L 422 3319  L 928 3319  L 928 2847  Q 1294 3394 1984 3394  Q 2284 3394 2536 3286  Q 2788 3178 2913 3003  Q 3038 2828 3088 2588  Q 3119 2431 3119 2041  L 3119 0  L 2556 0  L 2556 2019  Q 2556 2363 2490 2533  Q 2425 2703 2258 2804  Q 2091 2906 1866 2906  Q 1506 2906 1245 2678  Q 984 2450 984 1813  L 984 0  L 422 0  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-3d\" d=\"M 3381 2694  L 356 2694  L 356 3219  L 3381 3219  L 3381 2694  z M 3381 1303  L 356 1303  L 356 1828  L 3381 1828  L 3381 1303  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-31\" d=\"M 2384 0  L 1822 0  L 1822 3584  Q 1619 3391 1289 3197  Q 959 3003 697 2906  L 697 3450  Q 1169 3672 1522 3987  Q 1875 4303 2022 4600  L 2384 4600  L 2384 0  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-30\" d=\"M 266 2259  Q 266 3072 433 3567  Q 600 4063 929 4331  Q 1259 4600 1759 4600  Q 2128 4600 2406 4451  Q 2684 4303 2865 4023  Q 3047 3744 3150 3342  Q 3253 2941 3253 2259  Q 3253 1453 3087 958  Q 2922 463 2592 192  Q 2263 -78 1759 -78  Q 1097 -78 719 397  Q 266 969 266 2259  z M 844 2259  Q 844 1131 1108 757  Q 1372 384 1759 384  Q 2147 384 2411 759  Q 2675 1134 2675 2259  Q 2675 3391 2411 3762  Q 2147 4134 1753 4134  Q 1366 4134 1134 3806  Q 844 3388 844 2259  z \" transform=\"scale(0.015625)\" />\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-6e\" />\n",
       "     <use xlink:href=\"#ArialMT-3d\" x=\"55.615234\" />\n",
       "     <use xlink:href=\"#ArialMT-31\" x=\"114.013672\" />\n",
       "     <use xlink:href=\"#ArialMT-30\" x=\"169.628906\" />\n",
       "    </g>\n",
       "    \n",
       "    <g style=\"fill: #444443\" transform=\"translate(3.062734 37.946819)scale(0.09 -0.09)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-65\" d=\"M 2694 1069  L 3275 997  Q 3138 488 2766 206  Q 2394 -75 1816 -75  Q 1088 -75 661 373  Q 234 822 234 1631  Q 234 2469 665 2931  Q 1097 3394 1784 3394  Q 2450 3394 2872 2941  Q 3294 2488 3294 1666  Q 3294 1616 3291 1516  L 816 1516  Q 847 969 1125 678  Q 1403 388 1819 388  Q 2128 388 2347 550  Q 2566 713 2694 1069  z M 847 1978  L 2700 1978  Q 2663 2397 2488 2606  Q 2219 2931 1791 2931  Q 1403 2931 1139 2672  Q 875 2413 847 1978  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-67\" d=\"M 319 -275  L 866 -356  Q 900 -609 1056 -725  Q 1266 -881 1628 -881  Q 2019 -881 2231 -725  Q 2444 -569 2519 -288  Q 2563 -116 2559 434  Q 2191 0 1641 0  Q 956 0 581 494  Q 206 988 206 1678  Q 206 2153 378 2554  Q 550 2956 876 3175  Q 1203 3394 1644 3394  Q 2231 3394 2613 2919  L 2613 3319  L 3131 3319  L 3131 450  Q 3131 -325 2973 -648  Q 2816 -972 2473 -1159  Q 2131 -1347 1631 -1347  Q 1038 -1347 672 -1080  Q 306 -813 319 -275  z M 784 1719  Q 784 1066 1043 766  Q 1303 466 1694 466  Q 2081 466 2343 764  Q 2606 1063 2606 1700  Q 2606 2309 2336 2618  Q 2066 2928 1684 2928  Q 1309 2928 1046 2623  Q 784 2319 784 1719  z \" transform=\"scale(0.015625)\" />\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-6e\" />\n",
       "     <use xlink:href=\"#ArialMT-65\" x=\"55.615234\" />\n",
       "     <use xlink:href=\"#ArialMT-67\" x=\"111.230469\" />\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       "</svg></g>\n",
       "\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>node14-&gt;leaf15</title>\n",
       "<path fill=\"none\" stroke=\"#444443\" stroke-width=\"0.3\" d=\"M393.54,-134.95C391.24,-121.69 385.31,-87.54 381.35,-64.67\" />\n",
       "<polygon fill=\"#444443\" stroke=\"#444443\" stroke-width=\"0.3\" points=\"382.69,-64.24 380.63,-60.54 379.93,-64.72 382.69,-64.24\" />\n",
       "</g>\n",
       "\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>leaf16</title>\n",
       "<polygon fill=\"none\" stroke=\"#444443\" stroke-width=\"0\" points=\"422,-57 403,-57 403,-26 422,-26 422,-57\" />\n",
       "<svg width=\"11px\" height=\"23px\" viewBox=\"0 0 15.26625 30.941329\" version=\"1.1\" preserveAspectRatio=\"xMinYMin meet\" x=\"407.5\" y=\"-52.5\">\n",
       " <metadata>\n",
       "  <rdf:RDF>\n",
       "   <ns2:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\" />\n",
       "    <dc:date>2023-04-11T19:23:53.521850</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <ns2:Agent>\n",
       "      <dc:title>Matplotlib v3.5.2, https://matplotlib.org/</dc:title>\n",
       "     </ns2:Agent>\n",
       "    </dc:creator>\n",
       "   </ns2:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 30.941329  L 15.26625 30.941329  L 15.26625 -0  L 0 -0  L 0 30.941329  z \" style=\"fill: none\" />\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 13.975125 6.342  C 13.975125 5.509176 13.811077 4.684451 13.492369 3.915022  C 13.173661 3.145593 12.706492 2.446424 12.117596 1.857529  C 11.528701 1.268633 10.829532 0.801464 10.060103 0.482756  C 9.290674 0.164048 8.465949 0 7.633125 0  C 6.800301 0 5.975576 0.164048 5.206147 0.482756  C 4.436718 0.801464 3.737549 1.268633 3.148654 1.857529  C 2.559758 2.446424 2.092589 3.145593 1.773881 3.915022  C 1.455173 4.684451 1.291125 5.509176 1.291125 6.342  C 1.291125 7.174824 1.455173 7.999549 1.773881 8.768978  C 2.092589 9.538407 2.559758 10.237576 3.148654 10.826471  C 3.737549 11.415367 4.436718 11.882536 5.206147 12.201244  C 5.975576 12.519952 6.800301 12.684 7.633125 12.684  C 8.465949 12.684 9.290674 12.519952 10.060103 12.201244  C 10.829532 11.882536 11.528701 11.415367 12.117596 10.826471  C 12.706492 10.237576 13.173661 9.538407 13.492369 8.768978  C 13.811077 7.999549 13.975125 7.174824 13.975125 6.342  M 7.633125 6.342  M 13.975125 6.342  z \" style=\"fill: #a1dab4; stroke: #444443; stroke-width: 0.5; stroke-linejoin: miter\" />\n",
       "   </g>\n",
       "   <g id=\"text_1\">\n",
       "    \n",
       "    <g style=\"fill: #444443\" transform=\"translate(0 19.633391)scale(0.09 -0.09)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-6e\" d=\"M 422 0  L 422 3319  L 928 3319  L 928 2847  Q 1294 3394 1984 3394  Q 2284 3394 2536 3286  Q 2788 3178 2913 3003  Q 3038 2828 3088 2588  Q 3119 2431 3119 2041  L 3119 0  L 2556 0  L 2556 2019  Q 2556 2363 2490 2533  Q 2425 2703 2258 2804  Q 2091 2906 1866 2906  Q 1506 2906 1245 2678  Q 984 2450 984 1813  L 984 0  L 422 0  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-3d\" d=\"M 3381 2694  L 356 2694  L 356 3219  L 3381 3219  L 3381 2694  z M 3381 1303  L 356 1303  L 356 1828  L 3381 1828  L 3381 1303  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-33\" d=\"M 269 1209  L 831 1284  Q 928 806 1161 595  Q 1394 384 1728 384  Q 2125 384 2398 659  Q 2672 934 2672 1341  Q 2672 1728 2419 1979  Q 2166 2231 1775 2231  Q 1616 2231 1378 2169  L 1441 2663  Q 1497 2656 1531 2656  Q 1891 2656 2178 2843  Q 2466 3031 2466 3422  Q 2466 3731 2256 3934  Q 2047 4138 1716 4138  Q 1388 4138 1169 3931  Q 950 3725 888 3313  L 325 3413  Q 428 3978 793 4289  Q 1159 4600 1703 4600  Q 2078 4600 2393 4439  Q 2709 4278 2876 4000  Q 3044 3722 3044 3409  Q 3044 3113 2884 2869  Q 2725 2625 2413 2481  Q 2819 2388 3044 2092  Q 3269 1797 3269 1353  Q 3269 753 2831 336  Q 2394 -81 1725 -81  Q 1122 -81 723 278  Q 325 638 269 1209  z \" transform=\"scale(0.015625)\" />\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-6e\" />\n",
       "     <use xlink:href=\"#ArialMT-3d\" x=\"55.615234\" />\n",
       "     <use xlink:href=\"#ArialMT-33\" x=\"114.013672\" />\n",
       "    </g>\n",
       "    \n",
       "    <g style=\"fill: #444443\" transform=\"translate(0.378281 29.152579)scale(0.09 -0.09)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-70\" d=\"M 422 -1272  L 422 3319  L 934 3319  L 934 2888  Q 1116 3141 1344 3267  Q 1572 3394 1897 3394  Q 2322 3394 2647 3175  Q 2972 2956 3137 2557  Q 3303 2159 3303 1684  Q 3303 1175 3120 767  Q 2938 359 2589 142  Q 2241 -75 1856 -75  Q 1575 -75 1351 44  Q 1128 163 984 344  L 984 -1272  L 422 -1272  z M 931 1641  Q 931 1000 1190 694  Q 1450 388 1819 388  Q 2194 388 2461 705  Q 2728 1022 2728 1688  Q 2728 2322 2467 2637  Q 2206 2953 1844 2953  Q 1484 2953 1207 2617  Q 931 2281 931 1641  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-6f\" d=\"M 213 1659  Q 213 2581 725 3025  Q 1153 3394 1769 3394  Q 2453 3394 2887 2945  Q 3322 2497 3322 1706  Q 3322 1066 3130 698  Q 2938 331 2570 128  Q 2203 -75 1769 -75  Q 1072 -75 642 372  Q 213 819 213 1659  z M 791 1659  Q 791 1022 1069 705  Q 1347 388 1769 388  Q 2188 388 2466 706  Q 2744 1025 2744 1678  Q 2744 2294 2464 2611  Q 2184 2928 1769 2928  Q 1347 2928 1069 2612  Q 791 2297 791 1659  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-7a\" d=\"M 125 0  L 125 456  L 2238 2881  Q 1878 2863 1603 2863  L 250 2863  L 250 3319  L 2963 3319  L 2963 2947  L 1166 841  L 819 456  Q 1197 484 1528 484  L 3063 484  L 3063 0  L 125 0  z \" transform=\"scale(0.015625)\" />\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-70\" />\n",
       "     <use xlink:href=\"#ArialMT-6f\" x=\"55.615234\" />\n",
       "     <use xlink:href=\"#ArialMT-7a\" x=\"111.230469\" />\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       "</svg></g>\n",
       "\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>node14-&gt;leaf16</title>\n",
       "<path fill=\"none\" stroke=\"#444443\" stroke-width=\"0.3\" d=\"M395.52,-134.95C398.08,-121 404.89,-83.96 409.06,-61.22\" />\n",
       "<polygon fill=\"#444443\" stroke=\"#444443\" stroke-width=\"0.3\" points=\"410.46,-61.34 409.81,-57.15 407.71,-60.83 410.46,-61.34\" />\n",
       "</g>\n",
       "\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>leaf17</title>\n",
       "<polygon fill=\"none\" stroke=\"#444443\" stroke-width=\"0\" points=\"508.5,-166.5 472.5,-166.5 472.5,-116.5 508.5,-116.5 508.5,-166.5\" />\n",
       "<svg width=\"28px\" height=\"42px\" viewBox=\"0 0 38.052 57.324049\" version=\"1.1\" preserveAspectRatio=\"xMinYMin meet\" x=\"476.5\" y=\"-162.5\">\n",
       " <metadata>\n",
       "  <rdf:RDF>\n",
       "   <ns2:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\" />\n",
       "    <dc:date>2023-04-11T19:23:53.557988</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <ns2:Agent>\n",
       "      <dc:title>Matplotlib v3.5.2, https://matplotlib.org/</dc:title>\n",
       "     </ns2:Agent>\n",
       "    </dc:creator>\n",
       "   </ns2:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 57.324049  L 38.052 57.324049  L 38.052 0  L 0 0  L 0 57.324049  z \" style=\"fill: none\" />\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 38.052 19.026  C 38.052 16.527529 37.559856 14.053352 36.603732 11.745065  C 35.647608 9.436778 34.2461 7.339272 32.479414 5.572586  C 30.712728 3.8059 28.615222 2.404392 26.306935 1.448268  C 23.998648 0.492144 21.524471 -0 19.026 -0  C 16.527529 -0 14.053352 0.492144 11.745065 1.448268  C 9.436778 2.404392 7.339272 3.8059 5.572586 5.572586  C 3.8059 7.339272 2.404392 9.436778 1.448268 11.745065  C 0.492144 14.053352 0 16.527529 0 19.026  C 0 21.524471 0.492144 23.998648 1.448268 26.306935  C 2.404392 28.615222 3.8059 30.712728 5.572586 32.479414  C 7.339272 34.2461 9.436778 35.647608 11.745065 36.603732  C 14.053352 37.559856 16.527529 38.052 19.026 38.052  C 21.524471 38.052 23.998648 37.559856 26.306935 36.603732  C 28.615222 35.647608 30.712728 34.2461 32.479414 32.479414  C 34.2461 30.712728 35.647608 28.615222 36.603732 26.306935  C 37.559856 23.998648 38.052 21.524471 38.052 19.026  M 19.026 19.026  M 38.052 19.026  z \" style=\"fill: #a1dab4; stroke: #444443; stroke-width: 0.5; stroke-linejoin: miter\" />\n",
       "   </g>\n",
       "   <g id=\"text_1\">\n",
       "    \n",
       "    <g style=\"fill: #444443\" transform=\"translate(8.890453 46.016111)scale(0.09 -0.09)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-6e\" d=\"M 422 0  L 422 3319  L 928 3319  L 928 2847  Q 1294 3394 1984 3394  Q 2284 3394 2536 3286  Q 2788 3178 2913 3003  Q 3038 2828 3088 2588  Q 3119 2431 3119 2041  L 3119 0  L 2556 0  L 2556 2019  Q 2556 2363 2490 2533  Q 2425 2703 2258 2804  Q 2091 2906 1866 2906  Q 1506 2906 1245 2678  Q 984 2450 984 1813  L 984 0  L 422 0  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-3d\" d=\"M 3381 2694  L 356 2694  L 356 3219  L 3381 3219  L 3381 2694  z M 3381 1303  L 356 1303  L 356 1828  L 3381 1828  L 3381 1303  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-32\" d=\"M 3222 541  L 3222 0  L 194 0  Q 188 203 259 391  Q 375 700 629 1000  Q 884 1300 1366 1694  Q 2113 2306 2375 2664  Q 2638 3022 2638 3341  Q 2638 3675 2398 3904  Q 2159 4134 1775 4134  Q 1369 4134 1125 3890  Q 881 3647 878 3216  L 300 3275  Q 359 3922 746 4261  Q 1134 4600 1788 4600  Q 2447 4600 2831 4234  Q 3216 3869 3216 3328  Q 3216 3053 3103 2787  Q 2991 2522 2730 2228  Q 2469 1934 1863 1422  Q 1356 997 1212 845  Q 1069 694 975 541  L 3222 541  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-34\" d=\"M 2069 0  L 2069 1097  L 81 1097  L 81 1613  L 2172 4581  L 2631 4581  L 2631 1613  L 3250 1613  L 3250 1097  L 2631 1097  L 2631 0  L 2069 0  z M 2069 1613  L 2069 3678  L 634 1613  L 2069 1613  z \" transform=\"scale(0.015625)\" />\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-6e\" />\n",
       "     <use xlink:href=\"#ArialMT-3d\" x=\"55.615234\" />\n",
       "     <use xlink:href=\"#ArialMT-32\" x=\"114.013672\" />\n",
       "     <use xlink:href=\"#ArialMT-34\" x=\"169.628906\" />\n",
       "    </g>\n",
       "    \n",
       "    <g style=\"fill: #444443\" transform=\"translate(11.771156 55.535299)scale(0.09 -0.09)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-70\" d=\"M 422 -1272  L 422 3319  L 934 3319  L 934 2888  Q 1116 3141 1344 3267  Q 1572 3394 1897 3394  Q 2322 3394 2647 3175  Q 2972 2956 3137 2557  Q 3303 2159 3303 1684  Q 3303 1175 3120 767  Q 2938 359 2589 142  Q 2241 -75 1856 -75  Q 1575 -75 1351 44  Q 1128 163 984 344  L 984 -1272  L 422 -1272  z M 931 1641  Q 931 1000 1190 694  Q 1450 388 1819 388  Q 2194 388 2461 705  Q 2728 1022 2728 1688  Q 2728 2322 2467 2637  Q 2206 2953 1844 2953  Q 1484 2953 1207 2617  Q 931 2281 931 1641  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-6f\" d=\"M 213 1659  Q 213 2581 725 3025  Q 1153 3394 1769 3394  Q 2453 3394 2887 2945  Q 3322 2497 3322 1706  Q 3322 1066 3130 698  Q 2938 331 2570 128  Q 2203 -75 1769 -75  Q 1072 -75 642 372  Q 213 819 213 1659  z M 791 1659  Q 791 1022 1069 705  Q 1347 388 1769 388  Q 2188 388 2466 706  Q 2744 1025 2744 1678  Q 2744 2294 2464 2611  Q 2184 2928 1769 2928  Q 1347 2928 1069 2612  Q 791 2297 791 1659  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-7a\" d=\"M 125 0  L 125 456  L 2238 2881  Q 1878 2863 1603 2863  L 250 2863  L 250 3319  L 2963 3319  L 2963 2947  L 1166 841  L 819 456  Q 1197 484 1528 484  L 3063 484  L 3063 0  L 125 0  z \" transform=\"scale(0.015625)\" />\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-70\" />\n",
       "     <use xlink:href=\"#ArialMT-6f\" x=\"55.615234\" />\n",
       "     <use xlink:href=\"#ArialMT-7a\" x=\"111.230469\" />\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       "</svg></g>\n",
       "\n",
       "\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>node13</title>\n",
       "<text text-anchor=\"start\" x=\"343.5\" y=\"-213.9\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#444443\">Soca@1519.11</text>\n",
       "</g>\n",
       "\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>node13-&gt;node14</title>\n",
       "<path fill=\"none\" stroke=\"#444443\" stroke-width=\"0.3\" d=\"M385.28,-210.23C386.99,-197.72 391.04,-167.96 393.16,-152.38\" />\n",
       "<polygon fill=\"#444443\" stroke=\"#444443\" stroke-width=\"0.3\" points=\"394.59,-152.19 393.75,-148.04 391.82,-151.81 394.59,-152.19\" />\n",
       "</g>\n",
       "\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>node13-&gt;leaf17</title>\n",
       "<path fill=\"none\" stroke=\"#444443\" stroke-width=\"0.3\" d=\"M392.47,-210.48C408.53,-199.34 445.28,-173.86 468.94,-157.45\" />\n",
       "<polygon fill=\"#444443\" stroke=\"#444443\" stroke-width=\"0.3\" points=\"469.84,-158.53 472.33,-155.1 468.24,-156.23 469.84,-158.53\" />\n",
       "</g>\n",
       "\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>leaf18</title>\n",
       "<polygon fill=\"none\" stroke=\"#444443\" stroke-width=\"0\" points=\"459.5,-234 439.5,-234 439.5,-200 459.5,-200 459.5,-234\" />\n",
       "<svg width=\"12px\" height=\"26px\" viewBox=\"0 0 16.308 34.815758\" version=\"1.1\" preserveAspectRatio=\"xMinYMin meet\" x=\"443.5\" y=\"-230\">\n",
       " <metadata>\n",
       "  <rdf:RDF>\n",
       "   <ns2:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\" />\n",
       "    <dc:date>2023-04-11T19:23:53.594770</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <ns2:Agent>\n",
       "      <dc:title>Matplotlib v3.5.2, https://matplotlib.org/</dc:title>\n",
       "     </ns2:Agent>\n",
       "    </dc:creator>\n",
       "   </ns2:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M -0 34.815758  L 16.308 34.815758  L 16.308 0  L -0 0  L -0 34.815758  z \" style=\"fill: none\" />\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 16.308 8.154  C 16.308 7.083227 16.097081 6.022865 15.687314 5.033599  C 15.277546 4.044334 14.6769 3.145402 13.919749 2.388251  C 13.162598 1.6311 12.263666 1.030454 11.274401 0.620686  C 10.285135 0.210919 9.224773 0 8.154 0  C 7.083227 0 6.022865 0.210919 5.033599 0.620686  C 4.044334 1.030454 3.145402 1.6311 2.388251 2.388251  C 1.6311 3.145402 1.030454 4.044334 0.620686 5.033599  C 0.210919 6.022865 0 7.083227 0 8.154  C 0 9.224773 0.210919 10.285135 0.620686 11.274401  C 1.030454 12.263666 1.6311 13.162598 2.388251 13.919749  C 3.145402 14.6769 4.044334 15.277546 5.033599 15.687314  C 6.022865 16.097081 7.083227 16.308 8.154 16.308  C 9.224773 16.308 10.285135 16.097081 11.274401 15.687314  C 12.263666 15.277546 13.162598 14.6769 13.919749 13.919749  C 14.6769 13.162598 15.277546 12.263666 15.687314 11.274401  C 16.097081 10.285135 16.308 9.224773 16.308 8.154  M 8.154 8.154  M 16.308 8.154  z \" style=\"fill: #fefebb; stroke: #444443; stroke-width: 0.5; stroke-linejoin: miter\" />\n",
       "   </g>\n",
       "   <g id=\"text_1\">\n",
       "    \n",
       "    <g style=\"fill: #444443\" transform=\"translate(0.520875 23.402351)scale(0.09 -0.09)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-6e\" d=\"M 422 0  L 422 3319  L 928 3319  L 928 2847  Q 1294 3394 1984 3394  Q 2284 3394 2536 3286  Q 2788 3178 2913 3003  Q 3038 2828 3088 2588  Q 3119 2431 3119 2041  L 3119 0  L 2556 0  L 2556 2019  Q 2556 2363 2490 2533  Q 2425 2703 2258 2804  Q 2091 2906 1866 2906  Q 1506 2906 1245 2678  Q 984 2450 984 1813  L 984 0  L 422 0  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-3d\" d=\"M 3381 2694  L 356 2694  L 356 3219  L 3381 3219  L 3381 2694  z M 3381 1303  L 356 1303  L 356 1828  L 3381 1828  L 3381 1303  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-36\" d=\"M 3184 3459  L 2625 3416  Q 2550 3747 2413 3897  Q 2184 4138 1850 4138  Q 1581 4138 1378 3988  Q 1113 3794 959 3422  Q 806 3050 800 2363  Q 1003 2672 1297 2822  Q 1591 2972 1913 2972  Q 2475 2972 2870 2558  Q 3266 2144 3266 1488  Q 3266 1056 3080 686  Q 2894 316 2569 119  Q 2244 -78 1831 -78  Q 1128 -78 684 439  Q 241 956 241 2144  Q 241 3472 731 4075  Q 1159 4600 1884 4600  Q 2425 4600 2770 4297  Q 3116 3994 3184 3459  z M 888 1484  Q 888 1194 1011 928  Q 1134 663 1356 523  Q 1578 384 1822 384  Q 2178 384 2434 671  Q 2691 959 2691 1453  Q 2691 1928 2437 2201  Q 2184 2475 1800 2475  Q 1419 2475 1153 2201  Q 888 1928 888 1484  z \" transform=\"scale(0.015625)\" />\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-6e\" />\n",
       "     <use xlink:href=\"#ArialMT-3d\" x=\"55.615234\" />\n",
       "     <use xlink:href=\"#ArialMT-36\" x=\"114.013672\" />\n",
       "    </g>\n",
       "    \n",
       "    <g style=\"fill: #444443\" transform=\"translate(0.646734 32.921539)scale(0.09 -0.09)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-65\" d=\"M 2694 1069  L 3275 997  Q 3138 488 2766 206  Q 2394 -75 1816 -75  Q 1088 -75 661 373  Q 234 822 234 1631  Q 234 2469 665 2931  Q 1097 3394 1784 3394  Q 2450 3394 2872 2941  Q 3294 2488 3294 1666  Q 3294 1616 3291 1516  L 816 1516  Q 847 969 1125 678  Q 1403 388 1819 388  Q 2128 388 2347 550  Q 2566 713 2694 1069  z M 847 1978  L 2700 1978  Q 2663 2397 2488 2606  Q 2219 2931 1791 2931  Q 1403 2931 1139 2672  Q 875 2413 847 1978  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-67\" d=\"M 319 -275  L 866 -356  Q 900 -609 1056 -725  Q 1266 -881 1628 -881  Q 2019 -881 2231 -725  Q 2444 -569 2519 -288  Q 2563 -116 2559 434  Q 2191 0 1641 0  Q 956 0 581 494  Q 206 988 206 1678  Q 206 2153 378 2554  Q 550 2956 876 3175  Q 1203 3394 1644 3394  Q 2231 3394 2613 2919  L 2613 3319  L 3131 3319  L 3131 450  Q 3131 -325 2973 -648  Q 2816 -972 2473 -1159  Q 2131 -1347 1631 -1347  Q 1038 -1347 672 -1080  Q 306 -813 319 -275  z M 784 1719  Q 784 1066 1043 766  Q 1303 466 1694 466  Q 2081 466 2343 764  Q 2606 1063 2606 1700  Q 2606 2309 2336 2618  Q 2066 2928 1684 2928  Q 1309 2928 1046 2623  Q 784 2319 784 1719  z \" transform=\"scale(0.015625)\" />\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-6e\" />\n",
       "     <use xlink:href=\"#ArialMT-65\" x=\"55.615234\" />\n",
       "     <use xlink:href=\"#ArialMT-67\" x=\"111.230469\" />\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       "</svg></g>\n",
       "\n",
       "\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>node12-&gt;node13</title>\n",
       "<path fill=\"none\" stroke=\"#444443\" stroke-width=\"0.3\" d=\"M381.09,-250.82C381.72,-244.79 382.72,-235.15 383.47,-227.88\" />\n",
       "<polygon fill=\"#444443\" stroke=\"#444443\" stroke-width=\"0.3\" points=\"384.89,-227.83 383.91,-223.7 382.1,-227.54 384.89,-227.83\" />\n",
       "</g>\n",
       "\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>node12-&gt;leaf18</title>\n",
       "<path fill=\"none\" stroke=\"#444443\" stroke-width=\"0.3\" d=\"M390.39,-250.98C402.24,-244.37 422.18,-233.24 435.59,-225.76\" />\n",
       "<polygon fill=\"#444443\" stroke=\"#444443\" stroke-width=\"0.3\" points=\"436.37,-226.93 439.18,-223.76 435.01,-224.48 436.37,-226.93\" />\n",
       "</g>\n",
       "\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>node0</title>\n",
       "<text text-anchor=\"start\" x=\"274.5\" y=\"-313.9\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#444443\">Dinophysis fortii@30.50</text>\n",
       "</g>\n",
       "\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>node0-&gt;node1</title>\n",
       "<path fill=\"none\" stroke=\"#444443\" stroke-width=\"0.3\" d=\"M335.02,-310.28C328.94,-300.29 316.34,-279.6 309.05,-267.62\" />\n",
       "<polygon fill=\"#444443\" stroke=\"#444443\" stroke-width=\"0.3\" points=\"310.25,-266.89 306.97,-264.2 307.85,-268.35 310.25,-266.89\" />\n",
       "<text text-anchor=\"start\" x=\"320.5\" y=\"-275.8\" font-family=\"Times,serif\" font-size=\"14.00\">&#8804;</text>\n",
       "</g>\n",
       "\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>node0-&gt;node12</title>\n",
       "<path fill=\"none\" stroke=\"#444443\" stroke-width=\"0.3\" d=\"M342.68,-310.28C349.98,-300.29 365.09,-279.6 373.84,-267.62\" />\n",
       "<polygon fill=\"#444443\" stroke=\"#444443\" stroke-width=\"0.3\" points=\"375.11,-268.26 376.34,-264.2 372.85,-266.6 375.11,-268.26\" />\n",
       "<text text-anchor=\"start\" x=\"370.5\" y=\"-275.8\" font-family=\"Times,serif\" font-size=\"14.00\">&gt;</text>\n",
       "</g>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>legend</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-width=\"0\" points=\"455.5,-339 409.5,-339 409.5,-295 455.5,-295 455.5,-339\" />\n",
       "<svg width=\"42px\" height=\"40px\" viewBox=\"0 0 56.182812 54.36\" version=\"1.1\" preserveAspectRatio=\"xMinYMin meet\" x=\"411.5\" y=\"-337\">\n",
       " <metadata>\n",
       "  <rdf:RDF>\n",
       "   <ns2:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\" />\n",
       "    <dc:date>2023-04-11T19:23:53.158968</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <ns2:Agent>\n",
       "      <dc:title>Matplotlib v3.5.2, https://matplotlib.org/</dc:title>\n",
       "     </ns2:Agent>\n",
       "    </dc:creator>\n",
       "   </ns2:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 54.36  L 56.182812 54.36  L 56.182812 0  L 0 0  L 0 54.36  z \" style=\"fill: none\" />\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_2\">\n",
       "     <path d=\"M 2 54.262031  L 54.182812 54.262031  Q 56.182812 54.262031 56.182812 52.262031  L 56.182812 2.097969  Q 56.182812 0.097969 54.182812 0.097969  L 2 0.097969  Q 0 0.097969 0 2.097969  L 0 52.262031  Q 0 54.262031 2 54.262031  z \" style=\"fill: #ffffff; opacity: 0.8; stroke: #444443; stroke-width: 0.5; stroke-linejoin: miter\" />\n",
       "    </g>\n",
       "    <g id=\"text_1\">\n",
       "     \n",
       "     <g style=\"fill: #444443\" transform=\"translate(10.501562 15.696406)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-Bold-74\" d=\"M 1759 4494  L 1759 3500  L 2913 3500  L 2913 2700  L 1759 2700  L 1759 1216  Q 1759 972 1856 886  Q 1953 800 2241 800  L 2816 800  L 2816 0  L 1856 0  Q 1194 0 917 276  Q 641 553 641 1216  L 641 2700  L 84 2700  L 84 3500  L 641 3500  L 641 4494  L 1759 4494  z \" transform=\"scale(0.015625)\" />\n",
       "       <path id=\"DejaVuSans-Bold-61\" d=\"M 2106 1575  Q 1756 1575 1579 1456  Q 1403 1338 1403 1106  Q 1403 894 1545 773  Q 1688 653 1941 653  Q 2256 653 2472 879  Q 2688 1106 2688 1447  L 2688 1575  L 2106 1575  z M 3816 1997  L 3816 0  L 2688 0  L 2688 519  Q 2463 200 2181 54  Q 1900 -91 1497 -91  Q 953 -91 614 226  Q 275 544 275 1050  Q 275 1666 698 1953  Q 1122 2241 2028 2241  L 2688 2241  L 2688 2328  Q 2688 2594 2478 2717  Q 2269 2841 1825 2841  Q 1466 2841 1156 2769  Q 847 2697 581 2553  L 581 3406  Q 941 3494 1303 3539  Q 1666 3584 2028 3584  Q 2975 3584 3395 3211  Q 3816 2838 3816 1997  z \" transform=\"scale(0.015625)\" />\n",
       "       <path id=\"DejaVuSans-Bold-72\" d=\"M 3138 2547  Q 2991 2616 2845 2648  Q 2700 2681 2553 2681  Q 2122 2681 1889 2404  Q 1656 2128 1656 1613  L 1656 0  L 538 0  L 538 3500  L 1656 3500  L 1656 2925  Q 1872 3269 2151 3426  Q 2431 3584 2822 3584  Q 2878 3584 2943 3579  Q 3009 3575 3134 3559  L 3138 2547  z \" transform=\"scale(0.015625)\" />\n",
       "       <path id=\"DejaVuSans-Bold-67\" d=\"M 2919 594  Q 2688 288 2409 144  Q 2131 0 1766 0  Q 1125 0 706 504  Q 288 1009 288 1791  Q 288 2575 706 3076  Q 1125 3578 1766 3578  Q 2131 3578 2409 3434  Q 2688 3291 2919 2981  L 2919 3500  L 4044 3500  L 4044 353  Q 4044 -491 3511 -936  Q 2978 -1381 1966 -1381  Q 1638 -1381 1331 -1331  Q 1025 -1281 716 -1178  L 716 -306  Q 1009 -475 1290 -558  Q 1572 -641 1856 -641  Q 2406 -641 2662 -400  Q 2919 -159 2919 353  L 2919 594  z M 2181 2772  Q 1834 2772 1640 2515  Q 1447 2259 1447 1791  Q 1447 1309 1634 1061  Q 1822 813 2181 813  Q 2531 813 2725 1069  Q 2919 1325 2919 1791  Q 2919 2259 2725 2515  Q 2531 2772 2181 2772  z \" transform=\"scale(0.015625)\" />\n",
       "       <path id=\"DejaVuSans-Bold-65\" d=\"M 4031 1759  L 4031 1441  L 1416 1441  Q 1456 1047 1700 850  Q 1944 653 2381 653  Q 2734 653 3104 758  Q 3475 863 3866 1075  L 3866 213  Q 3469 63 3072 -14  Q 2675 -91 2278 -91  Q 1328 -91 801 392  Q 275 875 275 1747  Q 275 2603 792 3093  Q 1309 3584 2216 3584  Q 3041 3584 3536 3087  Q 4031 2591 4031 1759  z M 2881 2131  Q 2881 2450 2695 2645  Q 2509 2841 2209 2841  Q 1884 2841 1681 2658  Q 1478 2475 1428 2131  L 2881 2131  z \" transform=\"scale(0.015625)\" />\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-Bold-74\" />\n",
       "      <use xlink:href=\"#DejaVuSans-Bold-61\" x=\"47.802734\" />\n",
       "      <use xlink:href=\"#DejaVuSans-Bold-72\" x=\"115.283203\" />\n",
       "      <use xlink:href=\"#DejaVuSans-Bold-67\" x=\"164.599609\" />\n",
       "      <use xlink:href=\"#DejaVuSans-Bold-65\" x=\"236.181641\" />\n",
       "      <use xlink:href=\"#DejaVuSans-Bold-74\" x=\"304.003906\" />\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"patch_3\">\n",
       "     <path d=\"M 8 30.012031  L 28 30.012031  L 28 23.012031  L 8 23.012031  z \" style=\"fill: #fefebb; stroke: #444443; stroke-width: 0.4; stroke-linejoin: miter\" />\n",
       "    </g>\n",
       "    <g id=\"text_2\">\n",
       "     \n",
       "     <g style=\"fill: #444443\" transform=\"translate(31.5 30.012031)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"ArialMT-6e\" d=\"M 422 0  L 422 3319  L 928 3319  L 928 2847  Q 1294 3394 1984 3394  Q 2284 3394 2536 3286  Q 2788 3178 2913 3003  Q 3038 2828 3088 2588  Q 3119 2431 3119 2041  L 3119 0  L 2556 0  L 2556 2019  Q 2556 2363 2490 2533  Q 2425 2703 2258 2804  Q 2091 2906 1866 2906  Q 1506 2906 1245 2678  Q 984 2450 984 1813  L 984 0  L 422 0  z \" transform=\"scale(0.015625)\" />\n",
       "       <path id=\"ArialMT-65\" d=\"M 2694 1069  L 3275 997  Q 3138 488 2766 206  Q 2394 -75 1816 -75  Q 1088 -75 661 373  Q 234 822 234 1631  Q 234 2469 665 2931  Q 1097 3394 1784 3394  Q 2450 3394 2872 2941  Q 3294 2488 3294 1666  Q 3294 1616 3291 1516  L 816 1516  Q 847 969 1125 678  Q 1403 388 1819 388  Q 2128 388 2347 550  Q 2566 713 2694 1069  z M 847 1978  L 2700 1978  Q 2663 2397 2488 2606  Q 2219 2931 1791 2931  Q 1403 2931 1139 2672  Q 875 2413 847 1978  z \" transform=\"scale(0.015625)\" />\n",
       "       <path id=\"ArialMT-67\" d=\"M 319 -275  L 866 -356  Q 900 -609 1056 -725  Q 1266 -881 1628 -881  Q 2019 -881 2231 -725  Q 2444 -569 2519 -288  Q 2563 -116 2559 434  Q 2191 0 1641 0  Q 956 0 581 494  Q 206 988 206 1678  Q 206 2153 378 2554  Q 550 2956 876 3175  Q 1203 3394 1644 3394  Q 2231 3394 2613 2919  L 2613 3319  L 3131 3319  L 3131 450  Q 3131 -325 2973 -648  Q 2816 -972 2473 -1159  Q 2131 -1347 1631 -1347  Q 1038 -1347 672 -1080  Q 306 -813 319 -275  z M 784 1719  Q 784 1066 1043 766  Q 1303 466 1694 466  Q 2081 466 2343 764  Q 2606 1063 2606 1700  Q 2606 2309 2336 2618  Q 2066 2928 1684 2928  Q 1309 2928 1046 2623  Q 784 2319 784 1719  z \" transform=\"scale(0.015625)\" />\n",
       "      </defs>\n",
       "      <use xlink:href=\"#ArialMT-6e\" />\n",
       "      <use xlink:href=\"#ArialMT-65\" x=\"55.615234\" />\n",
       "      <use xlink:href=\"#ArialMT-67\" x=\"111.230469\" />\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"patch_4\">\n",
       "     <path d=\"M 8 44.274531  L 28 44.274531  L 28 37.274531  L 8 37.274531  z \" style=\"fill: #a1dab4; stroke: #444443; stroke-width: 0.4; stroke-linejoin: miter\" />\n",
       "    </g>\n",
       "    <g id=\"text_3\">\n",
       "     \n",
       "     <g style=\"fill: #444443\" transform=\"translate(31.5 44.274531)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"ArialMT-70\" d=\"M 422 -1272  L 422 3319  L 934 3319  L 934 2888  Q 1116 3141 1344 3267  Q 1572 3394 1897 3394  Q 2322 3394 2647 3175  Q 2972 2956 3137 2557  Q 3303 2159 3303 1684  Q 3303 1175 3120 767  Q 2938 359 2589 142  Q 2241 -75 1856 -75  Q 1575 -75 1351 44  Q 1128 163 984 344  L 984 -1272  L 422 -1272  z M 931 1641  Q 931 1000 1190 694  Q 1450 388 1819 388  Q 2194 388 2461 705  Q 2728 1022 2728 1688  Q 2728 2322 2467 2637  Q 2206 2953 1844 2953  Q 1484 2953 1207 2617  Q 931 2281 931 1641  z \" transform=\"scale(0.015625)\" />\n",
       "       <path id=\"ArialMT-6f\" d=\"M 213 1659  Q 213 2581 725 3025  Q 1153 3394 1769 3394  Q 2453 3394 2887 2945  Q 3322 2497 3322 1706  Q 3322 1066 3130 698  Q 2938 331 2570 128  Q 2203 -75 1769 -75  Q 1072 -75 642 372  Q 213 819 213 1659  z M 791 1659  Q 791 1022 1069 705  Q 1347 388 1769 388  Q 2188 388 2466 706  Q 2744 1025 2744 1678  Q 2744 2294 2464 2611  Q 2184 2928 1769 2928  Q 1347 2928 1069 2612  Q 791 2297 791 1659  z \" transform=\"scale(0.015625)\" />\n",
       "       <path id=\"ArialMT-7a\" d=\"M 125 0  L 125 456  L 2238 2881  Q 1878 2863 1603 2863  L 250 2863  L 250 3319  L 2963 3319  L 2963 2947  L 1166 841  L 819 456  Q 1197 484 1528 484  L 3063 484  L 3063 0  L 125 0  z \" transform=\"scale(0.015625)\" />\n",
       "      </defs>\n",
       "      <use xlink:href=\"#ArialMT-70\" />\n",
       "      <use xlink:href=\"#ArialMT-6f\" x=\"55.615234\" />\n",
       "      <use xlink:href=\"#ArialMT-7a\" x=\"111.230469\" />\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       "</svg></g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<dtreeviz.trees.DTreeViz at 0x145fa2760>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viz = dtreeviz(dt_clf, X, y,\n",
    "                target_name=\"target\",\n",
    "                feature_names=X.columns,\n",
    "                class_names=[\"neg\", \"poz\"],\n",
    "             fancy=False,\n",
    "               scale=1.5\n",
    "              )\n",
    "\n",
    "# Save the visualization as a PNG file\n",
    "viz_file_name = f\"{dir_path}/DT_visualisation.svg\"\n",
    "viz.save(viz_file_name)\n",
    "print(f\"Visualization saved as: {viz_file_name}\")\n",
    "\n",
    "# Display the visualization\n",
    "viz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0db8e4c-76d7-4b28-a874-e70f5256bf91",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be1409e-0948-4107-bc34-558b1bb08a35",
   "metadata": {},
   "source": [
    "#### Model evaluation (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d8c5f08-1792-4ae9-b2ca-e72b2c24fe00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 486 candidates, totalling 1458 fits\n",
      "Best estimator saved as: models/11042023_1923/RandomForestClassifier-HAB_modelling_5_8-11042023_1923.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>185</th>\n",
       "      <th>386</th>\n",
       "      <th>447</th>\n",
       "      <th>285</th>\n",
       "      <th>348</th>\n",
       "      <th>280</th>\n",
       "      <th>453</th>\n",
       "      <th>12</th>\n",
       "      <th>328</th>\n",
       "      <th>87</th>\n",
       "      <th>327</th>\n",
       "      <th>6</th>\n",
       "      <th>420</th>\n",
       "      <th>53</th>\n",
       "      <th>225</th>\n",
       "      <th>356</th>\n",
       "      <th>345</th>\n",
       "      <th>...</th>\n",
       "      <th>76</th>\n",
       "      <th>434</th>\n",
       "      <th>101</th>\n",
       "      <th>145</th>\n",
       "      <th>296</th>\n",
       "      <th>104</th>\n",
       "      <th>98</th>\n",
       "      <th>410</th>\n",
       "      <th>236</th>\n",
       "      <th>157</th>\n",
       "      <th>261</th>\n",
       "      <th>239</th>\n",
       "      <th>109</th>\n",
       "      <th>18</th>\n",
       "      <th>149</th>\n",
       "      <th>154</th>\n",
       "      <th>302</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>0.939862</td>\n",
       "      <td>0.295661</td>\n",
       "      <td>0.313121</td>\n",
       "      <td>0.373399</td>\n",
       "      <td>0.856021</td>\n",
       "      <td>0.318739</td>\n",
       "      <td>0.862027</td>\n",
       "      <td>0.275805</td>\n",
       "      <td>0.329109</td>\n",
       "      <td>0.87792</td>\n",
       "      <td>0.323497</td>\n",
       "      <td>0.280151</td>\n",
       "      <td>1.438709</td>\n",
       "      <td>1.321248</td>\n",
       "      <td>0.314814</td>\n",
       "      <td>0.805409</td>\n",
       "      <td>0.843328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.839594</td>\n",
       "      <td>0.267982</td>\n",
       "      <td>1.229292</td>\n",
       "      <td>1.225518</td>\n",
       "      <td>1.044927</td>\n",
       "      <td>1.32666</td>\n",
       "      <td>1.344771</td>\n",
       "      <td>0.807215</td>\n",
       "      <td>0.894717</td>\n",
       "      <td>1.89873</td>\n",
       "      <td>1.549682</td>\n",
       "      <td>0.936154</td>\n",
       "      <td>0.286719</td>\n",
       "      <td>0.737302</td>\n",
       "      <td>1.304765</td>\n",
       "      <td>1.278416</td>\n",
       "      <td>0.999747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.002491</td>\n",
       "      <td>0.01626</td>\n",
       "      <td>0.00544</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.020517</td>\n",
       "      <td>0.014038</td>\n",
       "      <td>0.002856</td>\n",
       "      <td>0.008103</td>\n",
       "      <td>0.01969</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.005756</td>\n",
       "      <td>0.005555</td>\n",
       "      <td>0.002092</td>\n",
       "      <td>0.006072</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008129</td>\n",
       "      <td>0.001865</td>\n",
       "      <td>0.010357</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>0.005145</td>\n",
       "      <td>0.015622</td>\n",
       "      <td>0.010404</td>\n",
       "      <td>0.007388</td>\n",
       "      <td>0.003722</td>\n",
       "      <td>0.074618</td>\n",
       "      <td>0.006707</td>\n",
       "      <td>0.004758</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.011362</td>\n",
       "      <td>0.00199</td>\n",
       "      <td>0.006205</td>\n",
       "      <td>0.002888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.145191</td>\n",
       "      <td>0.052945</td>\n",
       "      <td>0.05508</td>\n",
       "      <td>0.053324</td>\n",
       "      <td>0.142345</td>\n",
       "      <td>0.05561</td>\n",
       "      <td>0.141406</td>\n",
       "      <td>0.053363</td>\n",
       "      <td>0.053647</td>\n",
       "      <td>0.13688</td>\n",
       "      <td>0.062083</td>\n",
       "      <td>0.051576</td>\n",
       "      <td>0.218745</td>\n",
       "      <td>0.225622</td>\n",
       "      <td>0.051039</td>\n",
       "      <td>0.145442</td>\n",
       "      <td>0.143752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.13627</td>\n",
       "      <td>0.053227</td>\n",
       "      <td>0.223092</td>\n",
       "      <td>0.227322</td>\n",
       "      <td>0.148055</td>\n",
       "      <td>0.222411</td>\n",
       "      <td>0.214007</td>\n",
       "      <td>0.142191</td>\n",
       "      <td>0.133081</td>\n",
       "      <td>0.622419</td>\n",
       "      <td>0.215355</td>\n",
       "      <td>0.134774</td>\n",
       "      <td>0.056354</td>\n",
       "      <td>0.14077</td>\n",
       "      <td>0.226908</td>\n",
       "      <td>0.263143</td>\n",
       "      <td>0.143507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.015108</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.002263</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.002039</td>\n",
       "      <td>0.004946</td>\n",
       "      <td>0.00063</td>\n",
       "      <td>0.001714</td>\n",
       "      <td>0.011912</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>0.002388</td>\n",
       "      <td>0.004804</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.002257</td>\n",
       "      <td>0.002362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003121</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.003889</td>\n",
       "      <td>0.003749</td>\n",
       "      <td>0.002885</td>\n",
       "      <td>0.004196</td>\n",
       "      <td>0.001419</td>\n",
       "      <td>0.00362</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.062038</td>\n",
       "      <td>0.002026</td>\n",
       "      <td>0.001257</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>0.007386</td>\n",
       "      <td>0.003016</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.00033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__class_weight</th>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>None</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>None</td>\n",
       "      <td>balanced</td>\n",
       "      <td>None</td>\n",
       "      <td>balanced</td>\n",
       "      <td>None</td>\n",
       "      <td>balanced</td>\n",
       "      <td>None</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>balanced</td>\n",
       "      <td>None</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>None</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced_subsample</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__criterion</th>\n",
       "      <td>gini</td>\n",
       "      <td>entropy</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>gini</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>gini</td>\n",
       "      <td>gini</td>\n",
       "      <td>entropy</td>\n",
       "      <td>gini</td>\n",
       "      <td>gini</td>\n",
       "      <td>entropy</td>\n",
       "      <td>gini</td>\n",
       "      <td>entropy</td>\n",
       "      <td>gini</td>\n",
       "      <td>gini</td>\n",
       "      <td>...</td>\n",
       "      <td>entropy</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>entropy</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>entropy</td>\n",
       "      <td>entropy</td>\n",
       "      <td>entropy</td>\n",
       "      <td>entropy</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>entropy</td>\n",
       "      <td>entropy</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>gini</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>log_loss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__n_estimators</th>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>100</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>...</td>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>300</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>300</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_smt__k_neighbors</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_smt__sampling_strategy</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_under__sampling_strategy</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'clf__class_weight': 'balanced_subsample', 'c...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': 'balanced_subsample', 'c...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': 'balanced_subsample', 'c...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': 'balanced_subsample', 'c...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': 'balanced_subsample', 'c...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': 'balanced_subsample', 'c...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': 'balanced_subsample', 'c...</td>\n",
       "      <td>{'clf__class_weight': 'balanced_subsample', 'c...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': 'balanced_subsample', 'c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_recall</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_recall</th>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_recall</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.47619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_recall</th>\n",
       "      <td>0.578571</td>\n",
       "      <td>0.499206</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.434127</td>\n",
       "      <td>0.580159</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.499206</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.434127</td>\n",
       "      <td>0.451587</td>\n",
       "      <td>0.434127</td>\n",
       "      <td>0.434127</td>\n",
       "      <td>0.499206</td>\n",
       "      <td>0.48254</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.403175</td>\n",
       "      <td>0.529365</td>\n",
       "      <td>0.531746</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.402381</td>\n",
       "      <td>0.435714</td>\n",
       "      <td>0.434127</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.499206</td>\n",
       "      <td>0.402381</td>\n",
       "      <td>0.401587</td>\n",
       "      <td>0.451587</td>\n",
       "      <td>0.419048</td>\n",
       "      <td>0.419048</td>\n",
       "      <td>0.434127</td>\n",
       "      <td>0.48254</td>\n",
       "      <td>0.418254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_recall</th>\n",
       "      <td>0.092969</td>\n",
       "      <td>0.052176</td>\n",
       "      <td>0.030553</td>\n",
       "      <td>0.073334</td>\n",
       "      <td>0.059487</td>\n",
       "      <td>0.028857</td>\n",
       "      <td>0.050992</td>\n",
       "      <td>0.052176</td>\n",
       "      <td>0.01944</td>\n",
       "      <td>0.059487</td>\n",
       "      <td>0.058332</td>\n",
       "      <td>0.071066</td>\n",
       "      <td>0.071066</td>\n",
       "      <td>0.034794</td>\n",
       "      <td>0.070129</td>\n",
       "      <td>0.084739</td>\n",
       "      <td>0.030553</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058364</td>\n",
       "      <td>0.132656</td>\n",
       "      <td>0.029696</td>\n",
       "      <td>0.030553</td>\n",
       "      <td>0.053699</td>\n",
       "      <td>0.010102</td>\n",
       "      <td>0.071066</td>\n",
       "      <td>0.050992</td>\n",
       "      <td>0.034794</td>\n",
       "      <td>0.053699</td>\n",
       "      <td>0.074417</td>\n",
       "      <td>0.019473</td>\n",
       "      <td>0.041148</td>\n",
       "      <td>0.078919</td>\n",
       "      <td>0.071066</td>\n",
       "      <td>0.070129</td>\n",
       "      <td>0.052031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_recall</th>\n",
       "      <td>11</td>\n",
       "      <td>98</td>\n",
       "      <td>127</td>\n",
       "      <td>277</td>\n",
       "      <td>331</td>\n",
       "      <td>5</td>\n",
       "      <td>199</td>\n",
       "      <td>98</td>\n",
       "      <td>87</td>\n",
       "      <td>331</td>\n",
       "      <td>230</td>\n",
       "      <td>331</td>\n",
       "      <td>331</td>\n",
       "      <td>98</td>\n",
       "      <td>167</td>\n",
       "      <td>27</td>\n",
       "      <td>127</td>\n",
       "      <td>...</td>\n",
       "      <td>429</td>\n",
       "      <td>57</td>\n",
       "      <td>44</td>\n",
       "      <td>127</td>\n",
       "      <td>435</td>\n",
       "      <td>298</td>\n",
       "      <td>331</td>\n",
       "      <td>199</td>\n",
       "      <td>98</td>\n",
       "      <td>435</td>\n",
       "      <td>454</td>\n",
       "      <td>230</td>\n",
       "      <td>373</td>\n",
       "      <td>373</td>\n",
       "      <td>331</td>\n",
       "      <td>167</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_precision</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.368421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_precision</th>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.34375</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.37931</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.346154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_precision</th>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_precision</th>\n",
       "      <td>0.51075</td>\n",
       "      <td>0.580952</td>\n",
       "      <td>0.599782</td>\n",
       "      <td>0.685185</td>\n",
       "      <td>0.68895</td>\n",
       "      <td>0.499415</td>\n",
       "      <td>0.614496</td>\n",
       "      <td>0.572606</td>\n",
       "      <td>0.55485</td>\n",
       "      <td>0.65812</td>\n",
       "      <td>0.640043</td>\n",
       "      <td>0.677831</td>\n",
       "      <td>0.660131</td>\n",
       "      <td>0.544056</td>\n",
       "      <td>0.58132</td>\n",
       "      <td>0.494962</td>\n",
       "      <td>0.565359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.453044</td>\n",
       "      <td>0.360111</td>\n",
       "      <td>0.358735</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.455437</td>\n",
       "      <td>0.415584</td>\n",
       "      <td>0.408838</td>\n",
       "      <td>0.382479</td>\n",
       "      <td>0.360837</td>\n",
       "      <td>0.438008</td>\n",
       "      <td>0.437229</td>\n",
       "      <td>0.386473</td>\n",
       "      <td>0.414021</td>\n",
       "      <td>0.430861</td>\n",
       "      <td>0.391863</td>\n",
       "      <td>0.35873</td>\n",
       "      <td>0.389707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_precision</th>\n",
       "      <td>0.059127</td>\n",
       "      <td>0.078919</td>\n",
       "      <td>0.00934</td>\n",
       "      <td>0.06929</td>\n",
       "      <td>0.077978</td>\n",
       "      <td>0.108151</td>\n",
       "      <td>0.031757</td>\n",
       "      <td>0.066585</td>\n",
       "      <td>0.02551</td>\n",
       "      <td>0.094405</td>\n",
       "      <td>0.072399</td>\n",
       "      <td>0.101974</td>\n",
       "      <td>0.068666</td>\n",
       "      <td>0.024778</td>\n",
       "      <td>0.061908</td>\n",
       "      <td>0.086227</td>\n",
       "      <td>0.072634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051949</td>\n",
       "      <td>0.039437</td>\n",
       "      <td>0.042524</td>\n",
       "      <td>0.035047</td>\n",
       "      <td>0.036027</td>\n",
       "      <td>0.009183</td>\n",
       "      <td>0.064194</td>\n",
       "      <td>0.052947</td>\n",
       "      <td>0.036129</td>\n",
       "      <td>0.035344</td>\n",
       "      <td>0.012244</td>\n",
       "      <td>0.041557</td>\n",
       "      <td>0.025988</td>\n",
       "      <td>0.112323</td>\n",
       "      <td>0.054926</td>\n",
       "      <td>0.02944</td>\n",
       "      <td>0.04674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_precision</th>\n",
       "      <td>228</td>\n",
       "      <td>73</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>254</td>\n",
       "      <td>30</td>\n",
       "      <td>85</td>\n",
       "      <td>122</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>140</td>\n",
       "      <td>71</td>\n",
       "      <td>268</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>366</td>\n",
       "      <td>483</td>\n",
       "      <td>484</td>\n",
       "      <td>469</td>\n",
       "      <td>359</td>\n",
       "      <td>427</td>\n",
       "      <td>432</td>\n",
       "      <td>465</td>\n",
       "      <td>482</td>\n",
       "      <td>395</td>\n",
       "      <td>397</td>\n",
       "      <td>461</td>\n",
       "      <td>430</td>\n",
       "      <td>407</td>\n",
       "      <td>453</td>\n",
       "      <td>485</td>\n",
       "      <td>454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_f1</th>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.378378</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.390244</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.358974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_f1</th>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.415094</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.382979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_f1</th>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.40678</td>\n",
       "      <td>0.44898</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.44898</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.465116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_f1</th>\n",
       "      <td>0.541893</td>\n",
       "      <td>0.535287</td>\n",
       "      <td>0.534901</td>\n",
       "      <td>0.534139</td>\n",
       "      <td>0.532388</td>\n",
       "      <td>0.531064</td>\n",
       "      <td>0.530025</td>\n",
       "      <td>0.529166</td>\n",
       "      <td>0.525241</td>\n",
       "      <td>0.522678</td>\n",
       "      <td>0.521999</td>\n",
       "      <td>0.521371</td>\n",
       "      <td>0.520634</td>\n",
       "      <td>0.520099</td>\n",
       "      <td>0.519425</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.51789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426226</td>\n",
       "      <td>0.426116</td>\n",
       "      <td>0.425473</td>\n",
       "      <td>0.425468</td>\n",
       "      <td>0.425309</td>\n",
       "      <td>0.425249</td>\n",
       "      <td>0.420416</td>\n",
       "      <td>0.420119</td>\n",
       "      <td>0.418776</td>\n",
       "      <td>0.418197</td>\n",
       "      <td>0.415543</td>\n",
       "      <td>0.415538</td>\n",
       "      <td>0.414982</td>\n",
       "      <td>0.412765</td>\n",
       "      <td>0.41023</td>\n",
       "      <td>0.408997</td>\n",
       "      <td>0.402356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_f1</th>\n",
       "      <td>0.072766</td>\n",
       "      <td>0.058734</td>\n",
       "      <td>0.021224</td>\n",
       "      <td>0.036499</td>\n",
       "      <td>0.067446</td>\n",
       "      <td>0.067168</td>\n",
       "      <td>0.044858</td>\n",
       "      <td>0.030459</td>\n",
       "      <td>0.009732</td>\n",
       "      <td>0.070871</td>\n",
       "      <td>0.015829</td>\n",
       "      <td>0.04959</td>\n",
       "      <td>0.060354</td>\n",
       "      <td>0.026062</td>\n",
       "      <td>0.019135</td>\n",
       "      <td>0.083995</td>\n",
       "      <td>0.028543</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054891</td>\n",
       "      <td>0.072115</td>\n",
       "      <td>0.020836</td>\n",
       "      <td>0.030101</td>\n",
       "      <td>0.035767</td>\n",
       "      <td>0.004698</td>\n",
       "      <td>0.064854</td>\n",
       "      <td>0.052022</td>\n",
       "      <td>0.03655</td>\n",
       "      <td>0.039527</td>\n",
       "      <td>0.046713</td>\n",
       "      <td>0.029571</td>\n",
       "      <td>0.022378</td>\n",
       "      <td>0.070822</td>\n",
       "      <td>0.054943</td>\n",
       "      <td>0.032175</td>\n",
       "      <td>0.045447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_f1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>470</td>\n",
       "      <td>471</td>\n",
       "      <td>472</td>\n",
       "      <td>473</td>\n",
       "      <td>474</td>\n",
       "      <td>475</td>\n",
       "      <td>476</td>\n",
       "      <td>477</td>\n",
       "      <td>478</td>\n",
       "      <td>479</td>\n",
       "      <td>480</td>\n",
       "      <td>481</td>\n",
       "      <td>482</td>\n",
       "      <td>483</td>\n",
       "      <td>484</td>\n",
       "      <td>485</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_roc_auc</th>\n",
       "      <td>0.761842</td>\n",
       "      <td>0.790132</td>\n",
       "      <td>0.82807</td>\n",
       "      <td>0.809649</td>\n",
       "      <td>0.844518</td>\n",
       "      <td>0.807456</td>\n",
       "      <td>0.804605</td>\n",
       "      <td>0.802632</td>\n",
       "      <td>0.82807</td>\n",
       "      <td>0.828289</td>\n",
       "      <td>0.827193</td>\n",
       "      <td>0.805702</td>\n",
       "      <td>0.825658</td>\n",
       "      <td>0.804386</td>\n",
       "      <td>0.811623</td>\n",
       "      <td>0.816886</td>\n",
       "      <td>0.828947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.796272</td>\n",
       "      <td>0.742105</td>\n",
       "      <td>0.802412</td>\n",
       "      <td>0.767105</td>\n",
       "      <td>0.782675</td>\n",
       "      <td>0.794518</td>\n",
       "      <td>0.755921</td>\n",
       "      <td>0.774342</td>\n",
       "      <td>0.749342</td>\n",
       "      <td>0.80307</td>\n",
       "      <td>0.764693</td>\n",
       "      <td>0.798904</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.773026</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.798026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_roc_auc</th>\n",
       "      <td>0.851875</td>\n",
       "      <td>0.829962</td>\n",
       "      <td>0.863675</td>\n",
       "      <td>0.829962</td>\n",
       "      <td>0.844501</td>\n",
       "      <td>0.835651</td>\n",
       "      <td>0.838601</td>\n",
       "      <td>0.798357</td>\n",
       "      <td>0.821955</td>\n",
       "      <td>0.827223</td>\n",
       "      <td>0.822587</td>\n",
       "      <td>0.825327</td>\n",
       "      <td>0.80552</td>\n",
       "      <td>0.837337</td>\n",
       "      <td>0.82638</td>\n",
       "      <td>0.822587</td>\n",
       "      <td>0.791614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.82343</td>\n",
       "      <td>0.818163</td>\n",
       "      <td>0.772861</td>\n",
       "      <td>0.765697</td>\n",
       "      <td>0.811209</td>\n",
       "      <td>0.775811</td>\n",
       "      <td>0.807627</td>\n",
       "      <td>0.827012</td>\n",
       "      <td>0.738727</td>\n",
       "      <td>0.816477</td>\n",
       "      <td>0.785925</td>\n",
       "      <td>0.807838</td>\n",
       "      <td>0.801517</td>\n",
       "      <td>0.796249</td>\n",
       "      <td>0.823852</td>\n",
       "      <td>0.769912</td>\n",
       "      <td>0.797514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_roc_auc</th>\n",
       "      <td>0.864517</td>\n",
       "      <td>0.871892</td>\n",
       "      <td>0.864517</td>\n",
       "      <td>0.890434</td>\n",
       "      <td>0.856932</td>\n",
       "      <td>0.837547</td>\n",
       "      <td>0.829962</td>\n",
       "      <td>0.855247</td>\n",
       "      <td>0.827223</td>\n",
       "      <td>0.84724</td>\n",
       "      <td>0.8504</td>\n",
       "      <td>0.853982</td>\n",
       "      <td>0.847872</td>\n",
       "      <td>0.843026</td>\n",
       "      <td>0.824694</td>\n",
       "      <td>0.858618</td>\n",
       "      <td>0.851875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.82638</td>\n",
       "      <td>0.82048</td>\n",
       "      <td>0.813738</td>\n",
       "      <td>0.776022</td>\n",
       "      <td>0.809102</td>\n",
       "      <td>0.812052</td>\n",
       "      <td>0.827012</td>\n",
       "      <td>0.850822</td>\n",
       "      <td>0.795828</td>\n",
       "      <td>0.825116</td>\n",
       "      <td>0.814791</td>\n",
       "      <td>0.773704</td>\n",
       "      <td>0.806574</td>\n",
       "      <td>0.800885</td>\n",
       "      <td>0.838601</td>\n",
       "      <td>0.811631</td>\n",
       "      <td>0.812052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_roc_auc</th>\n",
       "      <td>0.826078</td>\n",
       "      <td>0.830662</td>\n",
       "      <td>0.852087</td>\n",
       "      <td>0.843348</td>\n",
       "      <td>0.84865</td>\n",
       "      <td>0.826885</td>\n",
       "      <td>0.824389</td>\n",
       "      <td>0.818745</td>\n",
       "      <td>0.825749</td>\n",
       "      <td>0.834251</td>\n",
       "      <td>0.833394</td>\n",
       "      <td>0.828337</td>\n",
       "      <td>0.82635</td>\n",
       "      <td>0.828249</td>\n",
       "      <td>0.820899</td>\n",
       "      <td>0.832697</td>\n",
       "      <td>0.824146</td>\n",
       "      <td>...</td>\n",
       "      <td>0.815361</td>\n",
       "      <td>0.793583</td>\n",
       "      <td>0.796337</td>\n",
       "      <td>0.769608</td>\n",
       "      <td>0.800996</td>\n",
       "      <td>0.794127</td>\n",
       "      <td>0.796854</td>\n",
       "      <td>0.817392</td>\n",
       "      <td>0.761299</td>\n",
       "      <td>0.814888</td>\n",
       "      <td>0.78847</td>\n",
       "      <td>0.793482</td>\n",
       "      <td>0.799188</td>\n",
       "      <td>0.776823</td>\n",
       "      <td>0.811826</td>\n",
       "      <td>0.790339</td>\n",
       "      <td>0.802531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_roc_auc</th>\n",
       "      <td>0.045714</td>\n",
       "      <td>0.033382</td>\n",
       "      <td>0.016986</td>\n",
       "      <td>0.034312</td>\n",
       "      <td>0.005856</td>\n",
       "      <td>0.01376</td>\n",
       "      <td>0.014427</td>\n",
       "      <td>0.02587</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>0.009195</td>\n",
       "      <td>0.012172</td>\n",
       "      <td>0.019825</td>\n",
       "      <td>0.017297</td>\n",
       "      <td>0.017033</td>\n",
       "      <td>0.006595</td>\n",
       "      <td>0.018476</td>\n",
       "      <td>0.024835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013551</td>\n",
       "      <td>0.036412</td>\n",
       "      <td>0.017232</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>0.012983</td>\n",
       "      <td>0.014798</td>\n",
       "      <td>0.030006</td>\n",
       "      <td>0.031955</td>\n",
       "      <td>0.024797</td>\n",
       "      <td>0.00907</td>\n",
       "      <td>0.020532</td>\n",
       "      <td>0.014453</td>\n",
       "      <td>0.007173</td>\n",
       "      <td>0.03081</td>\n",
       "      <td>0.028089</td>\n",
       "      <td>0.017043</td>\n",
       "      <td>0.006736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_roc_auc</th>\n",
       "      <td>133</td>\n",
       "      <td>86</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>127</td>\n",
       "      <td>148</td>\n",
       "      <td>224</td>\n",
       "      <td>135</td>\n",
       "      <td>57</td>\n",
       "      <td>66</td>\n",
       "      <td>109</td>\n",
       "      <td>131</td>\n",
       "      <td>111</td>\n",
       "      <td>203</td>\n",
       "      <td>73</td>\n",
       "      <td>153</td>\n",
       "      <td>...</td>\n",
       "      <td>269</td>\n",
       "      <td>452</td>\n",
       "      <td>431</td>\n",
       "      <td>483</td>\n",
       "      <td>404</td>\n",
       "      <td>449</td>\n",
       "      <td>429</td>\n",
       "      <td>241</td>\n",
       "      <td>486</td>\n",
       "      <td>275</td>\n",
       "      <td>466</td>\n",
       "      <td>453</td>\n",
       "      <td>414</td>\n",
       "      <td>481</td>\n",
       "      <td>311</td>\n",
       "      <td>463</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35 rows × 486 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              185  \\\n",
       "mean_fit_time                                                            0.939862   \n",
       "std_fit_time                                                             0.011765   \n",
       "mean_score_time                                                          0.145191   \n",
       "std_score_time                                                           0.015108   \n",
       "param_clf__class_weight                                        balanced_subsample   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__n_estimators                                                       300   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': 'balanced_subsample', 'c...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                       0.619048   \n",
       "split2_test_recall                                                       0.666667   \n",
       "mean_test_recall                                                         0.578571   \n",
       "std_test_recall                                                          0.092969   \n",
       "rank_test_recall                                                               11   \n",
       "split0_test_precision                                                    0.428571   \n",
       "split1_test_precision                                                    0.565217   \n",
       "split2_test_precision                                                    0.538462   \n",
       "mean_test_precision                                                       0.51075   \n",
       "std_test_precision                                                       0.059127   \n",
       "rank_test_precision                                                           228   \n",
       "split0_test_f1                                                           0.439024   \n",
       "split1_test_f1                                                           0.590909   \n",
       "split2_test_f1                                                           0.595745   \n",
       "mean_test_f1                                                             0.541893   \n",
       "std_test_f1                                                              0.072766   \n",
       "rank_test_f1                                                                    1   \n",
       "split0_test_roc_auc                                                      0.761842   \n",
       "split1_test_roc_auc                                                      0.851875   \n",
       "split2_test_roc_auc                                                      0.864517   \n",
       "mean_test_roc_auc                                                        0.826078   \n",
       "std_test_roc_auc                                                         0.045714   \n",
       "rank_test_roc_auc                                                             133   \n",
       "\n",
       "                                                                              386  \\\n",
       "mean_fit_time                                                            0.295661   \n",
       "std_fit_time                                                             0.001481   \n",
       "mean_score_time                                                          0.052945   \n",
       "std_score_time                                                           0.000501   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__n_estimators                                                       100   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                       0.571429   \n",
       "mean_test_recall                                                         0.499206   \n",
       "std_test_recall                                                          0.052176   \n",
       "rank_test_recall                                                               98   \n",
       "split0_test_precision                                                         0.6   \n",
       "split1_test_precision                                                     0.47619   \n",
       "split2_test_precision                                                    0.666667   \n",
       "mean_test_precision                                                      0.580952   \n",
       "std_test_precision                                                       0.078919   \n",
       "rank_test_precision                                                            73   \n",
       "split0_test_f1                                                           0.514286   \n",
       "split1_test_f1                                                            0.47619   \n",
       "split2_test_f1                                                           0.615385   \n",
       "mean_test_f1                                                             0.535287   \n",
       "std_test_f1                                                              0.058734   \n",
       "rank_test_f1                                                                    2   \n",
       "split0_test_roc_auc                                                      0.790132   \n",
       "split1_test_roc_auc                                                      0.829962   \n",
       "split2_test_roc_auc                                                      0.871892   \n",
       "mean_test_roc_auc                                                        0.830662   \n",
       "std_test_roc_auc                                                         0.033382   \n",
       "rank_test_roc_auc                                                              86   \n",
       "\n",
       "                                                                              447  \\\n",
       "mean_fit_time                                                            0.313121   \n",
       "std_fit_time                                                             0.002491   \n",
       "mean_score_time                                                           0.05508   \n",
       "std_score_time                                                           0.000496   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__n_estimators                                                       100   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.483333   \n",
       "std_test_recall                                                          0.030553   \n",
       "rank_test_recall                                                              127   \n",
       "split0_test_precision                                                         0.6   \n",
       "split1_test_precision                                                    0.588235   \n",
       "split2_test_precision                                                    0.611111   \n",
       "mean_test_precision                                                      0.599782   \n",
       "std_test_precision                                                        0.00934   \n",
       "rank_test_precision                                                            45   \n",
       "split0_test_f1                                                           0.514286   \n",
       "split1_test_f1                                                           0.526316   \n",
       "split2_test_f1                                                           0.564103   \n",
       "mean_test_f1                                                             0.534901   \n",
       "std_test_f1                                                              0.021224   \n",
       "rank_test_f1                                                                    3   \n",
       "split0_test_roc_auc                                                       0.82807   \n",
       "split1_test_roc_auc                                                      0.863675   \n",
       "split2_test_roc_auc                                                      0.864517   \n",
       "mean_test_roc_auc                                                        0.852087   \n",
       "std_test_roc_auc                                                         0.016986   \n",
       "rank_test_roc_auc                                                               4   \n",
       "\n",
       "                                                                              285  \\\n",
       "mean_fit_time                                                            0.373399   \n",
       "std_fit_time                                                              0.01626   \n",
       "mean_score_time                                                          0.053324   \n",
       "std_score_time                                                           0.000499   \n",
       "param_clf__class_weight                                        balanced_subsample   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__n_estimators                                                       100   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': 'balanced_subsample', 'c...   \n",
       "split0_test_recall                                                           0.35   \n",
       "split1_test_recall                                                        0.52381   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                             0.45   \n",
       "std_test_recall                                                          0.073334   \n",
       "rank_test_recall                                                              277   \n",
       "split0_test_precision                                                    0.777778   \n",
       "split1_test_precision                                                    0.611111   \n",
       "split2_test_precision                                                    0.666667   \n",
       "mean_test_precision                                                      0.685185   \n",
       "std_test_precision                                                        0.06929   \n",
       "rank_test_precision                                                             3   \n",
       "split0_test_f1                                                           0.482759   \n",
       "split1_test_f1                                                           0.564103   \n",
       "split2_test_f1                                                           0.555556   \n",
       "mean_test_f1                                                             0.534139   \n",
       "std_test_f1                                                              0.036499   \n",
       "rank_test_f1                                                                    4   \n",
       "split0_test_roc_auc                                                      0.809649   \n",
       "split1_test_roc_auc                                                      0.829962   \n",
       "split2_test_roc_auc                                                      0.890434   \n",
       "mean_test_roc_auc                                                        0.843348   \n",
       "std_test_roc_auc                                                         0.034312   \n",
       "rank_test_roc_auc                                                              19   \n",
       "\n",
       "                                                                              348  \\\n",
       "mean_fit_time                                                            0.856021   \n",
       "std_fit_time                                                              0.00544   \n",
       "mean_score_time                                                          0.142345   \n",
       "std_score_time                                                           0.002263   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__n_estimators                                                       300   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.35   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.434127   \n",
       "std_test_recall                                                          0.059487   \n",
       "rank_test_recall                                                              331   \n",
       "split0_test_precision                                                    0.583333   \n",
       "split1_test_precision                                                    0.714286   \n",
       "split2_test_precision                                                    0.769231   \n",
       "mean_test_precision                                                       0.68895   \n",
       "std_test_precision                                                       0.077978   \n",
       "rank_test_precision                                                             2   \n",
       "split0_test_f1                                                             0.4375   \n",
       "split1_test_f1                                                           0.571429   \n",
       "split2_test_f1                                                           0.588235   \n",
       "mean_test_f1                                                             0.532388   \n",
       "std_test_f1                                                              0.067446   \n",
       "rank_test_f1                                                                    5   \n",
       "split0_test_roc_auc                                                      0.844518   \n",
       "split1_test_roc_auc                                                      0.844501   \n",
       "split2_test_roc_auc                                                      0.856932   \n",
       "mean_test_roc_auc                                                         0.84865   \n",
       "std_test_roc_auc                                                         0.005856   \n",
       "rank_test_roc_auc                                                               8   \n",
       "\n",
       "                                                                              280  \\\n",
       "mean_fit_time                                                            0.318739   \n",
       "std_fit_time                                                             0.001033   \n",
       "mean_score_time                                                           0.05561   \n",
       "std_score_time                                                           0.001092   \n",
       "param_clf__class_weight                                        balanced_subsample   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__n_estimators                                                       100   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': 'balanced_subsample', 'c...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                       0.571429   \n",
       "split2_test_recall                                                       0.619048   \n",
       "mean_test_recall                                                         0.580159   \n",
       "std_test_recall                                                          0.028857   \n",
       "rank_test_recall                                                                5   \n",
       "split0_test_precision                                                    0.366667   \n",
       "split1_test_precision                                                    0.631579   \n",
       "split2_test_precision                                                         0.5   \n",
       "mean_test_precision                                                      0.499415   \n",
       "std_test_precision                                                       0.108151   \n",
       "rank_test_precision                                                           254   \n",
       "split0_test_f1                                                               0.44   \n",
       "split1_test_f1                                                                0.6   \n",
       "split2_test_f1                                                           0.553191   \n",
       "mean_test_f1                                                             0.531064   \n",
       "std_test_f1                                                              0.067168   \n",
       "rank_test_f1                                                                    6   \n",
       "split0_test_roc_auc                                                      0.807456   \n",
       "split1_test_roc_auc                                                      0.835651   \n",
       "split2_test_roc_auc                                                      0.837547   \n",
       "mean_test_roc_auc                                                        0.826885   \n",
       "std_test_roc_auc                                                          0.01376   \n",
       "rank_test_roc_auc                                                             127   \n",
       "\n",
       "                                                                              453  \\\n",
       "mean_fit_time                                                            0.862027   \n",
       "std_fit_time                                                             0.020517   \n",
       "mean_score_time                                                          0.141406   \n",
       "std_score_time                                                           0.002039   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__n_estimators                                                       300   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.4   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.466667   \n",
       "std_test_recall                                                          0.050992   \n",
       "rank_test_recall                                                              199   \n",
       "split0_test_precision                                                    0.571429   \n",
       "split1_test_precision                                                       0.625   \n",
       "split2_test_precision                                                    0.647059   \n",
       "mean_test_precision                                                      0.614496   \n",
       "std_test_precision                                                       0.031757   \n",
       "rank_test_precision                                                            30   \n",
       "split0_test_f1                                                           0.470588   \n",
       "split1_test_f1                                                           0.540541   \n",
       "split2_test_f1                                                           0.578947   \n",
       "mean_test_f1                                                             0.530025   \n",
       "std_test_f1                                                              0.044858   \n",
       "rank_test_f1                                                                    7   \n",
       "split0_test_roc_auc                                                      0.804605   \n",
       "split1_test_roc_auc                                                      0.838601   \n",
       "split2_test_roc_auc                                                      0.829962   \n",
       "mean_test_roc_auc                                                        0.824389   \n",
       "std_test_roc_auc                                                         0.014427   \n",
       "rank_test_roc_auc                                                             148   \n",
       "\n",
       "                                                                              12   \\\n",
       "mean_fit_time                                                            0.275805   \n",
       "std_fit_time                                                             0.014038   \n",
       "mean_score_time                                                          0.053363   \n",
       "std_score_time                                                           0.004946   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__n_estimators                                                       100   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                       0.571429   \n",
       "mean_test_recall                                                         0.499206   \n",
       "std_test_recall                                                          0.052176   \n",
       "rank_test_recall                                                               98   \n",
       "split0_test_precision                                                    0.529412   \n",
       "split1_test_precision                                                    0.666667   \n",
       "split2_test_precision                                                    0.521739   \n",
       "mean_test_precision                                                      0.572606   \n",
       "std_test_precision                                                       0.066585   \n",
       "rank_test_precision                                                            85   \n",
       "split0_test_f1                                                           0.486486   \n",
       "split1_test_f1                                                           0.555556   \n",
       "split2_test_f1                                                           0.545455   \n",
       "mean_test_f1                                                             0.529166   \n",
       "std_test_f1                                                              0.030459   \n",
       "rank_test_f1                                                                    8   \n",
       "split0_test_roc_auc                                                      0.802632   \n",
       "split1_test_roc_auc                                                      0.798357   \n",
       "split2_test_roc_auc                                                      0.855247   \n",
       "mean_test_roc_auc                                                        0.818745   \n",
       "std_test_roc_auc                                                          0.02587   \n",
       "rank_test_roc_auc                                                             224   \n",
       "\n",
       "                                                                              328  \\\n",
       "mean_fit_time                                                            0.329109   \n",
       "std_fit_time                                                             0.002856   \n",
       "mean_score_time                                                          0.053647   \n",
       "std_score_time                                                            0.00063   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__n_estimators                                                       100   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                              0.5   \n",
       "std_test_recall                                                           0.01944   \n",
       "rank_test_recall                                                               87   \n",
       "split0_test_precision                                                    0.526316   \n",
       "split1_test_precision                                                    0.588235   \n",
       "split2_test_precision                                                        0.55   \n",
       "mean_test_precision                                                       0.55485   \n",
       "std_test_precision                                                        0.02551   \n",
       "rank_test_precision                                                           122   \n",
       "split0_test_f1                                                           0.512821   \n",
       "split1_test_f1                                                           0.526316   \n",
       "split2_test_f1                                                           0.536585   \n",
       "mean_test_f1                                                             0.525241   \n",
       "std_test_f1                                                              0.009732   \n",
       "rank_test_f1                                                                    9   \n",
       "split0_test_roc_auc                                                       0.82807   \n",
       "split1_test_roc_auc                                                      0.821955   \n",
       "split2_test_roc_auc                                                      0.827223   \n",
       "mean_test_roc_auc                                                        0.825749   \n",
       "std_test_roc_auc                                                         0.002705   \n",
       "rank_test_roc_auc                                                             135   \n",
       "\n",
       "                                                                              87   \\\n",
       "mean_fit_time                                                             0.87792   \n",
       "std_fit_time                                                             0.008103   \n",
       "mean_score_time                                                           0.13688   \n",
       "std_score_time                                                           0.001714   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__n_estimators                                                       300   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                           0.35   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.434127   \n",
       "std_test_recall                                                          0.059487   \n",
       "rank_test_recall                                                              331   \n",
       "split0_test_precision                                                    0.538462   \n",
       "split1_test_precision                                                    0.769231   \n",
       "split2_test_precision                                                    0.666667   \n",
       "mean_test_precision                                                       0.65812   \n",
       "std_test_precision                                                       0.094405   \n",
       "rank_test_precision                                                             8   \n",
       "split0_test_f1                                                           0.424242   \n",
       "split1_test_f1                                                           0.588235   \n",
       "split2_test_f1                                                           0.555556   \n",
       "mean_test_f1                                                             0.522678   \n",
       "std_test_f1                                                              0.070871   \n",
       "rank_test_f1                                                                   10   \n",
       "split0_test_roc_auc                                                      0.828289   \n",
       "split1_test_roc_auc                                                      0.827223   \n",
       "split2_test_roc_auc                                                       0.84724   \n",
       "mean_test_roc_auc                                                        0.834251   \n",
       "std_test_roc_auc                                                         0.009195   \n",
       "rank_test_roc_auc                                                              57   \n",
       "\n",
       "                                                                              327  \\\n",
       "mean_fit_time                                                            0.323497   \n",
       "std_fit_time                                                              0.01969   \n",
       "mean_score_time                                                          0.062083   \n",
       "std_score_time                                                           0.011912   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__n_estimators                                                       100   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                       0.380952   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.451587   \n",
       "std_test_recall                                                          0.058332   \n",
       "rank_test_recall                                                              230   \n",
       "split0_test_precision                                                    0.642857   \n",
       "split1_test_precision                                                    0.727273   \n",
       "split2_test_precision                                                        0.55   \n",
       "mean_test_precision                                                      0.640043   \n",
       "std_test_precision                                                       0.072399   \n",
       "rank_test_precision                                                            12   \n",
       "split0_test_f1                                                           0.529412   \n",
       "split1_test_f1                                                                0.5   \n",
       "split2_test_f1                                                           0.536585   \n",
       "mean_test_f1                                                             0.521999   \n",
       "std_test_f1                                                              0.015829   \n",
       "rank_test_f1                                                                   11   \n",
       "split0_test_roc_auc                                                      0.827193   \n",
       "split1_test_roc_auc                                                      0.822587   \n",
       "split2_test_roc_auc                                                        0.8504   \n",
       "mean_test_roc_auc                                                        0.833394   \n",
       "std_test_roc_auc                                                         0.012172   \n",
       "rank_test_roc_auc                                                              66   \n",
       "\n",
       "                                                                              6    \\\n",
       "mean_fit_time                                                            0.280151   \n",
       "std_fit_time                                                             0.000164   \n",
       "mean_score_time                                                          0.051576   \n",
       "std_score_time                                                           0.000618   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__n_estimators                                                       100   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                           0.35   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.434127   \n",
       "std_test_recall                                                          0.071066   \n",
       "rank_test_recall                                                              331   \n",
       "split0_test_precision                                                    0.636364   \n",
       "split1_test_precision                                                    0.818182   \n",
       "split2_test_precision                                                    0.578947   \n",
       "mean_test_precision                                                      0.677831   \n",
       "std_test_precision                                                       0.101974   \n",
       "rank_test_precision                                                             4   \n",
       "split0_test_f1                                                           0.451613   \n",
       "split1_test_f1                                                             0.5625   \n",
       "split2_test_f1                                                               0.55   \n",
       "mean_test_f1                                                             0.521371   \n",
       "std_test_f1                                                               0.04959   \n",
       "rank_test_f1                                                                   12   \n",
       "split0_test_roc_auc                                                      0.805702   \n",
       "split1_test_roc_auc                                                      0.825327   \n",
       "split2_test_roc_auc                                                      0.853982   \n",
       "mean_test_roc_auc                                                        0.828337   \n",
       "std_test_roc_auc                                                         0.019825   \n",
       "rank_test_roc_auc                                                             109   \n",
       "\n",
       "                                                                              420  \\\n",
       "mean_fit_time                                                            1.438709   \n",
       "std_fit_time                                                             0.005756   \n",
       "mean_score_time                                                          0.218745   \n",
       "std_score_time                                                           0.002388   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__n_estimators                                                       500   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.35   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.434127   \n",
       "std_test_recall                                                          0.071066   \n",
       "rank_test_recall                                                              331   \n",
       "split0_test_precision                                                    0.583333   \n",
       "split1_test_precision                                                        0.75   \n",
       "split2_test_precision                                                    0.647059   \n",
       "mean_test_precision                                                      0.660131   \n",
       "std_test_precision                                                       0.068666   \n",
       "rank_test_precision                                                             7   \n",
       "split0_test_f1                                                             0.4375   \n",
       "split1_test_f1                                                           0.545455   \n",
       "split2_test_f1                                                           0.578947   \n",
       "mean_test_f1                                                             0.520634   \n",
       "std_test_f1                                                              0.060354   \n",
       "rank_test_f1                                                                   13   \n",
       "split0_test_roc_auc                                                      0.825658   \n",
       "split1_test_roc_auc                                                       0.80552   \n",
       "split2_test_roc_auc                                                      0.847872   \n",
       "mean_test_roc_auc                                                         0.82635   \n",
       "std_test_roc_auc                                                         0.017297   \n",
       "rank_test_roc_auc                                                             131   \n",
       "\n",
       "                                                                              53   \\\n",
       "mean_fit_time                                                            1.321248   \n",
       "std_fit_time                                                             0.005555   \n",
       "mean_score_time                                                          0.225622   \n",
       "std_score_time                                                           0.004804   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__n_estimators                                                       500   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                        0.52381   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.499206   \n",
       "std_test_recall                                                          0.034794   \n",
       "rank_test_recall                                                               98   \n",
       "split0_test_precision                                                    0.529412   \n",
       "split1_test_precision                                                    0.578947   \n",
       "split2_test_precision                                                     0.52381   \n",
       "mean_test_precision                                                      0.544056   \n",
       "std_test_precision                                                       0.024778   \n",
       "rank_test_precision                                                           140   \n",
       "split0_test_f1                                                           0.486486   \n",
       "split1_test_f1                                                               0.55   \n",
       "split2_test_f1                                                            0.52381   \n",
       "mean_test_f1                                                             0.520099   \n",
       "std_test_f1                                                              0.026062   \n",
       "rank_test_f1                                                                   14   \n",
       "split0_test_roc_auc                                                      0.804386   \n",
       "split1_test_roc_auc                                                      0.837337   \n",
       "split2_test_roc_auc                                                      0.843026   \n",
       "mean_test_roc_auc                                                        0.828249   \n",
       "std_test_roc_auc                                                         0.017033   \n",
       "rank_test_roc_auc                                                             111   \n",
       "\n",
       "                                                                              225  \\\n",
       "mean_fit_time                                                            0.314814   \n",
       "std_fit_time                                                             0.002092   \n",
       "mean_score_time                                                          0.051039   \n",
       "std_score_time                                                           0.000404   \n",
       "param_clf__class_weight                                        balanced_subsample   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__n_estimators                                                       100   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': 'balanced_subsample', 'c...   \n",
       "split0_test_recall                                                            0.4   \n",
       "split1_test_recall                                                       0.571429   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                          0.48254   \n",
       "std_test_recall                                                          0.070129   \n",
       "rank_test_recall                                                              167   \n",
       "split0_test_precision                                                    0.666667   \n",
       "split1_test_precision                                                    0.521739   \n",
       "split2_test_precision                                                    0.555556   \n",
       "mean_test_precision                                                       0.58132   \n",
       "std_test_precision                                                       0.061908   \n",
       "rank_test_precision                                                            71   \n",
       "split0_test_f1                                                                0.5   \n",
       "split1_test_f1                                                           0.545455   \n",
       "split2_test_f1                                                           0.512821   \n",
       "mean_test_f1                                                             0.519425   \n",
       "std_test_f1                                                              0.019135   \n",
       "rank_test_f1                                                                   15   \n",
       "split0_test_roc_auc                                                      0.811623   \n",
       "split1_test_roc_auc                                                       0.82638   \n",
       "split2_test_roc_auc                                                      0.824694   \n",
       "mean_test_roc_auc                                                        0.820899   \n",
       "std_test_roc_auc                                                         0.006595   \n",
       "rank_test_roc_auc                                                             203   \n",
       "\n",
       "                                                                              356  \\\n",
       "mean_fit_time                                                            0.805409   \n",
       "std_fit_time                                                             0.006072   \n",
       "mean_score_time                                                          0.145442   \n",
       "std_score_time                                                           0.002257   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__n_estimators                                                       300   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                       0.666667   \n",
       "mean_test_recall                                                         0.547619   \n",
       "std_test_recall                                                          0.084739   \n",
       "rank_test_recall                                                               27   \n",
       "split0_test_precision                                                         0.4   \n",
       "split1_test_precision                                                     0.47619   \n",
       "split2_test_precision                                                    0.608696   \n",
       "mean_test_precision                                                      0.494962   \n",
       "std_test_precision                                                       0.086227   \n",
       "rank_test_precision                                                           268   \n",
       "split0_test_f1                                                           0.444444   \n",
       "split1_test_f1                                                            0.47619   \n",
       "split2_test_f1                                                           0.636364   \n",
       "mean_test_f1                                                                0.519   \n",
       "std_test_f1                                                              0.083995   \n",
       "rank_test_f1                                                                   16   \n",
       "split0_test_roc_auc                                                      0.816886   \n",
       "split1_test_roc_auc                                                      0.822587   \n",
       "split2_test_roc_auc                                                      0.858618   \n",
       "mean_test_roc_auc                                                        0.832697   \n",
       "std_test_roc_auc                                                         0.018476   \n",
       "rank_test_roc_auc                                                              73   \n",
       "\n",
       "                                                                              345  \\\n",
       "mean_fit_time                                                            0.843328   \n",
       "std_fit_time                                                             0.025391   \n",
       "mean_score_time                                                          0.143752   \n",
       "std_score_time                                                           0.002362   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__n_estimators                                                       300   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.483333   \n",
       "std_test_recall                                                          0.030553   \n",
       "rank_test_recall                                                              127   \n",
       "split0_test_precision                                                    0.529412   \n",
       "split1_test_precision                                                    0.666667   \n",
       "split2_test_precision                                                         0.5   \n",
       "mean_test_precision                                                      0.565359   \n",
       "std_test_precision                                                       0.072634   \n",
       "rank_test_precision                                                           100   \n",
       "split0_test_f1                                                           0.486486   \n",
       "split1_test_f1                                                           0.555556   \n",
       "split2_test_f1                                                           0.511628   \n",
       "mean_test_f1                                                              0.51789   \n",
       "std_test_f1                                                              0.028543   \n",
       "rank_test_f1                                                                   17   \n",
       "split0_test_roc_auc                                                      0.828947   \n",
       "split1_test_roc_auc                                                      0.791614   \n",
       "split2_test_roc_auc                                                      0.851875   \n",
       "mean_test_roc_auc                                                        0.824146   \n",
       "std_test_roc_auc                                                         0.024835   \n",
       "rank_test_roc_auc                                                             153   \n",
       "\n",
       "                                ...  \\\n",
       "mean_fit_time                   ...   \n",
       "std_fit_time                    ...   \n",
       "mean_score_time                 ...   \n",
       "std_score_time                  ...   \n",
       "param_clf__class_weight         ...   \n",
       "param_clf__criterion            ...   \n",
       "param_clf__n_estimators         ...   \n",
       "param_smt__k_neighbors          ...   \n",
       "param_smt__sampling_strategy    ...   \n",
       "param_under__sampling_strategy  ...   \n",
       "params                          ...   \n",
       "split0_test_recall              ...   \n",
       "split1_test_recall              ...   \n",
       "split2_test_recall              ...   \n",
       "mean_test_recall                ...   \n",
       "std_test_recall                 ...   \n",
       "rank_test_recall                ...   \n",
       "split0_test_precision           ...   \n",
       "split1_test_precision           ...   \n",
       "split2_test_precision           ...   \n",
       "mean_test_precision             ...   \n",
       "std_test_precision              ...   \n",
       "rank_test_precision             ...   \n",
       "split0_test_f1                  ...   \n",
       "split1_test_f1                  ...   \n",
       "split2_test_f1                  ...   \n",
       "mean_test_f1                    ...   \n",
       "std_test_f1                     ...   \n",
       "rank_test_f1                    ...   \n",
       "split0_test_roc_auc             ...   \n",
       "split1_test_roc_auc             ...   \n",
       "split2_test_roc_auc             ...   \n",
       "mean_test_roc_auc               ...   \n",
       "std_test_roc_auc                ...   \n",
       "rank_test_roc_auc               ...   \n",
       "\n",
       "                                                                              76   \\\n",
       "mean_fit_time                                                            0.839594   \n",
       "std_fit_time                                                             0.008129   \n",
       "mean_score_time                                                           0.13627   \n",
       "std_score_time                                                           0.003121   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__n_estimators                                                       300   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                            0.4   \n",
       "split1_test_recall                                                       0.333333   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.403175   \n",
       "std_test_recall                                                          0.058364   \n",
       "rank_test_recall                                                              429   \n",
       "split0_test_precision                                                    0.421053   \n",
       "split1_test_precision                                                    0.411765   \n",
       "split2_test_precision                                                    0.526316   \n",
       "mean_test_precision                                                      0.453044   \n",
       "std_test_precision                                                       0.051949   \n",
       "rank_test_precision                                                           366   \n",
       "split0_test_f1                                                           0.410256   \n",
       "split1_test_f1                                                           0.368421   \n",
       "split2_test_f1                                                                0.5   \n",
       "mean_test_f1                                                             0.426226   \n",
       "std_test_f1                                                              0.054891   \n",
       "rank_test_f1                                                                  470   \n",
       "split0_test_roc_auc                                                      0.796272   \n",
       "split1_test_roc_auc                                                       0.82343   \n",
       "split2_test_roc_auc                                                       0.82638   \n",
       "mean_test_roc_auc                                                        0.815361   \n",
       "std_test_roc_auc                                                         0.013551   \n",
       "rank_test_roc_auc                                                             269   \n",
       "\n",
       "                                                                              434  \\\n",
       "mean_fit_time                                                            0.267982   \n",
       "std_fit_time                                                             0.001865   \n",
       "mean_score_time                                                          0.053227   \n",
       "std_score_time                                                           0.000622   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__n_estimators                                                       100   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.35   \n",
       "split1_test_recall                                                       0.666667   \n",
       "split2_test_recall                                                       0.571429   \n",
       "mean_test_recall                                                         0.529365   \n",
       "std_test_recall                                                          0.132656   \n",
       "rank_test_recall                                                               57   \n",
       "split0_test_precision                                                    0.304348   \n",
       "split1_test_precision                                                    0.388889   \n",
       "split2_test_precision                                                    0.387097   \n",
       "mean_test_precision                                                      0.360111   \n",
       "std_test_precision                                                       0.039437   \n",
       "rank_test_precision                                                           483   \n",
       "split0_test_f1                                                           0.325581   \n",
       "split1_test_f1                                                           0.491228   \n",
       "split2_test_f1                                                           0.461538   \n",
       "mean_test_f1                                                             0.426116   \n",
       "std_test_f1                                                              0.072115   \n",
       "rank_test_f1                                                                  471   \n",
       "split0_test_roc_auc                                                      0.742105   \n",
       "split1_test_roc_auc                                                      0.818163   \n",
       "split2_test_roc_auc                                                       0.82048   \n",
       "mean_test_roc_auc                                                        0.793583   \n",
       "std_test_roc_auc                                                         0.036412   \n",
       "rank_test_roc_auc                                                             452   \n",
       "\n",
       "                                                                              101  \\\n",
       "mean_fit_time                                                            1.229292   \n",
       "std_fit_time                                                             0.010357   \n",
       "mean_score_time                                                          0.223092   \n",
       "std_score_time                                                           0.003889   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__n_estimators                                                       500   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                        0.52381   \n",
       "split2_test_recall                                                       0.571429   \n",
       "mean_test_recall                                                         0.531746   \n",
       "std_test_recall                                                          0.029696   \n",
       "rank_test_recall                                                               44   \n",
       "split0_test_precision                                                    0.416667   \n",
       "split1_test_precision                                                     0.34375   \n",
       "split2_test_precision                                                    0.315789   \n",
       "mean_test_precision                                                      0.358735   \n",
       "std_test_precision                                                       0.042524   \n",
       "rank_test_precision                                                           484   \n",
       "split0_test_f1                                                           0.454545   \n",
       "split1_test_f1                                                           0.415094   \n",
       "split2_test_f1                                                            0.40678   \n",
       "mean_test_f1                                                             0.425473   \n",
       "std_test_f1                                                              0.020836   \n",
       "rank_test_f1                                                                  472   \n",
       "split0_test_roc_auc                                                      0.802412   \n",
       "split1_test_roc_auc                                                      0.772861   \n",
       "split2_test_roc_auc                                                      0.813738   \n",
       "mean_test_roc_auc                                                        0.796337   \n",
       "std_test_roc_auc                                                         0.017232   \n",
       "rank_test_roc_auc                                                             431   \n",
       "\n",
       "                                                                              145  \\\n",
       "mean_fit_time                                                            1.225518   \n",
       "std_fit_time                                                             0.002242   \n",
       "mean_score_time                                                          0.227322   \n",
       "std_score_time                                                           0.003749   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__n_estimators                                                       500   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.483333   \n",
       "std_test_recall                                                          0.030553   \n",
       "rank_test_recall                                                              127   \n",
       "split0_test_precision                                                    0.333333   \n",
       "split1_test_precision                                                    0.416667   \n",
       "split2_test_precision                                                    0.392857   \n",
       "mean_test_precision                                                      0.380952   \n",
       "std_test_precision                                                       0.035047   \n",
       "rank_test_precision                                                           469   \n",
       "split0_test_f1                                                           0.382979   \n",
       "split1_test_f1                                                           0.444444   \n",
       "split2_test_f1                                                            0.44898   \n",
       "mean_test_f1                                                             0.425468   \n",
       "std_test_f1                                                              0.030101   \n",
       "rank_test_f1                                                                  473   \n",
       "split0_test_roc_auc                                                      0.767105   \n",
       "split1_test_roc_auc                                                      0.765697   \n",
       "split2_test_roc_auc                                                      0.776022   \n",
       "mean_test_roc_auc                                                        0.769608   \n",
       "std_test_roc_auc                                                         0.004571   \n",
       "rank_test_roc_auc                                                             483   \n",
       "\n",
       "                                                                              296  \\\n",
       "mean_fit_time                                                            1.044927   \n",
       "std_fit_time                                                             0.005145   \n",
       "mean_score_time                                                          0.148055   \n",
       "std_score_time                                                           0.002885   \n",
       "param_clf__class_weight                                        balanced_subsample   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__n_estimators                                                       300   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': 'balanced_subsample', 'c...   \n",
       "split0_test_recall                                                           0.35   \n",
       "split1_test_recall                                                       0.380952   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.402381   \n",
       "std_test_recall                                                          0.053699   \n",
       "rank_test_recall                                                              435   \n",
       "split0_test_precision                                                    0.411765   \n",
       "split1_test_precision                                                         0.5   \n",
       "split2_test_precision                                                    0.454545   \n",
       "mean_test_precision                                                      0.455437   \n",
       "std_test_precision                                                       0.036027   \n",
       "rank_test_precision                                                           359   \n",
       "split0_test_f1                                                           0.378378   \n",
       "split1_test_f1                                                           0.432432   \n",
       "split2_test_f1                                                           0.465116   \n",
       "mean_test_f1                                                             0.425309   \n",
       "std_test_f1                                                              0.035767   \n",
       "rank_test_f1                                                                  474   \n",
       "split0_test_roc_auc                                                      0.782675   \n",
       "split1_test_roc_auc                                                      0.811209   \n",
       "split2_test_roc_auc                                                      0.809102   \n",
       "mean_test_roc_auc                                                        0.800996   \n",
       "std_test_roc_auc                                                         0.012983   \n",
       "rank_test_roc_auc                                                             404   \n",
       "\n",
       "                                                                              104  \\\n",
       "mean_fit_time                                                             1.32666   \n",
       "std_fit_time                                                             0.015622   \n",
       "mean_score_time                                                          0.222411   \n",
       "std_score_time                                                           0.004196   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__n_estimators                                                       500   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                       0.428571   \n",
       "mean_test_recall                                                         0.435714   \n",
       "std_test_recall                                                          0.010102   \n",
       "rank_test_recall                                                              298   \n",
       "split0_test_precision                                                    0.409091   \n",
       "split1_test_precision                                                    0.409091   \n",
       "split2_test_precision                                                    0.428571   \n",
       "mean_test_precision                                                      0.415584   \n",
       "std_test_precision                                                       0.009183   \n",
       "rank_test_precision                                                           427   \n",
       "split0_test_f1                                                           0.428571   \n",
       "split1_test_f1                                                           0.418605   \n",
       "split2_test_f1                                                           0.428571   \n",
       "mean_test_f1                                                             0.425249   \n",
       "std_test_f1                                                              0.004698   \n",
       "rank_test_f1                                                                  475   \n",
       "split0_test_roc_auc                                                      0.794518   \n",
       "split1_test_roc_auc                                                      0.775811   \n",
       "split2_test_roc_auc                                                      0.812052   \n",
       "mean_test_roc_auc                                                        0.794127   \n",
       "std_test_roc_auc                                                         0.014798   \n",
       "rank_test_roc_auc                                                             449   \n",
       "\n",
       "                                                                              98   \\\n",
       "mean_fit_time                                                            1.344771   \n",
       "std_fit_time                                                             0.010404   \n",
       "mean_score_time                                                          0.214007   \n",
       "std_score_time                                                           0.001419   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__n_estimators                                                       500   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                           0.35   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.434127   \n",
       "std_test_recall                                                          0.071066   \n",
       "rank_test_recall                                                              331   \n",
       "split0_test_precision                                                    0.318182   \n",
       "split1_test_precision                                                        0.45   \n",
       "split2_test_precision                                                    0.458333   \n",
       "mean_test_precision                                                      0.408838   \n",
       "std_test_precision                                                       0.064194   \n",
       "rank_test_precision                                                           432   \n",
       "split0_test_f1                                                           0.333333   \n",
       "split1_test_f1                                                           0.439024   \n",
       "split2_test_f1                                                           0.488889   \n",
       "mean_test_f1                                                             0.420416   \n",
       "std_test_f1                                                              0.064854   \n",
       "rank_test_f1                                                                  476   \n",
       "split0_test_roc_auc                                                      0.755921   \n",
       "split1_test_roc_auc                                                      0.807627   \n",
       "split2_test_roc_auc                                                      0.827012   \n",
       "mean_test_roc_auc                                                        0.796854   \n",
       "std_test_roc_auc                                                         0.030006   \n",
       "rank_test_roc_auc                                                             429   \n",
       "\n",
       "                                                                              410  \\\n",
       "mean_fit_time                                                            0.807215   \n",
       "std_fit_time                                                             0.007388   \n",
       "mean_score_time                                                          0.142191   \n",
       "std_score_time                                                            0.00362   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__n_estimators                                                       300   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.4   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.466667   \n",
       "std_test_recall                                                          0.050992   \n",
       "rank_test_recall                                                              199   \n",
       "split0_test_precision                                                    0.307692   \n",
       "split1_test_precision                                                    0.416667   \n",
       "split2_test_precision                                                    0.423077   \n",
       "mean_test_precision                                                      0.382479   \n",
       "std_test_precision                                                       0.052947   \n",
       "rank_test_precision                                                           465   \n",
       "split0_test_f1                                                           0.347826   \n",
       "split1_test_f1                                                           0.444444   \n",
       "split2_test_f1                                                           0.468085   \n",
       "mean_test_f1                                                             0.420119   \n",
       "std_test_f1                                                              0.052022   \n",
       "rank_test_f1                                                                  477   \n",
       "split0_test_roc_auc                                                      0.774342   \n",
       "split1_test_roc_auc                                                      0.827012   \n",
       "split2_test_roc_auc                                                      0.850822   \n",
       "mean_test_roc_auc                                                        0.817392   \n",
       "std_test_roc_auc                                                         0.031955   \n",
       "rank_test_roc_auc                                                             241   \n",
       "\n",
       "                                                                              236  \\\n",
       "mean_fit_time                                                            0.894717   \n",
       "std_fit_time                                                             0.003722   \n",
       "mean_score_time                                                          0.133081   \n",
       "std_score_time                                                           0.000577   \n",
       "param_clf__class_weight                                        balanced_subsample   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__n_estimators                                                       300   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': 'balanced_subsample', 'c...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                        0.52381   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.499206   \n",
       "std_test_recall                                                          0.034794   \n",
       "rank_test_recall                                                               98   \n",
       "split0_test_precision                                                    0.310345   \n",
       "split1_test_precision                                                     0.37931   \n",
       "split2_test_precision                                                    0.392857   \n",
       "mean_test_precision                                                      0.360837   \n",
       "std_test_precision                                                       0.036129   \n",
       "rank_test_precision                                                           482   \n",
       "split0_test_f1                                                           0.367347   \n",
       "split1_test_f1                                                               0.44   \n",
       "split2_test_f1                                                            0.44898   \n",
       "mean_test_f1                                                             0.418776   \n",
       "std_test_f1                                                               0.03655   \n",
       "rank_test_f1                                                                  478   \n",
       "split0_test_roc_auc                                                      0.749342   \n",
       "split1_test_roc_auc                                                      0.738727   \n",
       "split2_test_roc_auc                                                      0.795828   \n",
       "mean_test_roc_auc                                                        0.761299   \n",
       "std_test_roc_auc                                                         0.024797   \n",
       "rank_test_roc_auc                                                             486   \n",
       "\n",
       "                                                                              157  \\\n",
       "mean_fit_time                                                             1.89873   \n",
       "std_fit_time                                                             0.074618   \n",
       "mean_score_time                                                          0.622419   \n",
       "std_score_time                                                           0.062038   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__n_estimators                                                       500   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                           0.35   \n",
       "split1_test_recall                                                       0.380952   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.402381   \n",
       "std_test_recall                                                          0.053699   \n",
       "rank_test_recall                                                              435   \n",
       "split0_test_precision                                                    0.388889   \n",
       "split1_test_precision                                                    0.470588   \n",
       "split2_test_precision                                                    0.454545   \n",
       "mean_test_precision                                                      0.438008   \n",
       "std_test_precision                                                       0.035344   \n",
       "rank_test_precision                                                           395   \n",
       "split0_test_f1                                                           0.368421   \n",
       "split1_test_f1                                                           0.421053   \n",
       "split2_test_f1                                                           0.465116   \n",
       "mean_test_f1                                                             0.418197   \n",
       "std_test_f1                                                              0.039527   \n",
       "rank_test_f1                                                                  479   \n",
       "split0_test_roc_auc                                                       0.80307   \n",
       "split1_test_roc_auc                                                      0.816477   \n",
       "split2_test_roc_auc                                                      0.825116   \n",
       "mean_test_roc_auc                                                        0.814888   \n",
       "std_test_roc_auc                                                          0.00907   \n",
       "rank_test_roc_auc                                                             275   \n",
       "\n",
       "                                                                              261  \\\n",
       "mean_fit_time                                                            1.549682   \n",
       "std_fit_time                                                             0.006707   \n",
       "mean_score_time                                                          0.215355   \n",
       "std_score_time                                                           0.002026   \n",
       "param_clf__class_weight                                        balanced_subsample   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__n_estimators                                                       500   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': 'balanced_subsample', 'c...   \n",
       "split0_test_recall                                                            0.3   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.401587   \n",
       "std_test_recall                                                          0.074417   \n",
       "rank_test_recall                                                              454   \n",
       "split0_test_precision                                                    0.428571   \n",
       "split1_test_precision                                                    0.428571   \n",
       "split2_test_precision                                                    0.454545   \n",
       "mean_test_precision                                                      0.437229   \n",
       "std_test_precision                                                       0.012244   \n",
       "rank_test_precision                                                           397   \n",
       "split0_test_f1                                                           0.352941   \n",
       "split1_test_f1                                                           0.428571   \n",
       "split2_test_f1                                                           0.465116   \n",
       "mean_test_f1                                                             0.415543   \n",
       "std_test_f1                                                              0.046713   \n",
       "rank_test_f1                                                                  480   \n",
       "split0_test_roc_auc                                                      0.764693   \n",
       "split1_test_roc_auc                                                      0.785925   \n",
       "split2_test_roc_auc                                                      0.814791   \n",
       "mean_test_roc_auc                                                         0.78847   \n",
       "std_test_roc_auc                                                         0.020532   \n",
       "rank_test_roc_auc                                                             466   \n",
       "\n",
       "                                                                              239  \\\n",
       "mean_fit_time                                                            0.936154   \n",
       "std_fit_time                                                             0.004758   \n",
       "mean_score_time                                                          0.134774   \n",
       "std_score_time                                                           0.001257   \n",
       "param_clf__class_weight                                        balanced_subsample   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__n_estimators                                                       300   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': 'balanced_subsample', 'c...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.451587   \n",
       "std_test_recall                                                          0.019473   \n",
       "rank_test_recall                                                              230   \n",
       "split0_test_precision                                                    0.333333   \n",
       "split1_test_precision                                                    0.391304   \n",
       "split2_test_precision                                                    0.434783   \n",
       "mean_test_precision                                                      0.386473   \n",
       "std_test_precision                                                       0.041557   \n",
       "rank_test_precision                                                           461   \n",
       "split0_test_f1                                                           0.382979   \n",
       "split1_test_f1                                                           0.409091   \n",
       "split2_test_f1                                                           0.454545   \n",
       "mean_test_f1                                                             0.415538   \n",
       "std_test_f1                                                              0.029571   \n",
       "rank_test_f1                                                                  481   \n",
       "split0_test_roc_auc                                                      0.798904   \n",
       "split1_test_roc_auc                                                      0.807838   \n",
       "split2_test_roc_auc                                                      0.773704   \n",
       "mean_test_roc_auc                                                        0.793482   \n",
       "std_test_roc_auc                                                         0.014453   \n",
       "rank_test_roc_auc                                                             453   \n",
       "\n",
       "                                                                              109  \\\n",
       "mean_fit_time                                                            0.286719   \n",
       "std_fit_time                                                             0.002262   \n",
       "mean_score_time                                                          0.056354   \n",
       "std_score_time                                                           0.003436   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__n_estimators                                                       100   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                            0.4   \n",
       "split1_test_recall                                                       0.380952   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.419048   \n",
       "std_test_recall                                                          0.041148   \n",
       "rank_test_recall                                                              373   \n",
       "split0_test_precision                                                    0.380952   \n",
       "split1_test_precision                                                    0.444444   \n",
       "split2_test_precision                                                    0.416667   \n",
       "mean_test_precision                                                      0.414021   \n",
       "std_test_precision                                                       0.025988   \n",
       "rank_test_precision                                                           430   \n",
       "split0_test_f1                                                           0.390244   \n",
       "split1_test_f1                                                           0.410256   \n",
       "split2_test_f1                                                           0.444444   \n",
       "mean_test_f1                                                             0.414982   \n",
       "std_test_f1                                                              0.022378   \n",
       "rank_test_f1                                                                  482   \n",
       "split0_test_roc_auc                                                      0.789474   \n",
       "split1_test_roc_auc                                                      0.801517   \n",
       "split2_test_roc_auc                                                      0.806574   \n",
       "mean_test_roc_auc                                                        0.799188   \n",
       "std_test_roc_auc                                                         0.007173   \n",
       "rank_test_roc_auc                                                             414   \n",
       "\n",
       "                                                                              18   \\\n",
       "mean_fit_time                                                            0.737302   \n",
       "std_fit_time                                                             0.011362   \n",
       "mean_score_time                                                           0.14077   \n",
       "std_score_time                                                           0.007386   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__n_estimators                                                       300   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                            0.4   \n",
       "split1_test_recall                                                       0.333333   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.419048   \n",
       "std_test_recall                                                          0.078919   \n",
       "rank_test_recall                                                              373   \n",
       "split0_test_precision                                                    0.275862   \n",
       "split1_test_precision                                                    0.538462   \n",
       "split2_test_precision                                                    0.478261   \n",
       "mean_test_precision                                                      0.430861   \n",
       "std_test_precision                                                       0.112323   \n",
       "rank_test_precision                                                           407   \n",
       "split0_test_f1                                                           0.326531   \n",
       "split1_test_f1                                                           0.411765   \n",
       "split2_test_f1                                                                0.5   \n",
       "mean_test_f1                                                             0.412765   \n",
       "std_test_f1                                                              0.070822   \n",
       "rank_test_f1                                                                  483   \n",
       "split0_test_roc_auc                                                      0.733333   \n",
       "split1_test_roc_auc                                                      0.796249   \n",
       "split2_test_roc_auc                                                      0.800885   \n",
       "mean_test_roc_auc                                                        0.776823   \n",
       "std_test_roc_auc                                                          0.03081   \n",
       "rank_test_roc_auc                                                             481   \n",
       "\n",
       "                                                                              149  \\\n",
       "mean_fit_time                                                            1.304765   \n",
       "std_fit_time                                                              0.00199   \n",
       "mean_score_time                                                          0.226908   \n",
       "std_score_time                                                           0.003016   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__n_estimators                                                       500   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                           0.35   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.434127   \n",
       "std_test_recall                                                          0.071066   \n",
       "rank_test_recall                                                              331   \n",
       "split0_test_precision                                                    0.318182   \n",
       "split1_test_precision                                                        0.45   \n",
       "split2_test_precision                                                    0.407407   \n",
       "mean_test_precision                                                      0.391863   \n",
       "std_test_precision                                                       0.054926   \n",
       "rank_test_precision                                                           453   \n",
       "split0_test_f1                                                           0.333333   \n",
       "split1_test_f1                                                           0.439024   \n",
       "split2_test_f1                                                           0.458333   \n",
       "mean_test_f1                                                              0.41023   \n",
       "std_test_f1                                                              0.054943   \n",
       "rank_test_f1                                                                  484   \n",
       "split0_test_roc_auc                                                      0.773026   \n",
       "split1_test_roc_auc                                                      0.823852   \n",
       "split2_test_roc_auc                                                      0.838601   \n",
       "mean_test_roc_auc                                                        0.811826   \n",
       "std_test_roc_auc                                                         0.028089   \n",
       "rank_test_roc_auc                                                             311   \n",
       "\n",
       "                                                                              154  \\\n",
       "mean_fit_time                                                            1.278416   \n",
       "std_fit_time                                                             0.006205   \n",
       "mean_score_time                                                          0.263143   \n",
       "std_score_time                                                           0.000155   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__n_estimators                                                       500   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                            0.4   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                       0.571429   \n",
       "mean_test_recall                                                          0.48254   \n",
       "std_test_recall                                                          0.070129   \n",
       "rank_test_recall                                                              167   \n",
       "split0_test_precision                                                    0.333333   \n",
       "split1_test_precision                                                         0.4   \n",
       "split2_test_precision                                                    0.342857   \n",
       "mean_test_precision                                                       0.35873   \n",
       "std_test_precision                                                        0.02944   \n",
       "rank_test_precision                                                           485   \n",
       "split0_test_f1                                                           0.363636   \n",
       "split1_test_f1                                                           0.434783   \n",
       "split2_test_f1                                                           0.428571   \n",
       "mean_test_f1                                                             0.408997   \n",
       "std_test_f1                                                              0.032175   \n",
       "rank_test_f1                                                                  485   \n",
       "split0_test_roc_auc                                                      0.789474   \n",
       "split1_test_roc_auc                                                      0.769912   \n",
       "split2_test_roc_auc                                                      0.811631   \n",
       "mean_test_roc_auc                                                        0.790339   \n",
       "std_test_roc_auc                                                         0.017043   \n",
       "rank_test_roc_auc                                                             463   \n",
       "\n",
       "                                                                              302  \n",
       "mean_fit_time                                                            0.999747  \n",
       "std_fit_time                                                             0.002888  \n",
       "mean_score_time                                                          0.143507  \n",
       "std_score_time                                                            0.00033  \n",
       "param_clf__class_weight                                        balanced_subsample  \n",
       "param_clf__criterion                                                     log_loss  \n",
       "param_clf__n_estimators                                                       300  \n",
       "param_smt__k_neighbors                                                          5  \n",
       "param_smt__sampling_strategy                                                  0.3  \n",
       "param_under__sampling_strategy                                                0.7  \n",
       "params                          {'clf__class_weight': 'balanced_subsample', 'c...  \n",
       "split0_test_recall                                                           0.35  \n",
       "split1_test_recall                                                       0.428571  \n",
       "split2_test_recall                                                        0.47619  \n",
       "mean_test_recall                                                         0.418254  \n",
       "std_test_recall                                                          0.052031  \n",
       "rank_test_recall                                                              387  \n",
       "split0_test_precision                                                    0.368421  \n",
       "split1_test_precision                                                    0.346154  \n",
       "split2_test_precision                                                    0.454545  \n",
       "mean_test_precision                                                      0.389707  \n",
       "std_test_precision                                                        0.04674  \n",
       "rank_test_precision                                                           454  \n",
       "split0_test_f1                                                           0.358974  \n",
       "split1_test_f1                                                           0.382979  \n",
       "split2_test_f1                                                           0.465116  \n",
       "mean_test_f1                                                             0.402356  \n",
       "std_test_f1                                                              0.045447  \n",
       "rank_test_f1                                                                  486  \n",
       "split0_test_roc_auc                                                      0.798026  \n",
       "split1_test_roc_auc                                                      0.797514  \n",
       "split2_test_roc_auc                                                      0.812052  \n",
       "mean_test_roc_auc                                                        0.802531  \n",
       "std_test_roc_auc                                                         0.006736  \n",
       "rank_test_roc_auc                                                             394  \n",
       "\n",
       "[35 rows x 486 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Random forest with grid search for parameters, testing on 5-fold CV with shuffling\n",
    "\n",
    "pipeline = IMBLpipeline([\n",
    "   ('smt', SMOTE()), \n",
    "   ('under', RandomUnderSampler()), \n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "              'clf__n_estimators': [100,300,500],\n",
    "              'clf__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "              'clf__class_weight': ['balanced', 'balanced_subsample', None],\n",
    "              'smt__sampling_strategy': [ 0.2, 0.3, 0.4],\n",
    "              'under__sampling_strategy': [0.5, 0.6, 0.7],\n",
    "              'smt__k_neighbors': [3, 5]\n",
    "             }\n",
    "\n",
    "nfolds = 3\n",
    "scores = ['recall', 'precision', 'f1', 'roc_auc']\n",
    "refit_score = 'f1'\n",
    "gscv_rf = GridSearchCV(pipeline, \n",
    "                    parameters, \n",
    "                    scoring=scores,\n",
    "                    cv=StratifiedKFold(n_splits=nfolds, shuffle=True),\n",
    "                    return_train_score=False, \n",
    "                    verbose=1, \n",
    "                    refit=refit_score,\n",
    "                    n_jobs=-1)\n",
    "resultsGSCV = gscv_rf.fit(X, y)\n",
    "\n",
    "# Get the classifier name from the pipeline\n",
    "classifier_name = resultsGSCV.best_estimator_.named_steps['clf'].__class__.__name__\n",
    "    \n",
    "# save the best estimator\n",
    "save_best_estimator(gscv_rf, classifier_name, notebook_name)\n",
    "\n",
    "results = pd.DataFrame(resultsGSCV.cv_results_)\n",
    "display(results.sort_values(by=[f'rank_test_f1']).transpose())\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b6c5dc6-c5f1-4118-9af1-da0c03851db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.94      0.86      0.90       147\n",
      "         poz       0.47      0.69      0.56        26\n",
      "\n",
      "    accuracy                           0.84       173\n",
      "   macro avg       0.71      0.78      0.73       173\n",
      "weighted avg       0.87      0.84      0.85       173\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on test data\n",
    "rf_clf = gscv_rf.best_estimator_.steps[2][1]\n",
    "\n",
    "# # Load the best estimator from the saved pickle file (replace with acctual file name)\n",
    "# pickle_file_name = \"models/timestamp/classifier_name-notebook_name.pkl\"\n",
    "# rf_clf = load_best_estimator(pickle_file_name).steps[2][1]\n",
    "\n",
    "# Evaluation RF on test set\n",
    "y_pred = rf_clf.predict(X_eval)\n",
    "RF_classification_report = classification_report(y_eval, y_pred)\n",
    "\n",
    "# Create classification report as dictionary\n",
    "RF_report_dict = classification_report(y_eval, y_pred, output_dict=True)\n",
    "\n",
    "print(classification_report(y_eval, y_pred, target_names=[\"neg\", \"poz\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f416e105-b46a-497f-8347-aa1f623f305f",
   "metadata": {},
   "source": [
    "Plot the mean ROC curve of the algorithm with best performing parameter selection. We will perform CV once again and plot the ROC curve for each fold and compute and plot the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d97807f2-1c2c-404d-9ded-0b4e3a782ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Run classifier with cross-validation and plot ROC curves\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "classifier = resultsGSCV.best_estimator_\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "for i, (train, test) in enumerate(cv.split(X_eval, y_eval)):\n",
    "    classifier.fit(X_eval.iloc[train], y_eval[train])\n",
    "    viz = RocCurveDisplay.from_estimator(\n",
    "        classifier,\n",
    "        X_eval.iloc[test],\n",
    "        y_eval[test],\n",
    "        name=\"fold {}\".format(i),\n",
    "        alpha=0.3,\n",
    "        lw=1,\n",
    "        ax=ax,\n",
    "    )\n",
    "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(viz.roc_auc)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Baseline (random prediction)\", alpha=0.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(\n",
    "    mean_fpr,\n",
    "    mean_tpr,\n",
    "    color=\"b\",\n",
    "    label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n",
    "    lw=2,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax.fill_between(\n",
    "    mean_fpr,\n",
    "    tprs_lower,\n",
    "    tprs_upper,\n",
    "    color=\"grey\",\n",
    "    alpha=0.2,\n",
    "    label=r\"$\\pm$ 1 std. dev.\",\n",
    ")\n",
    "\n",
    "clfname = [str(step[1].__class__.__name__) for step in classifier.steps if step[0]=='clf'][0]\n",
    "ax.set(\n",
    "    xlim=[-0.05, 1.05],\n",
    "    ylim=[-0.05, 1.05],\n",
    "    title=f'{clfname} evaluation (ROC-AUC, {nfolds}-fold CV)',\n",
    ")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ab2296-3bf0-49c5-86ae-843d9a0488db",
   "metadata": {},
   "source": [
    "Plot the mean precision-recall curve. The approach is the same as for the mean ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f497691-967e-4591-afcf-71c9c7ce3377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "# cv = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "# classifier = resultsGSCV.best_estimator_\n",
    "\n",
    "# prs = []\n",
    "# aucs = []\n",
    "# mean_r = np.linspace(0, 1, 100)\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(10,8))\n",
    "# for i, (train, test) in enumerate(cv.split(X_eval, y_eval)):\n",
    "#     classifier.fit(X.iloc[train], y[train])\n",
    "#     viz = PrecisionRecallDisplay.from_estimator(\n",
    "#         classifier,\n",
    "#         X_eval.iloc[test],\n",
    "#         y_eval[test],\n",
    "#         name=\"fold {}\".format(i),\n",
    "#         alpha=0.3,\n",
    "#         lw=1,\n",
    "#         ax=ax,\n",
    "#     )\n",
    "#     interp_pr = np.interp(mean_r, viz.recall[::-1], viz.precision[::-1])\n",
    "#     prs.append(interp_pr)\n",
    "\n",
    "# mean_p = np.mean(prs, axis=0)\n",
    "# ax.plot(\n",
    "#     mean_r,\n",
    "#     mean_p,\n",
    "#     color=\"b\",\n",
    "#     label=f\"mean\",\n",
    "#     lw=2,\n",
    "#     alpha=0.8,\n",
    "# )\n",
    "# ax.legend(loc=\"lower left\")\n",
    "# clfname = [str(step[1].__class__.__name__) for step in classifier.steps if step[0]=='clf'][0]\n",
    "# ax.set(\n",
    "#     # xlim=[-0.05, 1.05],\n",
    "#     # ylim=[-0.05, 1.05],\n",
    "#     title=f'{clfname} evaluation (precision-recall, {nfolds}-fold CV)')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e419d1-f53e-4a68-a922-6ae77cac13e9",
   "metadata": {},
   "source": [
    "#### Feature importance (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d2cfb82-7f43-466c-b492-90b58e7c6966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Permutation Importance Random Forest')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArwAAAIYCAYAAACPNz+7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABA/ElEQVR4nO3deZhlVX23/fvLJKOi0BKNYikYEBBaKHGIROTBETWY6EOUR0UjiIpTJElHDbZDkjYk+saRoEFU0BBwCKGdh1ZsFaiGbhpEQLENikrhxCAiNL/3j7NKD2VVV/VQVad235/rOlfvs9faa//22d3w7dVrn0pVIUmSJHXVFnNdgCRJkjSTDLySJEnqNAOvJEmSOs3AK0mSpE4z8EqSJKnTDLySJEnqNAOvJM2yJKck+fu5rkMzK8kxSb4213VIMvBKGlBJ1iS5NcnNSX6S5PQkOw5AXacnect69P+90FNVx1fVm2egtsVJztjU426IQQ17SZYl+XX7fXVDko8nuc9c17UxkgwlqXZNY69Vs1xDJdlzNs8prQ8Dr6RB9rSq2hE4EBgGXr8+B6fH/87NsiRbzXUNUzih/b7aE9gR+Jc5rmdT2bmqdmyvA9b34Hlw36QN5v8IJA28qvoh8GlgP4Akj0zy9SS/SLIqyaFjfdsM3j8kWQ78CnhQm316aZKrk9yU5M1J9mhj3Jjkv5Js047/vZnJsdmrJMcBRwN/02bR/qe1L0ry3Tb2t5I8o+1/CHAK8KjW/xdt/11miZMcm+Q7SX6W5Nwk9x137uNb7b9I8u4kmc7ntp7XfWiSHyR5bZv5XJPk6L6x7pHkQ0lGk3w/yevH/jLRPrPlSd6e5KfAWZNc9xFJLmnnvjbJ4r7xx2Ypn5/kf1sNr+tr37LVNvY5r0hy/9a2d5LPt8/vyiT/dzqfT1X9AvgksLDvPC9IckU7xzVJXtzXNvYZvSbJ9Ul+lOQFfe27tPt3Y5ILgT3G3Y9HJ7koyS/br4/ua1uW5C3t3tyc5H/aeGe28S5KMjSd6xp3zvu2mn7Wfo8d29e2OMk5Sc5IciNwTLvP/9Gu7Yetpi1b/z2TfKXVf0OSs9r+r7YhV7Xaj1rfOqUZV1W+fPnyNXAvYA1weNu+P3A58GbgD4GfAk+h95f2x7f3C1rfZcD/AvsCWwFbAwX8N3D3tv824IvAg4B7AN8Cnt+OPwb42rhaCtizbZ8OvGVc+7OA+7Z6jgJuAe6zjvF+OwZwGHADvVnsuwHvBL467tznATsDuwOjwJMm+cwWA2eMO3a6130ocAfwtlbHY9t17NXaP9TG2gkYAq4C/rLvGu8AXt4+8+0mue5DgYe2z2l/4CfAka1tqNX7vnb8Aa3eh7T2vwZWA3sBae27ADsA1wIvaOd+WPs895nkM1oGvKht7wJ8AfjvvvYj6AXVtM/gV8CB4z6jN9H7ffWU1n7P1v6fwH+1mvYDfjj2GQD3An4OPLfV+ez2fpe+ur7Tzj12b64CDm/9PwR8YJJrGvvstpqg7avAe4Bt6QX7UeCwvt8vtwNHtnuyHfAJ4N/bNdwbuBB4cev/UeB1re+2wGMm+jPiy9cgvpzhlTTIPtlmB78GfAX4R+D/AZ+qqk9V1Z1V9XlghF74GHN6VV1eVXdU1e1t3z9X1Y1VdTlwGfC5qrqmqn5Jb/b4YRtaZFWdXVXXtXrOAq4GDp7m4UcDp1XVxVV1G/B39GZGh/r6LKmqX1TV/wJfpm9GchrW97r/vqpuq6qvAEuB/9tm+P4C+Luquqmq1gD/Si+8jbmuqt7ZPvNbJyqkqpZV1er2OV1KL0A9dly3N1bVrVW1ClhFL9gCvAh4fVVdWT2rquqnwFOBNVX1gXbuS4CP0ftLyGTekeSX9ILxrvSC+liNS6vqu+0cXwE+BxzSd+ztwJuq6vaq+hRwM7BX+4z+HDipqm6pqsuAD/YddwRwdVV9uNX5UeDbwNP6+nygnXvs3ny3qr5QVXcAZzP179Eb2r8C/CLJiW0G/I+Bv62qX1fVSuD9wPP6jvlGVX2yqu6k9xejpwCvatdwPfB2evd+7NofANy3jTdwa7SlyRh4JQ2yI6tq56p6QFW9tAWpBwDP6vsf+y+AxwD9Dx5dO8FYP+nbvnWC9xv8QFyS5yVZ2VfPfvSC1HTcF/j+2JuqupnejPUf9vX5cd/2r9az1vW57p9X1S1977/f6tuV3ozm98e19dc40Wd+F0kekeTLbVnEL4Hj+f3PabJrvT/w3QmGfQDwiHG/H44G/mAdpbyiqu5Bb5b5nsD9+mp8cpJvtiUAv6AXAPtr/GkLoONrXEBvJrb/c+j/vO477v1Ye/9nuLG/R3dtf152rqp/aef8WVXdtI5z9tf7AHr3+Ud9n+W/05vpBfgbejPfFya5PMkLp6hHGhgGXknzzbXAh/v+x75zVe1QVUv6+tRGjH8LsP3YmyTjg9Ndxk7yAHr/DH8CvX+e3pneTGom6j+B6+gFjbHxdqD3T+0/3IDaN9Y92/nH7E6vvhv43exef1t/jeOvc6Lr/ghwLnD/FjhP4Xef01SuZdya2L79Xxn3+2HHqnrJVANW1WrgLcC703M3erPD/wLs1u7lp6ZZ4yi95Q7379u3e9/2Xe5zX/tM3ufrgHsl2Wkd5+y/T9fSW0bSH5zvXlX7AlTVj6vq2Kq6L/Bi4D3xmxk0Txh4Jc03ZwBPS/LE9iDTtu1hovtNeeT0rAL2TbIwybb01jn2+wm9NbBjdqAXGkah99AT7eG6vv73S3s4bAIfBV7Qznc3ess2LmjLBubCG5Nsk+QQessFzq6qtfTWpv5Dkp1ayP8revdiMhNd9070Zhx/neRg4DnrUdf7gTcneXALp/sn2YXe+uY/SvLcJFu318PTe2BwOj4I7AY8HdiG3vrlUeCOJE8GnjCdQdpn9HFgcZLtk+wDPL+vy6danc9JslV7sGufVv+MqKprga8D/9T+nOwP/CWT3Leq+hG9JRz/muTuSbZI7yHHxwIkeVbfn7Of0/t9f2d7P/7PhTRQDLyS5pX2P/E/BV5LL5hcS++Bpk3y37OquoreQ0lfoLcWd/w6xf8A9mn/5PvJqvoWvfWs36D3P/2HAsv7+n+J3gN3P05ywwTn+wLw9/RmFn9EbxbzL8b3myU/phdkrgPOBI6vqm+3tpfTm/2+ht5n8hHgtHWMNdF1vxR4U5KbgJPohejpelvr/zngRnr3Ybv2z/VPoPeZXdeu4a30guuUquo3wL/RW7t8E/CKdp6f0wvk565HjSfQW3bwY3oPJn6g7zxj641fQ2/Jyt8AT62q3/s9sYk9m95DbdfReyDtDe333GSeRy/4f4veZ3AOv1su9HDggiQ30/tcXllV17S2xcAH25+LaX1LhjSbUrUx//InSeqC9L7a7Yyq2lQz5ZI0MJzhlSRJUqcZeCVJktRpLmmQJElSpznDK0mSpE4z8EqSJKnTtprrAjS4dt111xoaGprrMiRJkqa0YsWKG6pqwURtBl5NamhoiJGRkbkuQ5IkaUpJxv/47t9ySYMkSZI6zcArSZKkTjPwSpIkqdMMvJIkSeo0A68kSZI6zcArSZKkTjPwSpIkqdMMvJIkSeo0A68kSZI6zcArSZKkTjPwSpIkqdMMvJIkSeo0A68kSZI6zcArSZKkTjPwSpIkqdMMvJIkSeo0A68kSZI6zcArSZKkTjPwSpIkqdMMvJIkSeq0rea6AEndMrRo6VyXIEkaIGuWHDHXJTjDK0mSpG4z8EqSJKnTDLySJEnqNAOvJEmSOs3AK0mSpE4z8EqSJKnT5nXgTbI2ycoklydZleQ1SbZobcNJ3jED51yTZNeNHGO9akvyiiRXJDlzPY5ZmOQpfe+fnmRR2z4+yfPWr2pJkqT5ab5/D++tVbUQIMm9gY8AdwfeUFUjwMgc1japDajtpcDhVfWD6XROshWwEBgGPtXOeS5wbts+ZX3qlSRJms/m9Qxvv6q6HjgOOCE9hyY5DyDJ4iSnJVmW5Jokrxg7LslfJbmsvV7V9g0l+XaSM9vM6jlJtu873cuTXJxkdZK9k2yR5OokC9rxWyT5TpIFSZ7Vxl6V5Kutvb+2x7ZZ6pVJLkmyU/91JTkFeBDw6SSvTnKvJJ9McmmSbybZv+8aP5xkOfBh4E3AUW3co5Ick+RdfX1PnIHbIEmSNHA6E3gBquoaYEvg3hM07w08ETgYeEOSrZMcBLwAeATwSODYJA9r/fcC3lNVDwFupDfLOuaGqjoQeC9wYlXdCZwBHN3aDwdWVdUocBLwxKo6AHj6BHWdCLyszVQfAtw67pqOB64DHldVbwfeCFxSVfsDrwU+1Nd9H3ozwc9u5z2rqhZW1VmTfmjjJDkuyUiSkdHR0ekeJkmSNLA6FXinsLSqbquqG4Drgd2AxwCfqKpbqupm4OP0QifAtVW1vG2f0fqO+Xj7dQUw1LZPA8bWxb4Q+EDbXg6cnuRYemF8vOXA29qs885VdccU1/EYejO4VNWXgF2S3L21nVtVt0565DRU1alVNVxVwwsWLNiYoSRJkgZCpwJvkgcBa+kF2vFu69tey9Trl2sd78fG+u04VXUt8JMkh9GbRf5023888Hrg/sCKJLvcZdCqJcCLgO2A5Un2nqKudbllI46VJEnqpM4E3rZ+9hTgXVU1PqxO5nzgyCTbJ9kBeEbbB7B7kke17ecAX5vGeO+nNxt8dlWtbXXtUVUXVNVJwCi94Ntf9x5Vtbqq3gpcRG/pxVQ1H92OPZTe8oobJ+h3E7DTBPslSZI2K/M98G439rVkwBeAz9Fb4zotVXUxcDpwIXAB8P6quqQ1Xwm8LMkVwD3prdedyrnAjvxuOQPAye3htsuArwOrxh3zqvZQ26XA7bSZ4XVYDBzU+i8Bnj9Jvy8D+4w9tDaN2iVJkjop058M3XwkGQLOq6r91vO4YeDtVXXIlJ3ngeHh4RoZGchvdtMAG1q0dK5LkCQNkDVLjpiV8yRZUVXDE7XN9+/hHRjthzq8hN99U4MkSZIGwHxf0jAjqmrN+s7uVtWSqnpAVU1nra8kSZJmiYFXkiRJnWbglSRJUqcZeCVJktRpPrQmaZOaradxJUmaLmd4JUmS1GkGXkmSJHWagVeSJEmdZuCVJElSp/nQmqRNyh8trPnKBy6l7nKGV5IkSZ1m4JUkSVKnGXglSZLUaQZeSZIkdZqBV5IkSZ1m4JUkSVKnGXglSZLUaVMG3iRrk6xMclmSs5Nsn2QoyWXrc6Iki5OcuOGlDp4khyZ59CYe77wp+ixM8pRNdU5JkqSum84M761VtbCq9gN+Axw/wzVNKj2zOiudZF0/nONQYJMF3mlaCBh4JUmSpml9w+P5wJ5te8sk70tyeZLPJdkOIMmxSS5KsirJx5JsP36Qyfok2S3JJ9r+VUke3WaTr0zyIeAy4P5JTm4zzquTHNWOPTTJV5L8d5JrkixJcnSSC1u/PVq/pyW5IMklSb6QZLcJ6jsmyblJvgR8Mcm9knwyyaVJvplk/yRD9ML/q9sM+CFJTk/yzL5xbu6rbVmSc5J8O8mZSdLantT2XQz8Wd+xByf5Rqvz60n2SrIN8CbgqHbOo8bPnLfPZai9vt1quqqd8/Aky5NcneTg9bz3kiRJ89K0A2+b6XwysLrtejDw7qraF/gF8Odt/8er6uFVdQBwBfCXEww3WZ93AF9p+w8ELu8713vauYbpzXIeABwOnJzkPq3fAfRC6EOA5wJ/VFUHA+8HXt76fA14ZFU9DPhP4G8mueQDgWdW1WOBNwKXVNX+wGuBD1XVGuAU4O1tBvz8ST+8nocBrwL2AR4E/HGSbYH3AU8DDgL+oK//t4FDWp0nAf9YVb9p22e1c541xTn3BP4V2Lu9ngM8BjixXcfvSXJckpEkI6Ojo1MML0mSNPjW9c/1Y7ZLsrJtnw/8B3Bf4HtVNbZ/BTDUtvdL8hZgZ2BH4LMTjDlZn8OA5wFU1Vrgl0nuCXy/qr7Z+jwG+Ghr/0mSrwAPB24ELqqqHwEk+S7wuXbMauBxbft+wFktJG8DfG+S6/58Vf2s75x/3ur6UpJdktx9kuMmc2FV/aDVtpLe53Uzvc/x6rb/DOC41v8ewAeTPBgoYOv1PB9t7NVt7MuBL1ZVJVnN7+7XXVTVqcCpAMPDw7UB55QkSRoo67OGd2FVvbzNMgLc1tdnLb8Lz6cDJ1TVQ+nNjG47wZjT6dPvlmnUOb6mO/ve39lX3zuBd7Vzv3gd557uOfvdQftM21rjbSaprf/zmsybgS+3tdNPW0edvz1n099vOp+HJElSp83EA2A7AT9KsjVw9Hr2+SLwEoAkWya5xwTHnk9vDeuWSRYAfwJcuB713QP4Ydt+/jSPOX+sziSHAjdU1Y3ATe1axqyhtzQB4OlMPSv7bWBobH0x8OxJ6jymb/9E5zyw1XYg8MApzilJkrRZmYnA+/fABcByeoFuffq8Enhc+yf3FfTWu473CeBSYBXwJeBvqurH61HfYuDsJCuAG9bjmIOSXAos4XdB+X+AZ4w9tEZvPe5jk6wCHsUUs8RV9Wt6SxiWtofWru9r/mfgn5Jcwl1nY78M7DP20BrwMeBebcnCCcBV07wmSZKkzUKqXKapiQ0PD9fIyMhcl6F5ZmjR0rkuQdoga5YcMdclSNoISVZU1fBEbf6kNUmSJHWagVeSJEmdZuCVJElSpxl4JUmS1Gl+F6ukTcoHfyRJg8YZXkmSJHWagVeSJEmdZuCVJElSpxl4JUmS1GkGXkmSJHWa39IgaZOaix8t7DdDSJLWxRleSZIkdZqBV5IkSZ1m4JUkSVKnGXglSZLUaQZeSZIkdZqBV5IkSZ1m4J0nkrwuyeVJLk2yMskj5romSZKk+cDv4Z0HkjwKeCpwYFXdlmRXYJs5LkuSJGlecIZ3frgPcENV3QZQVTdU1XVJ/k+SS5KsTnJakrsBJHl4kq8nWZXkwiQ7JRlKcn6Si9vr0XN6RZIkSbPEwDs/fA64f5KrkrwnyWOTbAucDhxVVQ+lN1v/kiTbAGcBr6yqA4DDgVuB64HHV9WBwFHAOyY6UZLjkowkGRkdHZ35K5MkSZphBt55oKpuBg4CjgNG6QXaFwPfq6qrWrcPAn8C7AX8qKouasfeWFV3AFsD70uyGjgb2GeSc51aVcNVNbxgwYKZvCxJkqRZ4RreeaKq1gLLgGUttL5sPYd4NfAT4AB6f9H59SYtUJIkaUA5wzsPJNkryYP7di0EvgsMJdmz7Xsu8BXgSuA+SR7ejt0pyVbAPejN/N7Z+m45W/VLkiTNJWd454cdgXcm2Rm4A/gOveUNHwXOboH2IuCUqvpNkqNa/+3ord89HHgP8LEkzwM+A9wy+5chSZI0+wy880BVrQAm+laFLwIPm6D/RcAjx+2+Gti/7/3fbrICJUmSBphLGiRJktRpBl5JkiR1moFXkiRJnWbglSRJUqcZeCVJktRpfkuDpE1qzZIj5roESZLuwhleSZIkdZqBV5IkSZ1m4JUkSVKnGXglSZLUaT60JmmTGlq0dNbP6YNykqR1cYZXkiRJnWbglSRJUqcZeCVJktRpBl5JkiR1moFXkiRJnWbglSRJUqcZeOdAkk8l2Xka/V6VZPtZKEmSJKmzDLxzoKqeUlW/6N+XnvH341WAgVeSJGkjGHhnUJJPJlmR5PIkx/XtX5Nk1yRDSa5M8iHgMuD+fX1eAdwX+HKSL7d9T0jyjSQXJzk7yY594/1TkpVJRpIcmOSzSb6b5PjW59AkX02ytJ3zlAkCtiRJUucYeGbWC6vqIGAYeEWSXSbo82DgPVW1b1V9f2xnVb0DuA54XFU9LsmuwOuBw6vqQGAE+Ku+cf63qhYC5wOnA88EHgm8sa/PwcDLgX2APYA/2yRXKUmSNMD80cIz6xVJntG2708v3P50XJ/vV9U3pzHWI+kF1eVJALYBvtHXfm77dTWwY1XdBNyU5La+9cIXVtU1AEk+CjwGOKf/JG0m+jiA3XfffRplSZIkDTYD7wxJcihwOPCoqvpVkmXAthN0vWW6QwKfr6pnT9J+W/v1zr7tsfdj97nGHTP+PVV1KnAqwPDw8O+1S5IkzTcuaZg59wB+3sLu3vRmaNfXTcBObfubwB8n2RMgyQ5J/mg9xzs4yQPb2t2jgK9tQE2SJEnzioF35nwG2CrJFcASeoF1fZ0KfCbJl6tqFDgG+GiSS+ktZ9h7Pce7CHgXcAXwPeATG1CTJEnSvOKShhlSVbcBT56kbaht3gDst44x3gm8s+/9l4CHr2M8qup0eg+t3aWtrfu9saqeOs1LkCRJ6gRneCVJktRpzvBuJqpqGbBsjsuQJEmadc7wSpIkqdMMvJIkSeo0A68kSZI6zTW8kjapNUuOmOsSJEm6C2d4JUmS1GkGXkmSJHWagVeSJEmdZuCVJElSpxl4JUmS1Gl+S4OkTWpo0dIZG9tvgJAkbQhneCVJktRpBl5JkiR1moFXkiRJnWbglSRJUqcZeCVJktRpBl5JkiR1moF3gCRZm2RlksuSnJ1k+7b/fkn+O8nVSb6b5N+SbDPu2N2T3JzkxEnGPibJaBv/W0mOnY1rkiRJmmsG3sFya1UtrKr9gN8AxycJ8HHgk1X1YOCPgB2Bfxh37NuAT08x/llVtRA4FPjHJLttyuIlSZIGkYF3cJ0P7AkcBvy6qj4AUFVrgVcDL+ybAT4S+B5w+XQGrqrrge8CD9j0ZUuSJA0WA+8ASrIV8GRgNbAvsKK/vapuBP4X2DPJjsDfAm9cj/EfBDwI+M4EbcclGUkyMjo6uuEXIUmSNCAMvINluyQrgRF6gfY/pnHMYuDtVXXzNPoe1cb/KPDiqvrZ+A5VdWpVDVfV8IIFC6ZduCRJ0qDaaq4L0F3c2tbY/laSbwHPHLfv7sDu9GZoHwE8M8k/AzsDdyb5NVDA2INpT2m/nlVVJ8xY9ZIkSQPIwDv4vggsSfK8qvpQki2BfwVOr6pfAYeMdUyyGLi5qt7Vdr27r20WS5YkSRocLmkYcFVVwDOAZyW5GrgK+DXw2jktTJIkaZ5whneAVNWOk+y/FnjaNI5fvI6204HTN7A0SZKkecsZXkmSJHWagVeSJEmdZuCVJElSpxl4JUmS1GkGXkmSJHWa39IgaZNas+SIuS5BkqS7cIZXkiRJnWbglSRJUqcZeCVJktRpBl5JkiR1mg+tSdqkhhYtnXZfH3CTJM0GZ3glSZLUaQZeSZIkdZqBV5IkSZ1m4JUkSVKnGXglSZLUaQZeSZIkdZpfS7aZSLIL8MX29g+AtcBoe39wVf1mTgqTJEmaYQbezURV/RRYCJBkMXBzVf3LXNYkSZI0G1zSIEmSpE4z8EqSJKnTDLy6iyTHJRlJMjI6Ojr1AZIkSQPOwKu7qKpTq2q4qoYXLFgw1+VIkiRtNAOvJEmSOs3AK0mSpE7za8k2Q1W1eK5rkCRJmi3O8EqSJKnTDLySJEnqNAOvJEmSOs3AK0mSpE4z8EqSJKnT/JYGSZvUmiVHzHUJkiTdhTO8kiRJ6jQDryRJkjrNwCtJkqROM/BKkiSp0wy8kiRJ6jS/pUHSJjW0aOmE+/32BknSXHGGV5IkSZ1m4JUkSVKnGXglSZLUaQZeSZIkdZqBV5IkSZ1m4JUkSVKnGXhnUJLhJO+Yos99k5zTthcmeco0xr1LvyRPT7Jo4yuWJEnqHgPvekiy5fr0r6qRqnrFFH2uq6pntrcLgSkD7/h+VXVuVS1Zn9okSZI2FwbeJslQkm8nOTPJFUnOSbJ9kjVJ3prkYuBZSZ6Q5BtJLk5ydpId2/EPT/L1JKuSXJhkpySHJjmvtS9O8uF27NVJju0772VJtgHeBByVZGWSo5Ic3Ppf0sbea5J+xyR5V994X0pyaZIvJtm97T89yTvaONckeeYEH4MkSVLnGHjvai/gPVX1EOBG4KVt/0+r6kDgC8DrgcPb+xHgr1oIPQt4ZVUdABwO3DrB+PsDhwGPAk5Kct+xhqr6DXAScFZVLayqs4BvA4dU1cNa2z9O0q/fO4EPVtX+wJlA/5KK+wCPAZ4KTDgjnOS4JCNJRkZHR6f8wCRJkgadP1r4rq6tquVt+wxgbDnCWKh8JLAPsDwJwDbAN+gF5R9V1UUAVXUjQOvT77+r6lbg1iRfBg4GVq6jnnsAH0zyYKCAradxDY8C/qxtfxj45762T1bVncC3kuw20cFVdSpwKsDw8HBN43ySJEkDzcB7V+MD3tj7W9qvAT5fVc/u75TkoRs5/mTeDHy5qp6RZAhYNs3zTOa2vu3fS+OSJEld5JKGu9o9yaPa9nOAr41r/ybwx0n2BEiyQ5I/Aq4E7pPk4W3/Tkkm+svEnybZNskuwKHARePabwJ26nt/D+CHbfuYdfTr93XgL9r20cD5k/STJEnaLBh47+pK4GVJrgDuCby3v7GqRukFz48muZTecoa927rao4B3JlkFfB7YdoLxLwW+TC84v7mqrhvX/mVgn7GH0egtR/inJJdw19n48f36vRx4QavvucAr1+sTkCRJ6phUuUwTet9uAJxXVfvN0PiLgZur6l9mYvyZMDw8XCMjI3NdhuaZoUVLJ9y/ZskRs1yJJGlzkmRFVQ1P1OYMryRJkjrNh9aaqloDzMjsbht/8UyNLUmSpMk5wytJkqROM/BKkiSp0wy8kiRJ6jTX8ErapPw2BknSoHGGV5IkSZ1m4JUkSVKnGXglSZLUaQZeSZIkdZoPrUnapPzRwpKkQeMMryRJkjrNwCtJkqROM/BKkiSp0wy8kiRJ6jQDryRJkjrNwCtJkqROm/XAm2RtkpVJLk+yKslrkmzR2oaTvGMGzrkmya4bOcaM1LapJDk0yXlzXYckSdKgmYvv4b21qhYCJLk38BHg7sAbqmoEGJmDmqY0yLVJkiRpcnO6pKGqrgeOA05Iz29nKZMsTnJakmVJrknyirHjkvxVksva61Vt31CSbyc5M8kVSc5Jsn3f6V6e5OIkq5PsnWSLJFcnWdCO3yLJd5IsSPKsNvaqJF9t7f21PbbNUq9MckmSnfqvK8kOSZa24y9LclTbf1KSi9q+U5Ok7d8zyRda/4uT7NH2/22rd1WSJW3fsiTDbXvXJGvGf67tszux7/1l7fOZsC5JkqQum/M1vFV1DbAlcO8JmvcGnggcDLwhydZJDgJeADwCeCRwbJKHtf57Ae+pqocANwIv7Rvrhqo6EHgvcGJV3QmcARzd2g8HVlXVKHAS8MSqOgB4+gR1nQi8rM1UHwLcOq79ScB1VXVAVe0HfKbtf1dVPbzt2w54att/JvDudr5HAz9K8mTgT4FHtP3/PNHnt54mq0uSJKmz5jzwTmFpVd1WVTcA1wO7AY8BPlFVt1TVzcDH6YVOgGurannbPqP1HfPx9usKYKhtnwY8r22/EPhA214OnJ7kWHphfLzlwNvarPPOVXXHuPbVwOOTvDXJIVX1y7b/cUkuSLIaOAzYt80O/2FVfQKgqn5dVb+iF8A/0Lapqp9N8VlNx2R1/VaS45KMJBkZHR3dBKeUJEmaW3MeeJM8CFhLL9COd1vf9lqmXnNc63g/NtZvx6mqa4GfJDmM3izyp9v+44HXA/cHViTZ5S6DVi0BXkRvlnZ5kr3HtV8FHEgvYL6lLWXYFngP8MyqeijwPmDbKa5nInfwu/s22fH9fX7bb6K6xh9YVadW1XBVDS9YsGADypMkSRoscxp42/rZU+j9U//4sDqZ84Ejk2yfZAfgGW0fwO5JHtW2nwN8bRrjvZ/ebPDZVbW21bVHVV1QVScBo/SCb3/de1TV6qp6K3ARvaUX/e33BX5VVWcAJ9MLmWPh9IYkOwLPBKiqm4AfJDmyHXu3tvb488ALxtYhJ7lXO34NcFDbfuYk17SmnZMkBwIPXEddkiRJnTYX39KwXZKVwNb0ZiI/DLxtugdX1cVJTgcubLveX1WXJBkCrgReluQ04Fv01utO5Vx6Sxk+0Lfv5CQPBgJ8EVgFPLav/VVJHgfcCVxOmxnu89A2xp3A7cBLquoXSd4HXAb8mF5QHvNc4N+TvKn1f1ZVfSbJQmAkyW+ATwGvBf4F+K8kxwFLJ7mmjwHPS3I5cAFw1WR1TfnpSJIkzXOZ/sTqYGuB97z2MNb6HDcMvL2qDpmy82ZmeHi4Rkb8Jjatn6FFE/89bM2SI2a5EknS5iTJiqoanqhtLmZ4B0aSRfRmOY+eqq8kSZLmpzl/aG1Tqao16zu7W1VLquoBVTWdtb6SJEmahzoTeCVJkqSJGHglSZLUaQZeSZIkddpm/dCapE3Pb2OQJA0aZ3glSZLUaQZeSZIkdZqBV5IkSZ1m4JUkSVKnGXglSZLUaQZeSZIkdZqBV5IkSZ1m4JUkSVKnGXglSZLUaQZeSZIkdZqBV5IkSZ1m4JUkSVKnbTXXBWj9JVkLrAa2Bu4APgS8varuTLI98D5gfyDAL4AnVdXNfcdtBVwBPL+qfjUHlyBJkjRrDLzz061VtRAgyb2BjwB3B94AvBL4SVU9tLXvBdw+wXFnAscDb5vVyiVJkmaZSxrmuaq6HjgOOCFJgPsAP+xrv7Kqbpvg0POBPWenSkmSpLlj4O2AqroG2BK4N3Aa8LdJvpHkLUkePL5/kq2AJ9Nb3jC+7bgkI0lGRkdHZ7p0SZKkGWfg7ZiqWgk8CDgZuBdwUZKHtObtkqwERoD/Bf5jguNPrarhqhpesGDB7BQtSZI0g1zD2wFJHgSsBa4HqKqbgY8DH09yJ/AUeg+p/XYNryRJ0ubCGd55LskC4BTgXVVVSf44yT1b2zbAPsD357JGSZKkueQM7/w0tjRh7GvJPszvvm1hD+C97QG2LYClwMfmokhJkqRBYOCdh6pqy3W0fYje9/JO1LbjjBUlSZI0oFzSIEmSpE4z8EqSJKnTDLySJEnqNAOvJEmSOs3AK0mSpE4z8EqSJKnTDLySJEnqNAOvJEmSOs3AK0mSpE4z8EqSJKnT/NHCktZpaNHS9eq/ZskRM1SJJEkbxhleSZIkdZqBV5IkSZ1m4JUkSVKnGXglSZLUaQZeSZIkdZqBV5IkSZ02bwJvkrVJVia5PMmqJK9JskVrG07yjhk455oku27kGNOuLcnOSV46RZ+vb0w9kiRJm5v59D28t1bVQoAk9wY+AtwdeENVjQAjc1jbpNaztp2BlwLvGd+QZKuquqOqHr0Jy5MkSeq8eTPD26+qrgeOA05Iz6FJzgNIsjjJaUmWJbkmySvGjkvyV0kua69XtX1DSb6d5MwkVyQ5J8n2fad7eZKLk6xOsneSLZJcnWRBO36LJN9JsiDJs9rYq5J8tbX31/bYNku9MsklSXYad2lLgD1a+8nt2POTnAt8q41xc9+4X02yNMmVSU7pm/F+dqv3siRvbfu2THJ627c6yas38W2RJEkaSPNphvcuquqaJFsC956geW/gccBOwJVJ3gvsD7wAeAQQ4IIkXwF+DuwF/GVVLU9yGr1Z1n9pY91QVQe2pQYnVtWLkpwBHA38f8DhwKqqGk1yEvDEqvphkp0nqOtE4GXtPDsCvx7XvgjYr28m+1DgwLbvexOMdzCwD/B94DPAn7UlD28FDmrX9rkkRwLXAn9YVfu1sSeqT5IkqXPm5QzvNCytqtuq6gbgemA34DHAJ6rqlqq6Gfg4cEjrf21VLW/bZ7S+Yz7efl0BDLXt04Dnte0XAh9o28uB05McC2w5QV3Lgbe1Weedq+qOaVzLhZOE3bG2a6pqLfDRVvfDgWVVNdrGPxP4E+Aa4EFJ3pnkScCNEw2Y5LgkI0lGRkdHp1GeJEnSYJu3gTfJg4C19ALteLf1ba9l6pnsWsf7sbF+O05VXQv8JMlh9GZZP932Hw+8Hrg/sCLJLncZtGoJ8CJgO2B5kr2nqAvglg2s+64NVT8HDgCWAccD75+k36lVNVxVwwsWLJhGeZIkSYNtXgbetn72FOBdVTVpyBvnfODIJNsn2QF4RtsHsHuSR7Xt5wBfm8Z476c3G3x2m2ElyR5VdUFVnQSM0gu+/XXvUVWrq+qtwEX0ll70u4neMozpOjjJA9va3aNa3RcCj02ya1vy8WzgK+3bJraoqo/RC+UHrsd5JEmS5q35FHi3G/taMuALwOeAN0734Kq6GDidXiC8AHh/VV3Smq8EXpbkCuCewHunMeS5wI78bjkDwMljD4sBXwdWjTvmVe2hsUuB22kzw301/pTezO9lSU6eRg0XAe8CrgC+R2/Jxo/orQX+cjv/iqr6b+APgWVJVtIL6n83jfElSZLmvUx/grSbkgwB5409zLUexw0Db6+qQ6bsPAPaA20nVtVTZ+ocw8PDNTIykN/2plk0tGjpevVfs+SIGapEkqTJJVlRVcMTtc3bb2mYS0kWAS+h900NkiRJGmDzaUnDjKiqNes7u1tVS6rqAVU1nbW+M6Kqls3k7K4kSVJXbPaBV5IkSd1m4JUkSVKnGXglSZLUaT60Jmmd/NYFSdJ85wyvJEmSOs3AK0mSpE4z8EqSJKnTDLySJEnqNAOvJEmSOs1vaZAEwNCipZtkHL/VQZI0aJzhlSRJUqcZeCVJktRpBl5JkiR1moFXkiRJnWbglSRJUqcZeCVJktRpBl5JkiR1moG3I5KsTbIyyX3b+5sn6ffqJP+b5F2zW6EkSdLc8AdPdMetVbVwqk5V9fYkPweGZ74kSZKkuecM7yxLskOSpUlWJbksyVFt/5oku7bt4STL2vbiJKclWZbkmiSvWI9z/UM7zzeT7DbNY45LMpJkZHR0dAOuUJIkabAYeGffk4DrquqAqtoP+Mw0jtkbeCJwMPCGJFtP45gdgG9W1QHAV4Fjp1NcVZ1aVcNVNbxgwYLpHCJJkjTQDLyzbzXw+CRvTXJIVf1yGscsrarbquoG4HpgOrO1vwHOa9srgKENqlaSJGmeM/DOsqq6CjiQXvB9S5KTWtMd/O5+bDvusNv6ttcyvbXXt1dVrecxkiRJnWPgnWXtWxR+VVVnACfTC78Aa4CD2vafz0FpkiRJneSs3+x7KHBykjuB24GXtP1vBP4jyZuBZXNUmyRJUucYeGdZVX0W+OwE+88H/miC/YvHvd9vmufZsW/7HOCc9a1VkiSpC1zS0B039v/gickkeTXwd8CNs1OWJEnS3HKGtyOqap1Bt6/f24G3z3A5kiRJA8MZXkmSJHWagVeSJEmd5pIGSQCsWXLEXJcgSdKMcIZXkiRJnWbglSRJUqcZeCVJktRpBl5JkiR1mg+tSfPI0KKlc13ClHz4TZI0aJzhlSRJUqcZeCVJktRpBl5JkiR1moFXkiRJnWbglSRJUqcZeCVJktRpBt55LMnaJCuTXJ5kVZLXJNmitR2a5Ly2fUySO5Ps33fsZUmG5qh0SZKkWWPgnd9uraqFVbUv8HjgycAbJun7A+B1s1aZJEnSgDDwdkRVXQ8cB5yQJBN0OQ/YN8les1uZJEnS3DLwdkhVXQNsCdx7guY7gX8GXjurRUmSJM0xA+/m5SPAI5M8cLIOSY5LMpJkZHR0dBZLkyRJmhkG3g5J8iBgLXD9RO1VdQfwr8DfTjZGVZ1aVcNVNbxgwYKZKVSSJGkWGXg7IskC4BTgXVVV6+h6OnA4YJqVJEmbha3mugBtlO2SrAS2Bu4APgy8bV0HVNVvkrwD+LeZL0+SJGnuGXjnsarach1ty4Blbft0ejO7Y23vAN4xo8VJkiQNCJc0SJIkqdMMvJIkSeo0A68kSZI6zcArSZKkTjPwSpIkqdP8lgZpHlmz5Ii5LkGSpHnHGV5JkiR1moFXkiRJnWbglSRJUqcZeCVJktRpPrQmDaChRUvnuoQN5oN1kqRB4wyvJEmSOs3AK0mSpE4z8EqSJKnTDLySJEnqNAOvJEmSOs3AK0mSpE4z8EqSJKnTDLwdlGTnJC/te39okvPmsiZJkqS5YuDtpp2Bl07VSZIkaXNg4J1jSYaSfDvJ6UmuSnJmksOTLE9ydZKDk9wrySeTXJrkm0n2b8cuTnJakmVJrknyijbsEmCPJCuTnNz27ZjknHauM5NkTi5YkiRplvmjhQfDnsCzgBcCFwHPAR4DPB14LXAtcElVHZnkMOBDwMJ27N7A44CdgCuTvBdYBOxXVQuht6QBeBiwL3AdsBz4Y+Br4wtJchxwHMDuu+++qa9TkiRp1jnDOxi+V1Wrq+pO4HLgi1VVwGpgiF74/TBAVX0J2CXJ3duxS6vqtqq6Abge2G2Sc1xYVT9o51jZxv09VXVqVQ1X1fCCBQs2zdVJkiTNIQPvYLitb/vOvvd3MvUsfP+xa9fRf7r9JEmSOsXAOz+cDxwNv12ecENV3biO/jfRW+IgSZK02XOWb35YDJyW5FLgV8Dz19W5qn7aHnq7DPg0sHTmS5QkSRpMBt45VlVrgP363h8zSduRExy7eNz7/nGeM677sr62Eza0XkmSpPnGJQ2SJEnqNAOvJEmSOs3AK0mSpE4z8EqSJKnTDLySJEnqNL+lQRpAa5YcMdclSJLUGc7wSpIkqdMMvJIkSeo0A68kSZI6zcArSZKkTvOhNWkODS1aOtclbHI+cCdJGjTO8EqSJKnTDLySJEnqNAOvJEmSOs3AK0mSpE4z8EqSJKnTDLySJEnqtDkLvEnWJlmZ5PIkq5K8JskWrW04yTtm4Jxrkuy6kWPMSG3rWcPiJCdO0efIJPvMVk2SJEmDai6/h/fWqloIkOTewEeAuwNvqKoRYGQOa5vUINc2zpHAecC35rgOSZKkOTUQSxqq6nrgOOCE9Bya5Dz47WzmaUmWJbkmySvGjkvyV0kua69XtX1DSb6d5MwkVyQ5J8n2fad7eZKLk6xOsneSLZJcnWRBO36LJN9JsiDJs9rYq5J8tbX31/bYNku9MsklSXYaf21Jnpfk0jbGh9u+pyW5oB3zhSS79V3riX3HXpZkqG2/LslVSb4G7NXX59gkF7XxP5Zk+ySPBp4OnNxq22Oifht/5yRJkgbfQARegKq6BtgSuPcEzXsDTwQOBt6QZOskBwEvAB4BPBI4NsnDWv+9gPdU1UOAG4GX9o11Q1UdCLwXOLGq7gTOAI5u7YcDq6pqFDgJeGJVHUAvQI53IvCyNlN9CHBrf2OSfYHXA4e1MV7Zmr4GPLKqHgb8J/A36/ps2rX+BbAQeArw8L7mj1fVw9v4VwB/WVVfB84F/rqqFlbVdyfqt65zSpIkdcXABN4pLK2q26rqBuB6YDfgMcAnquqWqroZ+Di90AlwbVUtb9tntL5jPt5+XQEMte3TgOe17RcCH2jby4HTkxxLL4yPtxx4W5t13rmq7hjXfhhwdqubqvpZ238/4LNJVgN/Dew7xfUf0q71V1V1I70wO2a/JOe3sY5ex1jT6pfkuCQjSUZGR0enKEuSJGnwDUzgTfIgYC29QDvebX3ba5l67XGt4/3YWL8dp6quBX6S5DB6s8ifbvuPpzdDe39gRZJd7jJo1RLgRcB2wPIke09R15h3Au+qqocCLwa2bfvv4K73ZNvxB07gdOCENtYb13HMtPpV1alVNVxVwwsWLJjG6SVJkgbbQATetn72FHohcHxYncz5wJFtzeoOwDPaPoDdkzyqbT+H3hKCqbyf3mzw2VW1ttW1R1VdUFUnAaP0gm9/3XtU1eqqeitwEb2lF/2+BDxrLCgnuVfbfw/gh237+X391wAHtr4HAg9s+7/arnW7tk74aX3H7AT8KMnW/G5ZBsBNrW2qfpIkSZ02l4F3u7GvJQO+AHyO3szjtFTVxfRmLS8ELgDeX1WXtOYrgZcluQK4J731ulM5F9iR3y1ngN5DX6uTXAZ8HVg17phXtQfLLgVup80M99V4OfAPwFeSrALe1poWA2cnWQHc0HfIx4B7tc/kBOCqvms9q53/0/TC9Zi/b9e/HPh23/7/BP66PRi3xzr6SZIkdVqmP6E6P7RvNTivqvZbz+OGgbdX1SFTdt5MDA8P18jIfPgGtvlraNHSuS5hk1uz5Ii5LkGStBlKsqKqhidqm8vv4R0YSRYBL8F/6pckSeqcgVjDuylV1Zr1nd2tqiVV9YCqms5aX0mSJM0jnQu8kiRJUj8DryRJkjrNwCtJkqRO86E1aQ75jQaSJM08Z3glSZLUaQZeSZIkdZqBV5IkSZ1m4JUkSVKn+dCaNIf80cKSJM08Z3glSZLUaQZeSZIkdZqBV5IkSZ1m4JUkSVKnGXglSZLUaQZeSZIkdZqBV5IkSZ1m4B0nyaeS7Lwe/YeSXLYe/V+7QYWtpyQLkzyl7/3iJCfOxrklSZIGiYF3nKp6SlX9YgZPMWHgTc+mvB8LgadM1UmSJKnrNqvAm+Svk7yibb89yZfa9mFJzmzba5Ls2mZur0jyviSXJ/lcku1an4OSrEqyCnjZJOe6T5KvJlmZ5LIkhyRZAmzX9p3ZznFlkg8BlwH3bzVelOTSJG9sY62rloe3viuTnNzOtQ3wJuCotv+oVtY+SZYluWbsc5AkSeq6zSrwAucDh7TtYWDHJFu3fV+doP+DgXdX1b7AL4A/b/s/ALy8qg5Yx7meA3y2qhYCBwArq2oRcGtVLayqo/vO8Z52jr3a+4PpzdAelORPplHLi9t51gJU1W+Ak4Cz2rnOan33Bp7Yxn9Du/a7SHJckpEkI6Ojo+u4PEmSpPlhcwu8K+iFyLsDtwHfoBd8D6EXhsf7XlWt7Dt2qK3v3bmqxgLyhyc510XAC5IsBh5aVTdN0u/7VfXNtv2E9roEuJheQH3wFLXsVFXfaPs/Msk5xiytqtuq6gbgemC38R2q6tSqGq6q4QULFkwxnCRJ0uDbrAJvVd0OfA84Bvg6vZD7OGBP4IoJDrmtb3stsNV6nOurwJ8APwROT/K8Sbre0rcd4J/arOzCqtqzqv5jY2vpsynGkCRJmlc2q8DbnA+cSG8Jw/nA8cAlVVXTObg90PaLJI9pu46eqF+SBwA/qar3Ae8HDmxNt0+0lKD5LPDCJDu2Mf4wyb2nqOWmJI9ou/6ir/kmYKepr0iSJKnbNtfAex/gG1X1E+DXTLycYV1eALw7yUp6s7ITORRYleQS4Cjg39r+U4FLxx6S61dVn6O3LOEbSVYD5zB1aP1L4H2tlh2AX7b9X6b3kFr/Q2uSJEmbnUxzYlMDKsmOVXVz214E3KeqXrkpxh4eHq6RkZFNMZQmMbRo6VyXsMmtWXLEXJcgSdoMJVlRVcMTtbmGc/47Isnf0buX36e3PlmSJEmNgXeea185dtaUHSVJkjZTm+MaXkmSJG1GDLySJEnqNAOvJEmSOs01vNIc8hsNJEmaec7wSpIkqdMMvJIkSeo0A68kSZI6zcArSZKkTvOhNWkDdfHHAm8KPognSRo0zvBKkiSp0wy8kiRJ6jQDryRJkjrNwCtJkqROM/BKkiSp0wy8kiRJ6jQD7wBL8gdJ/jPJd5OsSPKpJMclOW+S/suSDM92nZIkSYPMwDugkgT4BLCsqvaoqoOAvwN2m9vKJEmS5hcD7+B6HHB7VZ0ytqOqVgHnAzsmOSfJt5Oc2cLxpJIMJTk/ycXt9egZrl2SJGlg+JPWBtd+wIpJ2h4G7AtcBywH/hj42jrGuh54fFX9OsmDgY8CEy59SHIccBzA7rvvvmGVS5IkDRBneOenC6vqB1V1J7ASGJqi/9bA+5KsBs4G9pmsY1WdWlXDVTW8YMGCTVWvJEnSnDHwDq7LgYMmabutb3st42bqkzwjycr2GgZeDfwEOIDezO42M1CvJEnSQDLwDq4vAXdrSwwASLI/cMhUB1bVJ6pqYXuNAPcAftRmhJ8LbDlTRUuSJA0aA++AqqoCngEc3r6W7HLgn4Afb8Bw7wGen2QVsDdwy6arVJIkabD50NoAq6rrgP87QdP7+vqc0Ld96CTjXA3s37frbzdRiZIkSQPPGV5JkiR1moFXkiRJnWbglSRJUqcZeCVJktRpBl5JkiR1mt/SIG2gNUuOmOsSJEnSNDjDK0mSpE4z8EqSJKnTDLySJEnqNAOvJEmSOs2H1jSnhhYtnesStIn5MJ8kadA4wytJkqROM/BKkiSp0wy8kiRJ6jQDryRJkjrNwCtJkqROM/BKkiSp0wy8kiRJ6jQD7wBLcnqSZ7bt9yfZZ4r+xyd5Xts+Jsl9Z6NOSZKkQeYPnpgnqupF0+hzSt/bY4DLgOtmqiZJkqT5wBneWZZkhyRLk6xKclmSo5KclOSi9v7UJJnguGVJhtv2zUn+oY3xzSS7tf2Lk5zYZoWHgTOTrExyRJJP9o31+CSfmKVLliRJmlMG3tn3JOC6qjqgqvYDPgO8q6oe3t5vBzx1ijF2AL5ZVQcAXwWO7W+sqnOAEeDoqloIfArYO8mC1uUFwGkTDZzkuCQjSUZGR0c37AolSZIGiIF39q0GHp/krUkOqapfAo9LckGS1cBhwL5TjPEb4Ly2vQIYWlfnqirgw8D/S7Iz8Cjg05P0PbWqhqtqeMGCBRN1kSRJmldcwzvLquqqJAcCTwHekuSLwMuA4aq6NsliYNsphrm9hViAtUzvPn4A+B/g18DZVXXHBl2AJEnSPOMM7yxr35zwq6o6AzgZOLA13ZBkR+CZm+hUNwE7jb2pquvoPcD2enrhV5IkabPgDO/seyhwcpI7gduBlwBH0vtGhR8DF22i85wOnJLkVuBRVXUrcCawoKqu2ETnkCRJGngG3llWVZ8FPjtu9wi9mdfxfY/p2z60b3vHvu1zgHPa9uK+/R8DPjZuyMcA79vQ2iVJkuYjA+9mIskK4BbgNXNdiyRJ0mwy8G4mquqgua5BkiRpLvjQmiRJkjrNwCtJkqROM/BKkiSp01zDqzm1ZskRc12CJEnqOGd4JUmS1GkGXkmSJHWagVeSJEmdZuCVJElSpxl4JUmS1GkGXkmSJHWagVeSJEmdZuCVJElSpxl4JUmS1GkGXkmSJHWagVeSJEmdZuCVJElSpxl4JUmS1GkGXkmSJHWagVeSJEmdZuCVJElSpxl4JUmS1GkGXkmSJHWagVeSJEmdZuCVJElSpxl4JUmS1GmpqrmuQQMqySjw/Rk+za7ADTN8Ds0u72n3eE+7x3vaPd5TeEBVLZiowcCrOZVkpKqG57oObTre0+7xnnaP97R7vKfr5pIGSZIkdZqBV5IkSZ1m4NVcO3WuC9Am5z3tHu9p93hPu8d7ug6u4ZUkSVKnOcMrSZKkTjPwakYluVeSzye5uv16z0n6Pb/1uTrJ8/v2/0OSa5PcPHtVayJJnpTkyiTfSbJogva7JTmrtV+QZKiv7e/a/iuTPHFWC9c6beh9TbJLki8nuTnJu2a9cE1qI+7p45OsSLK6/XrYrBevCW3EPT04ycr2WpXkGbNe/IAw8GqmLQK+WFUPBr7Y3t9FknsBbwAeARwMvKEvGP9P26c5lGRL4N3Ak4F9gGcn2Wdct78Efl5VewJvB97ajt0H+AtgX+BJwHvaeJpjG3NfgV8Dfw+cOEvlaho28p7eADytqh4KPB/48OxUrXXZyHt6GTBcVQvp/ff335NsNSuFDxgDr2banwIfbNsfBI6coM8Tgc9X1c+q6ufA5+n9waSqvllVP5qNQrVOBwPfqaprquo3wH/Su7f9+u/1OcD/SZK2/z+r6raq+h7wHfxLzKDY4PtaVbdU1dfoBV8Njo25p5dU1XVt/+XAdknuNitVa1025p7+qqruaPu3BTbbB7cMvJppu/UF1h8Du03Q5w+Ba/ve/6Dt0+CYzj36bZ/2H9hfArtM81jNjY25rxpMm+qe/jlwcVXdNkN1avo26p4meUSSy4HVwPF9AXizsllOa2vTSvIF4A8maHpd/5uqqiSb7d8uJWk+SLIvvX8Sf8Jc16KNV1UXAPsmeQjwwSSfrqrN7l9mDLzaaFV1+GRtSX6S5D5V9aMk9wGun6DbD4FD+97fD1i2SYvUxvohcP++9/dr+ybq84O2RuwewE+neazmxsbcVw2mjbqnSe4HfAJ4XlV9d+bL1TRskj+nVXVFewB8P2Bk5sodTC5p0Ew7l97DD7Rf/3uCPp8FnpDknu1htSe0fRocFwEPTvLAJNvQewjt3HF9+u/1M4EvVe+Lvs8F/qI9RfxA4MHAhbNUt9ZtY+6rBtMG39MkOwNLgUVVtXy2CtaUNuaePnDsIbUkDwD2BtbMTtkDpqp8+ZqxF701RF8Erga+ANyr7R8G3t/X74X0Hmb6DvCCvv3/TG+90p3t18VzfU2b6wt4CnAV8F3gdW3fm4Cnt+1tgbPbPbwQeFDfsa9rx10JPHmur8XXJruva4CfATe3P5/7zPX1+Nrwewq8HrgFWNn3uvdcX4+vjbqnz6X3AOJK4GLgyLm+lrl6+ZPWJEmS1GkuaZAkSVKnGXglSZLUaQZeSZIkdZqBV5IkSZ1m4JUkSVKnGXglSZLUaQZeSZIkdZqBV5IkSZ32/wNBhy84yxfAHAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feature importance of model (best RandomForest from gridsearch) with three methods!\n",
    "\n",
    "fig, (ax2) = plt.subplots(1, 1, figsize=(10,9))\n",
    "plt.subplots_adjust(wspace=1.1)\n",
    "\n",
    "# Get feature importance with Permutation Based Feature Importance (randomly shuffles each feature and compute the \n",
    "# change in the model’s performance. The features which impact the performance the most are the most important one).\n",
    "perm_importance = permutation_importance(rf_clf, X, y)\n",
    "perm_sorted_idx = perm_importance.importances_mean.argsort()\n",
    "x2 = X.columns[perm_sorted_idx]\n",
    "y2 = perm_importance.importances_mean[perm_sorted_idx]\n",
    "ax2.barh(x2, y2)\n",
    "ax2.set_title(\"Permutation Importance Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "917caad5-b4d1-4364-a63e-b0f77f687670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probaj enako z X_eval in primerjaj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0547d681-1201-4853-a89a-006a0efddb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get feature importance with SHAP\n",
    "# explainer = shap.TreeExplainer(rf)\n",
    "# shap_values = explainer.shap_values(X)\n",
    "# RF_shap = shap.summary_plot(shap_values, X, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d081269b-9944-4784-abf0-d36ad02267aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probaj enako z X_eval in primerjaj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88d84a41-047b-4a58-a46e-689374f7d873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x597.6 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SHAP summary plot\n",
    "explainer = shap.TreeExplainer(rf_clf)\n",
    "shap_values = explainer.shap_values(X)\n",
    "classid = 1\n",
    "shap.summary_plot(shap_values[classid], X, max_display=len(X.columns), class_names=le.classes_)\n",
    "\n",
    "# Save the SHAP summary plot\n",
    "plt.savefig(f\"{dir_path}/shap_summary_plot-{classifier_name}-{notebook_name}-{timestamp}.svg\", format=\"svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ca727-c52f-412b-a499-d6e9cdee947f",
   "metadata": {},
   "source": [
    "### Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc6e6b4-1a36-47c3-92c2-51a1340b79c1",
   "metadata": {},
   "source": [
    "#### Model Evaluation (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b70de528-5c6d-428b-a0f4-890707fc436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for NN in scikit_learn\n",
    "\n",
    "# Model evaluation with the pipeline of SMOTE oversampling and undersampling on the training dataset only (within each cross-validation fold)!\n",
    "\n",
    "# one-hot encoding of month feature\n",
    "Xohe = pd.get_dummies(X, columns=[\"month\"])\n",
    "\n",
    "X_display = Xohe.copy()  # *used for SHAP visualization so we can show unscaled values\n",
    "\n",
    "# scalling numeric values for NN\n",
    "scaled_array = StandardScaler().fit_transform(Xohe)\n",
    "Xsc = pd.DataFrame(scaled_array, columns=Xohe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a550fb63-51e5-464b-ae88-7b8579958cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c60b9414-2528-450c-b46e-44967eee00a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 648 candidates, totalling 1944 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "432 fits failed out of a total of 1944.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "432 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mmarzi/.pyenv/versions/3.8.0/envs/hab/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/mmarzi/.pyenv/versions/3.8.0/envs/hab/lib/python3.8/site-packages/imblearn/pipeline.py\", line 268, in fit\n",
      "    Xt, yt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/mmarzi/.pyenv/versions/3.8.0/envs/hab/lib/python3.8/site-packages/imblearn/pipeline.py\", line 226, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "  File \"/Users/mmarzi/.pyenv/versions/3.8.0/envs/hab/lib/python3.8/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/mmarzi/.pyenv/versions/3.8.0/envs/hab/lib/python3.8/site-packages/imblearn/pipeline.py\", line 394, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **fit_params)\n",
      "  File \"/Users/mmarzi/.pyenv/versions/3.8.0/envs/hab/lib/python3.8/site-packages/imblearn/base.py\", line 79, in fit_resample\n",
      "    self.sampling_strategy_ = check_sampling_strategy(\n",
      "  File \"/Users/mmarzi/.pyenv/versions/3.8.0/envs/hab/lib/python3.8/site-packages/imblearn/utils/_validation.py\", line 534, in check_sampling_strategy\n",
      "    _sampling_strategy_float(sampling_strategy, y, sampling_type).items()\n",
      "  File \"/Users/mmarzi/.pyenv/versions/3.8.0/envs/hab/lib/python3.8/site-packages/imblearn/utils/_validation.py\", line 393, in _sampling_strategy_float\n",
      "    raise ValueError(\n",
      "ValueError: The specified ratio required to generate new sample in the majority class while trying to remove samples. Please increase the ratio.\n",
      "\n",
      "One or more of the test scores are non-finite: [0.44761905 0.51507937 0.46746032 0.42142857 0.50079365 0.4531746\n",
      "        nan        nan 0.38333333 0.48253968 0.42063492 0.58015873\n",
      " 0.43492063 0.40238095 0.40396825        nan        nan 0.41825397\n",
      " 0.49920635 0.45238095 0.43492063 0.35396825 0.43571429 0.4984127\n",
      "        nan        nan 0.54920635 0.46746032 0.48492063 0.41825397\n",
      " 0.43412698 0.43650794 0.40238095        nan        nan 0.27380952\n",
      " 0.46904762 0.56428571 0.5        0.43650794 0.42142857 0.3547619\n",
      "        nan        nan 0.5015873  0.43650794 0.53095238 0.51666667\n",
      " 0.4015873  0.45079365 0.51666667        nan        nan 0.45079365\n",
      " 0.33968254 0.51587302 0.41904762 0.03174603 0.48333333 0.40555556\n",
      "        nan        nan 0.53095238 0.23968254 0.27698413 0.5\n",
      " 0.36825397 0.24603175 0.53095238        nan        nan 0.48333333\n",
      " 0.43412698 0.38888889 0.4515873  0.17539683 0.53333333 0.45238095\n",
      "        nan        nan 0.48412698 0.3547619  0.38809524 0.51507937\n",
      " 0.38571429 0.38888889 0.48492063        nan        nan 0.46825397\n",
      " 0.34047619 0.36587302 0.46825397 0.35396825 0.51666667 0.59603175\n",
      "        nan        nan 0.59761905 0.33968254 0.22539683 0.35873016\n",
      " 0.27619048 0.20634921 0.59603175        nan        nan 0.53174603\n",
      " 0.37142857 0.48333333 0.46825397 0.58174603 0.45238095 0.53174603\n",
      "        nan        nan 0.56428571 0.45238095 0.43571429 0.46904762\n",
      " 0.45       0.43333333 0.46746032        nan        nan 0.53174603\n",
      " 0.4031746  0.4984127  0.54603175 0.34047619 0.48333333 0.46666667\n",
      "        nan        nan 0.51666667 0.5484127  0.45       0.61031746\n",
      " 0.41904762 0.41904762 0.45079365        nan        nan 0.46825397\n",
      " 0.43650794 0.46825397 0.52936508 0.51666667 0.45       0.48571429\n",
      "        nan        nan 0.43412698 0.5        0.5        0.61190476\n",
      " 0.38809524 0.38730159 0.57936508        nan        nan 0.40396825\n",
      " 0.35634921 0.64603175 0.32301587 0.38809524 0.62777778 0.53492063\n",
      "        nan        nan 0.53095238 0.51190476 0.48015873 0.57698413\n",
      " 0.46666667 0.62619048 0.48571429        nan        nan 0.56349206\n",
      " 0.53412698 0.35634921 0.56507937 0.48571429 0.33333333 0.48571429\n",
      "        nan        nan 0.49920635 0.41984127 0.54920635 0.57777778\n",
      " 0.53174603 0.56428571 0.51428571        nan        nan 0.50238095\n",
      " 0.53333333 0.5468254  0.41269841 0.51666667 0.41984127 0.59761905\n",
      "        nan        nan 0.53015873 0.45       0.38809524 0.45079365\n",
      " 0.48412698 0.51587302 0.4047619         nan        nan 0.46746032\n",
      " 0.14285714 0.46111111 0.45238095 0.16666667 0.22460317 0.36825397\n",
      "        nan        nan 0.35952381 0.28571429 0.4484127  0.38888889\n",
      " 0.16666667 0.22222222 0.19047619        nan        nan 0.44603175\n",
      " 0.3968254  0.34126984 0.27063492 0.17857143 0.32460317 0.34285714\n",
      "        nan        nan 0.38412698 0.16666667 0.28888889 0.03174603\n",
      " 0.35714286 0.54761905 0.71428571        nan        nan 0.43571429\n",
      " 0.27539683 0.21507937 0.38492063 0.20634921 0.28571429 0.3968254\n",
      "        nan        nan 0.30714286 0.51507937 0.03174603 0.56269841\n",
      " 0.37380952 0.30873016 0.28571429        nan        nan 0.40238095\n",
      " 0.43174603 0.5        0.30714286 0.35396825 0.54603175 0.51587302\n",
      "        nan        nan 0.51507937 0.38650794 0.43492063 0.56190476\n",
      " 0.30952381 0.5015873  0.48333333        nan        nan 0.46428571\n",
      " 0.46904762 0.49920635 0.4531746  0.54920635 0.48412698 0.5452381\n",
      "        nan        nan 0.50079365 0.33888889 0.41904762 0.45\n",
      " 0.3015873  0.42142857 0.4015873         nan        nan 0.43650794\n",
      " 0.38571429 0.38809524 0.42142857 0.4515873  0.50079365 0.48412698\n",
      "        nan        nan 0.4984127  0.41904762 0.48571429 0.43650794\n",
      " 0.49920635 0.43492063 0.41666667        nan        nan 0.51507937\n",
      " 0.49920635 0.43412698 0.53253968 0.43492063 0.42301587 0.43492063\n",
      "        nan        nan 0.38809524 0.44761905 0.52936508 0.43492063\n",
      " 0.45       0.49920635 0.48412698        nan        nan 0.4984127\n",
      " 0.5        0.51587302 0.5468254  0.4031746  0.48333333 0.5\n",
      "        nan        nan 0.45       0.53095238 0.38730159 0.53333333\n",
      " 0.4031746  0.5        0.46587302        nan        nan 0.56428571\n",
      " 0.44920635 0.53174603 0.5        0.53015873 0.38412698 0.58095238\n",
      "        nan        nan 0.41904762 0.56507937 0.41746032 0.4031746\n",
      " 0.5        0.4984127  0.4031746         nan        nan 0.57936508\n",
      " 0.37063492 0.41984127 0.38730159 0.4031746  0.4515873  0.45079365\n",
      "        nan        nan 0.4984127  0.36984127 0.37301587 0.4515873\n",
      " 0.41984127 0.41746032 0.51587302        nan        nan 0.53253968\n",
      " 0.41984127 0.56428571 0.51666667 0.41825397 0.46666667 0.46587302\n",
      "        nan        nan 0.56349206 0.29285714 0.43333333 0.51507937\n",
      " 0.31825397 0.4047619  0.57857143        nan        nan 0.5\n",
      " 0.46825397 0.45396825 0.43333333 0.3547619  0.45079365 0.40396825\n",
      "        nan        nan 0.51587302 0.41825397 0.5        0.5484127\n",
      " 0.30238095 0.43492063 0.56349206        nan        nan 0.49920635\n",
      " 0.41984127 0.51666667 0.56507937 0.48571429 0.41825397 0.46746032\n",
      "        nan        nan 0.53253968 0.43333333 0.46587302 0.44920635\n",
      " 0.50079365 0.45079365 0.62777778        nan        nan 0.58095238\n",
      " 0.48412698 0.43650794 0.51666667 0.51587302 0.49761905 0.45396825\n",
      "        nan        nan 0.51666667 0.5015873  0.56507937 0.35634921\n",
      " 0.43571429 0.4515873  0.5               nan        nan 0.53174603\n",
      " 0.46825397 0.56349206 0.5        0.56507937 0.45079365 0.48492063\n",
      "        nan        nan 0.48412698 0.45       0.43492063 0.56349206\n",
      " 0.53174603 0.51587302 0.51666667        nan        nan 0.48412698\n",
      " 0.43333333 0.56428571 0.4031746  0.39920635 0.52936508 0.49761905\n",
      "        nan        nan 0.55       0.43333333 0.48412698 0.46825397\n",
      " 0.42063492 0.48412698 0.40238095        nan        nan 0.51666667\n",
      " 0.49920635 0.54920635 0.56349206 0.4968254  0.32142857 0.46904762\n",
      "        nan        nan 0.33888889 0.5452381  0.54920635 0.4515873\n",
      " 0.54920635 0.50079365 0.41825397        nan        nan 0.40396825\n",
      " 0.5        0.51666667 0.54761905 0.49920635 0.51666667 0.53333333\n",
      "        nan        nan 0.48492063 0.41984127 0.61031746 0.4515873\n",
      " 0.5        0.46825397 0.45079365        nan        nan 0.48412698\n",
      " 0.45079365 0.15873016 0.36507937 0.12698413 0.43571429 0.32142857\n",
      "        nan        nan 0.56428571 0.46825397 0.4984127  0.48333333\n",
      " 0.35634921 0.37063492 0.53015873        nan        nan 0.48412698\n",
      " 0.41984127 0.45079365 0.17460317 0.35396825 0.27539683 0.34047619\n",
      "        nan        nan 0.36904762 0.28571429 0.30793651 0.28809524\n",
      " 0.24285714 0.5952381  0.43650794        nan        nan 0.45\n",
      " 0.38571429 0.53333333 0.53095238 0.38809524 0.45       0.54920635\n",
      "        nan        nan 0.49920635 0.40555556 0.56428571 0.4952381\n",
      " 0.37380952 0.16031746 0.4531746         nan        nan 0.29365079\n",
      " 0.46825397 0.46746032 0.56269841 0.43253968 0.33968254 0.43650794\n",
      "        nan        nan 0.45238095 0.4047619  0.4984127  0.43412698\n",
      " 0.45079365 0.48333333 0.38571429        nan        nan 0.46666667\n",
      " 0.48571429 0.48492063 0.46587302 0.37142857 0.46666667 0.53333333\n",
      "        nan        nan 0.50079365 0.53253968 0.46825397 0.4984127\n",
      " 0.46746032 0.46825397 0.53174603        nan        nan 0.42063492\n",
      " 0.40396825 0.42142857 0.48333333 0.41984127 0.4015873  0.51507937\n",
      "        nan        nan 0.43492063 0.40238095 0.43412698 0.53253968\n",
      " 0.4515873  0.41984127 0.40396825        nan        nan 0.48492063]\n",
      "One or more of the test scores are non-finite: [0.29816457 0.3301239  0.28970814 0.34816327 0.3859127  0.29163474\n",
      "        nan        nan 0.22569749 0.25392757 0.2521978  0.28649376\n",
      " 0.31836945 0.33650157 0.37614871        nan        nan 0.33481878\n",
      " 0.36018724 0.29404337 0.37218045 0.36797386 0.35991494 0.28780284\n",
      "        nan        nan 0.29494949 0.3458486  0.2670895  0.30155945\n",
      " 0.41923436 0.31686508 0.31656307        nan        nan 0.29164092\n",
      " 0.30119588 0.30012771 0.29368687 0.4541847  0.32768515 0.26940457\n",
      "        nan        nan 0.36996561 0.30773124 0.36775362 0.35045948\n",
      " 0.41088228 0.33648223 0.2661449         nan        nan 0.32148244\n",
      " 0.29468599 0.28627266 0.2645216  0.06666667 0.27142857 0.30317982\n",
      "        nan        nan 0.22931185 0.23504274 0.27556391 0.29144265\n",
      " 0.27356257 0.15925926 0.24137681        nan        nan 0.2454191\n",
      " 0.30940681 0.24418094 0.30596491 0.13804714 0.32364927 0.21682799\n",
      "        nan        nan 0.32892416 0.37487179 0.27449275 0.22717262\n",
      " 0.2750306  0.34542879 0.28719434        nan        nan 0.247003\n",
      " 0.32649573 0.17101164 0.23941094 0.33165205 0.35508901 0.23741756\n",
      "        nan        nan 0.21981162 0.2043774  0.14345377 0.18074255\n",
      " 0.27648579 0.1547619  0.30802206        nan        nan 0.29827586\n",
      " 0.41890959 0.38184524 0.31655543 0.34003623 0.36076193 0.28419913\n",
      "        nan        nan 0.26786787 0.35357369 0.31017905 0.24412393\n",
      " 0.29278069 0.29444444 0.34649601        nan        nan 0.28938462\n",
      " 0.34012728 0.32144756 0.31635721 0.35024155 0.34649573 0.28366659\n",
      "        nan        nan 0.29229285 0.37566755 0.26996997 0.2519647\n",
      " 0.3865973  0.26831502 0.26301359        nan        nan 0.30288462\n",
      " 0.36308244 0.29425579 0.28259338 0.34968081 0.27464584 0.34085213\n",
      "        nan        nan 0.28243243 0.37511871 0.32803922 0.30933642\n",
      " 0.30983103 0.32598312 0.32233318        nan        nan 0.2654321\n",
      " 0.36950147 0.29484127 0.29417588 0.33146168 0.26459627 0.23856421\n",
      "        nan        nan 0.21652047 0.22931905 0.2561769  0.23112837\n",
      " 0.30265711 0.25697699 0.34677707        nan        nan 0.29818345\n",
      " 0.33464052 0.18463843 0.31525573 0.21197017 0.20601238 0.32603301\n",
      "        nan        nan 0.27993174 0.40094851 0.28978012 0.29576243\n",
      " 0.25770944 0.3015873  0.33393142        nan        nan 0.25155531\n",
      " 0.28488975 0.33277639 0.22962963 0.31153846 0.27473868 0.29096063\n",
      "        nan        nan 0.24168721 0.29219903 0.34402631 0.31040564\n",
      " 0.26613825 0.23116496 0.32272727        nan        nan 0.26133832\n",
      " 0.15       0.17049062 0.23154762 0.07936508 0.20648148 0.2336851\n",
      "        nan        nan 0.18520928 0.18369453 0.27643098 0.1257384\n",
      " 0.12820513 0.24827586 0.15987461        nan        nan 0.22268579\n",
      " 0.2762963  0.22569444 0.20786919 0.15931373 0.14637681 0.18579627\n",
      "        nan        nan 0.22751323 0.09803922 0.13984236 0.07407407\n",
      " 0.34076551 0.23128307 0.18547833        nan        nan 0.28959276\n",
      " 0.20454545 0.31165312 0.17964811 0.16466466 0.21414141 0.15972222\n",
      "        nan        nan 0.20443967 0.26221702 0.11111111 0.25874126\n",
      " 0.20277778 0.18686428 0.18609023        nan        nan 0.27605364\n",
      " 0.29545703 0.37211099 0.228844   0.37129537 0.28482628 0.26335523\n",
      "        nan        nan 0.3528348  0.32938289 0.43429487 0.31623932\n",
      " 0.21259259 0.28090118 0.25634119        nan        nan 0.30454545\n",
      " 0.33273641 0.36380471 0.26798853 0.30097093 0.36598017 0.33474545\n",
      "        nan        nan 0.31516618 0.32913279 0.29164141 0.28239065\n",
      " 0.17575758 0.13573798 0.33485958        nan        nan 0.28413196\n",
      " 0.2987013  0.14214047 0.29713262 0.32103263 0.26262315 0.34054362\n",
      "        nan        nan 0.28641457 0.34733894 0.29358917 0.30785599\n",
      " 0.39545792 0.33272283 0.29524763        nan        nan 0.25156423\n",
      " 0.30515873 0.32021974 0.31472144 0.29858586 0.32350819 0.2950753\n",
      "        nan        nan 0.31071429 0.29116059 0.27022547 0.28660085\n",
      " 0.32000789 0.3464591  0.2999957         nan        nan 0.35847983\n",
      " 0.34677419 0.33223864 0.30654088 0.33300551 0.32938835 0.26792418\n",
      "        nan        nan 0.31908832 0.3017683  0.32077295 0.30258467\n",
      " 0.39241623 0.39159892 0.28469541        nan        nan 0.29351767\n",
      " 0.30716846 0.28535354 0.30534448 0.30165692 0.29231445 0.35222315\n",
      "        nan        nan 0.3388676  0.33360467 0.32993636 0.31344086\n",
      " 0.36965812 0.29682218 0.31143026        nan        nan 0.31274264\n",
      " 0.32614645 0.32521118 0.21792837 0.27492877 0.28819444 0.26666667\n",
      "        nan        nan 0.26933073 0.27279968 0.24641577 0.28950558\n",
      " 0.31347319 0.315157   0.25330711        nan        nan 0.30112045\n",
      " 0.30770464 0.27293748 0.28151709 0.36399711 0.30883691 0.26095258\n",
      "        nan        nan 0.36356679 0.23981191 0.28026316 0.28664344\n",
      " 0.32354437 0.36190476 0.31008478        nan        nan 0.28315892\n",
      " 0.35769231 0.32150225 0.25702256 0.29984985 0.30941522 0.28022545\n",
      "        nan        nan 0.30015023 0.40509259 0.2952381  0.24978829\n",
      " 0.263147   0.23997799 0.25962091        nan        nan 0.28553831\n",
      " 0.30793651 0.36441755 0.35589431 0.30373728 0.33401752 0.33977477\n",
      "        nan        nan 0.3207933  0.2526455  0.28611111 0.30647479\n",
      " 0.36574074 0.38047619 0.32841933        nan        nan 0.31921922\n",
      " 0.32470539 0.32777778 0.31925546 0.38964389 0.36555919 0.28543695\n",
      "        nan        nan 0.39630667 0.39606227 0.33289443 0.27370469\n",
      " 0.3281746  0.33337823 0.33333333        nan        nan 0.31428571\n",
      " 0.33761691 0.35372103 0.2940685  0.44367816 0.31015326 0.27474621\n",
      "        nan        nan 0.32794289 0.34333333 0.33995859 0.33404918\n",
      " 0.38306452 0.37225275 0.35391333        nan        nan 0.36776197\n",
      " 0.24991292 0.25551369 0.29210659 0.32478632 0.29630371 0.27898918\n",
      "        nan        nan 0.32953466 0.33096525 0.29946524 0.26269592\n",
      " 0.31797492 0.35470085 0.28818575        nan        nan 0.35802469\n",
      " 0.36267753 0.30087368 0.33058311 0.31650746 0.2427473  0.34157509\n",
      "        nan        nan 0.32878788 0.36368588 0.37057685 0.32990196\n",
      " 0.25697552 0.29514786 0.33772894        nan        nan 0.3154321\n",
      " 0.36763383 0.28387534 0.31315715 0.32596419 0.31609977 0.22418458\n",
      "        nan        nan 0.34700855 0.29093567 0.24323212 0.30979748\n",
      " 0.35338283 0.43249158 0.34007597        nan        nan 0.3038451\n",
      " 0.334196   0.07751938 0.25595238 0.08080808 0.29024377 0.20257296\n",
      "        nan        nan 0.33536255 0.3107705  0.29217172 0.25181797\n",
      " 0.34549214 0.26918964 0.30987654        nan        nan 0.31727107\n",
      " 0.41503268 0.19687174 0.08730159 0.31845238 0.23307494 0.28407557\n",
      "        nan        nan 0.31516831 0.24751067 0.16860465 0.22373737\n",
      " 0.2152381  0.32919799 0.27212121        nan        nan 0.28634212\n",
      " 0.30549451 0.30979748 0.20568343 0.30538462 0.26946483 0.24867725\n",
      "        nan        nan 0.32200749 0.21342771 0.24728488 0.31608392\n",
      " 0.28888889 0.13765182 0.22536129        nan        nan 0.17118554\n",
      " 0.3480464  0.34796128 0.32134059 0.35384259 0.3440122  0.28138528\n",
      "        nan        nan 0.29181287 0.35205314 0.35700758 0.32343732\n",
      " 0.29906305 0.40119048 0.32833333        nan        nan 0.28551191\n",
      " 0.38383257 0.3458048  0.31848259 0.36222222 0.40692641 0.29457672\n",
      "        nan        nan 0.30563505 0.29516391 0.3553339  0.30690225\n",
      " 0.35858586 0.32087565 0.37819575        nan        nan 0.3037037\n",
      " 0.35164452 0.31735043 0.25238095 0.36102151 0.26994048 0.26210317\n",
      "        nan        nan 0.29673203 0.34062007 0.35033085 0.32935455\n",
      " 0.37154635 0.34038462 0.29636886        nan        nan 0.3703273 ]\n",
      "One or more of the test scores are non-finite: [0.3480685  0.38861621 0.35725941 0.35555556 0.41692906 0.33713675\n",
      "        nan        nan 0.26381836 0.32572087 0.31298548 0.38091522\n",
      " 0.35862069 0.35938697 0.38611111        nan        nan 0.36780282\n",
      " 0.41173059 0.34666501 0.39716553 0.3588694  0.38820755 0.36310645\n",
      "        nan        nan 0.3782716  0.37698413 0.33862434 0.34009982\n",
      " 0.41165664 0.36233211 0.35052779        nan        nan 0.2594802\n",
      " 0.34822729 0.38280922 0.36946153 0.42593046 0.36092452 0.29791667\n",
      "        nan        nan 0.41303855 0.35787646 0.42874317 0.4096448\n",
      " 0.40526316 0.37669822 0.34409171        nan        nan 0.3741672\n",
      " 0.3096757  0.36044987 0.32290933 0.04301075 0.34738131 0.34165457\n",
      "        nan        nan 0.31906898 0.22334267 0.26944444 0.36520428\n",
      " 0.29719822 0.18070818 0.33151846        nan        nan 0.32515689\n",
      " 0.35788568 0.28964646 0.3532844  0.1464453  0.39369766 0.28888889\n",
      "        nan        nan 0.39035088 0.35603024 0.31357049 0.31437465\n",
      " 0.31396116 0.3638823  0.36039905        nan        nan 0.31760567\n",
      " 0.33055986 0.22445668 0.31299546 0.33644504 0.37973101 0.33761792\n",
      "        nan        nan 0.31490968 0.24784053 0.17434422 0.23997848\n",
      " 0.20468137 0.17687075 0.40259756        nan        nan 0.37752842\n",
      " 0.38990748 0.42233666 0.37770899 0.42506082 0.40007763 0.36462585\n",
      "        nan        nan 0.36086845 0.38728945 0.36171032 0.31767443\n",
      " 0.34814066 0.34890703 0.39646465        nan        nan 0.36856904\n",
      " 0.36358025 0.36225843 0.39094823 0.33675048 0.40134378 0.35102678\n",
      "        nan        nan 0.37268874 0.44241349 0.33713249 0.35413853\n",
      " 0.3997175  0.3232224  0.33206557        nan        nan 0.36515432\n",
      " 0.39268824 0.35895958 0.36791143 0.40747208 0.34065934 0.39031199\n",
      "        nan        nan 0.34130606 0.42795508 0.39174352 0.40754659\n",
      " 0.34138524 0.34703517 0.40936285        nan        nan 0.3188829\n",
      " 0.34584473 0.40244966 0.30492802 0.35364798 0.36794563 0.32398381\n",
      "        nan        nan 0.30368635 0.2979798  0.32755153 0.31929415\n",
      " 0.3488045  0.36050511 0.39361612        nan        nan 0.38127921\n",
      " 0.38266254 0.24323322 0.39619883 0.29204572 0.25333333 0.38501047\n",
      "        nan        nan 0.34881111 0.38470233 0.36101526 0.38681992\n",
      " 0.34271738 0.39074573 0.38852259        nan        nan 0.32176516\n",
      " 0.36948798 0.41264    0.29233512 0.37730149 0.32094793 0.3696932\n",
      "        nan        nan 0.32912495 0.35417249 0.35550239 0.36379928\n",
      " 0.30988456 0.3159125  0.34971954        nan        nan 0.33301587\n",
      " 0.14634146 0.23125892 0.29971548 0.10752688 0.18253968 0.27292677\n",
      "        nan        nan 0.23872146 0.21111111 0.32878493 0.18857143\n",
      " 0.14492754 0.21935484 0.17302326        nan        nan 0.27070015\n",
      " 0.32316298 0.27169811 0.21920849 0.16081871 0.20176043 0.24023845\n",
      "        nan        nan 0.28338523 0.12345679 0.18167362 0.04444444\n",
      " 0.34358073 0.32443427 0.28660819        nan        nan 0.33982619\n",
      " 0.22795543 0.17577413 0.24458586 0.17742411 0.17388742 0.22749231\n",
      "        nan        nan 0.23394383 0.34055911 0.04938272 0.35374971\n",
      " 0.2625731  0.23258959 0.22414389        nan        nan 0.3272956\n",
      " 0.34582011 0.42587413 0.25959009 0.356281   0.35377585 0.34299152\n",
      "        nan        nan 0.40897436 0.3532634  0.42073566 0.40079051\n",
      " 0.2494824  0.35750916 0.32509385        nan        nan 0.36262902\n",
      " 0.38549784 0.41510721 0.33172915 0.38339856 0.41138522 0.40856568\n",
      "        nan        nan 0.386111   0.3199535  0.34307011 0.34526448\n",
      " 0.22021116 0.20530303 0.36360051        nan        nan 0.34005075\n",
      " 0.33317478 0.20718816 0.33504274 0.36998025 0.33885088 0.3989899\n",
      "        nan        nan 0.36038647 0.37050961 0.34360594 0.35845295\n",
      " 0.43820513 0.37699645 0.34183721        nan        nan 0.33630687\n",
      " 0.37616741 0.35572708 0.39561868 0.35237343 0.35012009 0.34444444\n",
      "        nan        nan 0.34106747 0.34300063 0.35316298 0.34153369\n",
      " 0.37272727 0.40277778 0.36722613        nan        nan 0.41666667\n",
      " 0.4042735  0.40086022 0.38361144 0.35676928 0.39107254 0.34522309\n",
      "        nan        nan 0.37076271 0.38312937 0.34909091 0.38497603\n",
      " 0.39503857 0.4172043  0.34981175        nan        nan 0.38345103\n",
      " 0.36481649 0.36796441 0.37219251 0.38435168 0.33039983 0.4338816\n",
      "        nan        nan 0.37273243 0.41535513 0.36384356 0.34803002\n",
      " 0.42172792 0.36662132 0.34860425        nan        nan 0.40546682\n",
      " 0.34487734 0.35900383 0.27884104 0.32524783 0.34967423 0.33506494\n",
      "        nan        nan 0.34443563 0.31373886 0.29356196 0.35025004\n",
      " 0.35791962 0.35707756 0.32930495        nan        nan 0.38399872\n",
      " 0.34483206 0.36503378 0.36129293 0.3877551  0.36984934 0.33394407\n",
      "        nan        nan 0.44073986 0.26198386 0.3320723  0.36792347\n",
      " 0.27449141 0.37930701 0.40022676        nan        nan 0.3604282\n",
      " 0.40480545 0.36924401 0.32125969 0.3195963  0.36023107 0.32290704\n",
      "        nan        nan 0.37695078 0.40218424 0.37000672 0.34281661\n",
      " 0.27639662 0.30877956 0.35215979        nan        nan 0.36251061\n",
      " 0.35092097 0.42665705 0.43114754 0.37229204 0.37014767 0.3902367\n",
      "        nan        nan 0.39915571 0.31697451 0.35324327 0.36410256\n",
      " 0.41891534 0.40962364 0.43068857        nan        nan 0.4118493\n",
      " 0.38497107 0.37283951 0.38964389 0.44273107 0.41994949 0.34868421\n",
      "        nan        nan 0.44434006 0.43854727 0.41747506 0.30900464\n",
      " 0.37336357 0.38192735 0.39985476        nan        nan 0.39502165\n",
      " 0.38909091 0.43442814 0.36823051 0.49334661 0.3672956  0.34736123\n",
      "        nan        nan 0.38982845 0.38941799 0.37826087 0.41926407\n",
      " 0.44353947 0.43167626 0.41513584        nan        nan 0.41627817\n",
      " 0.31450549 0.35056814 0.33724061 0.3330178  0.37548126 0.34093098\n",
      "        nan        nan 0.39233079 0.36967707 0.37000614 0.33008951\n",
      " 0.36198582 0.40838958 0.32039374        nan        nan 0.42186151\n",
      " 0.41312183 0.38056456 0.41479666 0.37275457 0.27605688 0.38510005\n",
      "        nan        nan 0.33345938 0.42176265 0.43273484 0.36826156\n",
      " 0.34356693 0.3678641  0.3695496         nan        nan 0.34854281\n",
      " 0.41911905 0.36583261 0.3960199  0.38137255 0.38612546 0.31297044\n",
      "        nan        nan 0.39866036 0.33400075 0.34392253 0.36213152\n",
      " 0.40423732 0.44140009 0.38710402        nan        nan 0.36769908\n",
      " 0.38228246 0.10416667 0.30060326 0.09876543 0.3372723  0.23266964\n",
      "        nan        nan 0.41968525 0.36833319 0.35711501 0.32725038\n",
      " 0.33132814 0.30736257 0.3850392         nan        nan 0.36806224\n",
      " 0.41142071 0.27174573 0.11640212 0.32340521 0.2375     0.25344828\n",
      "        nan        nan 0.33531368 0.25862069 0.21714744 0.20210526\n",
      " 0.22799576 0.42345978 0.32881172        nan        nan 0.34978355\n",
      " 0.33695652 0.38938314 0.29460805 0.34161491 0.33355747 0.34091234\n",
      "        nan        nan 0.38800092 0.27919911 0.34156578 0.38148148\n",
      " 0.32444444 0.14766321 0.28958219        nan        nan 0.21618164\n",
      " 0.39916256 0.39808441 0.40729093 0.38550437 0.34069767 0.33715651\n",
      "        nan        nan 0.34148205 0.37643098 0.41349706 0.36998062\n",
      " 0.35898281 0.43653754 0.35467721        nan        nan 0.35211017\n",
      " 0.41575092 0.40037123 0.37780255 0.34833087 0.43358118 0.37029048\n",
      "        nan        nan 0.37868468 0.37813086 0.39832902 0.37878788\n",
      " 0.3968254  0.38019485 0.44098765        nan        nan 0.34747475\n",
      " 0.37346411 0.35405695 0.33131313 0.3852692  0.32198924 0.34187296\n",
      "        nan        nan 0.34184601 0.36682365 0.38535188 0.39586107\n",
      " 0.39688845 0.36176447 0.33987421        nan        nan 0.41739988]\n",
      "One or more of the test scores are non-finite: [0.67800225 0.65084688 0.6582144  0.67229421 0.66269509 0.67188158\n",
      "        nan        nan 0.63507783 0.6379313  0.63519196 0.6584032\n",
      " 0.65907209 0.64813666 0.65612861        nan        nan 0.66166171\n",
      " 0.68758105 0.61580463 0.70495801 0.66608353 0.65290069 0.65381836\n",
      "        nan        nan 0.66899032 0.69426895 0.70216655 0.6481361\n",
      " 0.67832441 0.65375228 0.67737726        nan        nan 0.5871004\n",
      " 0.67467091 0.69456052 0.67011417 0.6858908  0.68408133 0.59442236\n",
      "        nan        nan 0.67837274 0.68324905 0.68552401 0.66723742\n",
      " 0.65677283 0.68733051 0.63467463        nan        nan 0.6794684\n",
      " 0.64721427 0.67592747 0.66783385 0.51370924 0.64675054 0.67182336\n",
      "        nan        nan 0.66522926 0.56035646 0.64641268 0.6479558\n",
      " 0.62589595 0.62494936 0.64105572        nan        nan 0.66757842\n",
      " 0.69018342 0.61726495 0.65004879 0.58944143 0.67451067 0.63859575\n",
      "        nan        nan 0.7016275  0.63662641 0.66356378 0.68547198\n",
      " 0.67913404 0.69693555 0.69625095        nan        nan 0.66038603\n",
      " 0.67012923 0.56173259 0.63550026 0.72129882 0.65251662 0.64998817\n",
      "        nan        nan 0.63007075 0.57041766 0.57322713 0.60559095\n",
      " 0.61036367 0.60533949 0.70891323        nan        nan 0.73224377\n",
      " 0.68352851 0.69820504 0.67726192 0.74198346 0.68199084 0.68045177\n",
      "        nan        nan 0.71192934 0.7277543  0.69335433 0.66941441\n",
      " 0.69613719 0.66717587 0.69789056        nan        nan 0.69764308\n",
      " 0.6788836  0.67871578 0.69129923 0.67709447 0.71049508 0.69327864\n",
      "        nan        nan 0.7015544  0.7087699  0.68483746 0.64306886\n",
      " 0.69615373 0.66221934 0.67273586        nan        nan 0.67757262\n",
      " 0.70177536 0.66510838 0.67217435 0.685881   0.6785036  0.71270544\n",
      "        nan        nan 0.65085298 0.69511999 0.74807677 0.70585017\n",
      " 0.66956495 0.69224527 0.69464258        nan        nan 0.6604851\n",
      " 0.6892092  0.70397509 0.63459257 0.61291596 0.67928024 0.61808892\n",
      "        nan        nan 0.61642482 0.60460166 0.59915293 0.6336285\n",
      " 0.61118597 0.65302683 0.68750601        nan        nan 0.68816714\n",
      " 0.64611141 0.62477017 0.72732181 0.6070571  0.61518472 0.6654566\n",
      "        nan        nan 0.67341972 0.65428828 0.66572811 0.72081346\n",
      " 0.62585982 0.67839566 0.67983075        nan        nan 0.67281099\n",
      " 0.63825068 0.66845303 0.64924849 0.63084721 0.60201111 0.6530212\n",
      "        nan        nan 0.6366144  0.67782694 0.66212452 0.63550349\n",
      " 0.62575622 0.64086063 0.63856599        nan        nan 0.66948779\n",
      " 0.55802957 0.6281369  0.57297207 0.61667009 0.61915223 0.60624385\n",
      "        nan        nan 0.59604505 0.61201797 0.62277301 0.57255399\n",
      " 0.55291215 0.5765642  0.64419788        nan        nan 0.594864\n",
      " 0.66716145 0.60523396 0.56289082 0.57157246 0.62419212 0.6529846\n",
      "        nan        nan 0.64913482 0.61840368 0.57697258 0.43097696\n",
      " 0.69634198 0.6523621  0.5475673         nan        nan 0.6385878\n",
      " 0.61658183 0.54945402 0.57924624 0.54763254 0.51827707 0.61019806\n",
      "        nan        nan 0.66693467 0.63242444 0.57559718 0.67823144\n",
      " 0.65260977 0.60317996 0.61912026        nan        nan 0.65935839\n",
      " 0.65913364 0.71400875 0.6118483  0.65361098 0.67269538 0.64622563\n",
      "        nan        nan 0.70459482 0.69295603 0.70267409 0.72220217\n",
      " 0.63145179 0.68333102 0.65588972        nan        nan 0.66221546\n",
      " 0.71299109 0.69978865 0.6739938  0.70535437 0.67753316 0.68874343\n",
      "        nan        nan 0.6763059  0.6279968  0.64659954 0.66100964\n",
      " 0.58582666 0.58213177 0.69767043        nan        nan 0.65448891\n",
      " 0.69132695 0.57181634 0.66287825 0.63930993 0.66796758 0.64773678\n",
      "        nan        nan 0.67861375 0.66555881 0.63524482 0.64217882\n",
      " 0.68394484 0.65501105 0.65224353        nan        nan 0.61008809\n",
      " 0.69755972 0.63711778 0.69111551 0.67976819 0.68141399 0.68271333\n",
      "        nan        nan 0.67765718 0.68850445 0.673705   0.65907616\n",
      " 0.69369423 0.67632669 0.68023506        nan        nan 0.69035633\n",
      " 0.65699019 0.65935987 0.68451503 0.64145624 0.67317446 0.62822321\n",
      "        nan        nan 0.68151703 0.69577964 0.65707632 0.70190521\n",
      " 0.68309888 0.68052746 0.6517701         nan        nan 0.68555561\n",
      " 0.68990784 0.69051353 0.68506231 0.7145454  0.6042856  0.68840677\n",
      "        nan        nan 0.66608409 0.72421883 0.6475161  0.66721727\n",
      " 0.71337821 0.66907821 0.63329424        nan        nan 0.71569743\n",
      " 0.64860363 0.67802924 0.64544362 0.67340253 0.69072534 0.68072282\n",
      "        nan        nan 0.66363253 0.68510112 0.64118362 0.66304903\n",
      " 0.6857446  0.68437669 0.65842778        nan        nan 0.7247819\n",
      " 0.6717778  0.67740295 0.67927719 0.68627228 0.71301262 0.67152376\n",
      "        nan        nan 0.72155278 0.61926719 0.6665323  0.69971573\n",
      " 0.62041433 0.67059075 0.71210548        nan        nan 0.66826081\n",
      " 0.71978638 0.66210844 0.66853324 0.65372234 0.7077697  0.64274828\n",
      "        nan        nan 0.69505567 0.6886061  0.7164226  0.69612647\n",
      " 0.65505513 0.65077129 0.67346371        nan        nan 0.70269128\n",
      " 0.67254105 0.71976189 0.72209451 0.74200065 0.69966528 0.7080974\n",
      "        nan        nan 0.71861383 0.68286073 0.7052968  0.67898267\n",
      " 0.74303532 0.70970568 0.7383503         nan        nan 0.71727105\n",
      " 0.6897707  0.71475647 0.712221   0.7149985  0.73821233 0.69604274\n",
      "        nan        nan 0.72226732 0.71732955 0.73082762 0.68231863\n",
      " 0.67587479 0.68805984 0.72500148        nan        nan 0.68856415\n",
      " 0.70042196 0.74733709 0.69201451 0.7746723  0.71326232 0.67662556\n",
      "        nan        nan 0.68364606 0.6946862  0.72567093 0.71429154\n",
      " 0.73894249 0.73883566 0.71617928        nan        nan 0.75464565\n",
      " 0.61704316 0.66143826 0.62920077 0.64482436 0.68938312 0.6770503\n",
      "        nan        nan 0.69554602 0.67311429 0.672443   0.64207181\n",
      " 0.65599193 0.69899722 0.62488365        nan        nan 0.69989899\n",
      " 0.67260251 0.66185615 0.69799573 0.66800852 0.59490356 0.72331742\n",
      "        nan        nan 0.65693114 0.69409706 0.69598036 0.67275342\n",
      " 0.65752083 0.67138985 0.62854722        nan        nan 0.6684266\n",
      " 0.70764503 0.66628971 0.67745276 0.64106561 0.67260999 0.64767209\n",
      "        nan        nan 0.69271094 0.6486856  0.66951505 0.66271837\n",
      " 0.66772444 0.69716298 0.67689966        nan        nan 0.65202691\n",
      " 0.7103682  0.59411591 0.66927514 0.54750852 0.68153496 0.57058114\n",
      "        nan        nan 0.75374101 0.71084699 0.6732604  0.6742727\n",
      " 0.65882479 0.63687463 0.66584585        nan        nan 0.68243396\n",
      " 0.72362155 0.60181113 0.64285261 0.62720564 0.5823324  0.60103457\n",
      "        nan        nan 0.66981595 0.62368874 0.60327959 0.5846153\n",
      " 0.56916582 0.70839895 0.64578814        nan        nan 0.67320199\n",
      " 0.67266322 0.70403387 0.6047303  0.68462214 0.63989528 0.69385346\n",
      "        nan        nan 0.6569632  0.65052694 0.66846744 0.68430488\n",
      " 0.65976593 0.5655272  0.64138554        nan        nan 0.60779881\n",
      " 0.71989404 0.71815564 0.71711829 0.66930176 0.61272808 0.64200047\n",
      "        nan        nan 0.67474845 0.68469653 0.70224547 0.65562634\n",
      " 0.68273468 0.71900271 0.68481796        nan        nan 0.67023477\n",
      " 0.72589364 0.70588002 0.66507807 0.60541102 0.691765   0.6529407\n",
      "        nan        nan 0.68372905 0.68980369 0.69674666 0.7009685\n",
      " 0.67041664 0.68566845 0.72517808        nan        nan 0.6712906\n",
      " 0.71631448 0.71042716 0.65884475 0.68988788 0.61681268 0.6462906\n",
      "        nan        nan 0.67925326 0.71198627 0.63796743 0.69067876\n",
      " 0.67118358 0.68748152 0.64624134        nan        nan 0.72890116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator saved as: models/11042023_1923/MLPClassifier-HAB_modelling_5_8-11042023_1923.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>471</th>\n",
       "      <th>458</th>\n",
       "      <th>480</th>\n",
       "      <th>453</th>\n",
       "      <th>135</th>\n",
       "      <th>535</th>\n",
       "      <th>626</th>\n",
       "      <th>404</th>\n",
       "      <th>459</th>\n",
       "      <th>318</th>\n",
       "      <th>607</th>\n",
       "      <th>469</th>\n",
       "      <th>365</th>\n",
       "      <th>616</th>\n",
       "      <th>514</th>\n",
       "      <th>481</th>\n",
       "      <th>434</th>\n",
       "      <th>...</th>\n",
       "      <th>186</th>\n",
       "      <th>178</th>\n",
       "      <th>177</th>\n",
       "      <th>141</th>\n",
       "      <th>456</th>\n",
       "      <th>169</th>\n",
       "      <th>168</th>\n",
       "      <th>465</th>\n",
       "      <th>466</th>\n",
       "      <th>160</th>\n",
       "      <th>295</th>\n",
       "      <th>474</th>\n",
       "      <th>475</th>\n",
       "      <th>151</th>\n",
       "      <th>150</th>\n",
       "      <th>457</th>\n",
       "      <th>159</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>2.167066</td>\n",
       "      <td>2.734507</td>\n",
       "      <td>2.047373</td>\n",
       "      <td>1.964402</td>\n",
       "      <td>1.852783</td>\n",
       "      <td>0.262392</td>\n",
       "      <td>2.837131</td>\n",
       "      <td>1.911611</td>\n",
       "      <td>1.970372</td>\n",
       "      <td>2.099764</td>\n",
       "      <td>2.457142</td>\n",
       "      <td>2.269336</td>\n",
       "      <td>0.201277</td>\n",
       "      <td>3.240838</td>\n",
       "      <td>0.296037</td>\n",
       "      <td>2.14666</td>\n",
       "      <td>1.936174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015199</td>\n",
       "      <td>0.026251</td>\n",
       "      <td>0.015776</td>\n",
       "      <td>0.015556</td>\n",
       "      <td>0.016102</td>\n",
       "      <td>0.015105</td>\n",
       "      <td>0.015137</td>\n",
       "      <td>0.015664</td>\n",
       "      <td>0.015746</td>\n",
       "      <td>0.015762</td>\n",
       "      <td>0.019749</td>\n",
       "      <td>0.018816</td>\n",
       "      <td>0.015867</td>\n",
       "      <td>0.015411</td>\n",
       "      <td>0.015846</td>\n",
       "      <td>0.016008</td>\n",
       "      <td>0.015395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.22789</td>\n",
       "      <td>0.649784</td>\n",
       "      <td>0.560389</td>\n",
       "      <td>0.354595</td>\n",
       "      <td>0.095719</td>\n",
       "      <td>0.106844</td>\n",
       "      <td>0.210817</td>\n",
       "      <td>0.380058</td>\n",
       "      <td>0.07063</td>\n",
       "      <td>0.514306</td>\n",
       "      <td>0.509649</td>\n",
       "      <td>0.157522</td>\n",
       "      <td>0.053225</td>\n",
       "      <td>0.552569</td>\n",
       "      <td>0.18885</td>\n",
       "      <td>0.19812</td>\n",
       "      <td>0.30956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.015456</td>\n",
       "      <td>0.000554</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.004465</td>\n",
       "      <td>0.003997</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.000295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.011518</td>\n",
       "      <td>0.014452</td>\n",
       "      <td>0.011492</td>\n",
       "      <td>0.011541</td>\n",
       "      <td>0.01126</td>\n",
       "      <td>0.011578</td>\n",
       "      <td>0.012101</td>\n",
       "      <td>0.011378</td>\n",
       "      <td>0.012193</td>\n",
       "      <td>0.011546</td>\n",
       "      <td>0.011981</td>\n",
       "      <td>0.011551</td>\n",
       "      <td>0.011491</td>\n",
       "      <td>0.011555</td>\n",
       "      <td>0.011626</td>\n",
       "      <td>0.011487</td>\n",
       "      <td>0.011514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.002173</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__hidden_layer_sizes</th>\n",
       "      <td>(3,)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(2,)</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>...</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>(2,)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(2,)</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(2,)</td>\n",
       "      <td>(2,)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(2,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__solver</th>\n",
       "      <td>adam</td>\n",
       "      <td>adam</td>\n",
       "      <td>adam</td>\n",
       "      <td>adam</td>\n",
       "      <td>adam</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>adam</td>\n",
       "      <td>sgd</td>\n",
       "      <td>adam</td>\n",
       "      <td>adam</td>\n",
       "      <td>adam</td>\n",
       "      <td>adam</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>adam</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>adam</td>\n",
       "      <td>adam</td>\n",
       "      <td>...</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>adam</td>\n",
       "      <td>adam</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>adam</td>\n",
       "      <td>adam</td>\n",
       "      <td>adam</td>\n",
       "      <td>adam</td>\n",
       "      <td>adam</td>\n",
       "      <td>adam</td>\n",
       "      <td>adam</td>\n",
       "      <td>adam</td>\n",
       "      <td>adam</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_over__k_neighbors</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_over__sampling_strategy</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_under__sampling_strategy</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (2,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3, 3), 'clf__solv...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3, 3), 'clf__solv...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (2, 2), 'clf__solv...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3, 3), 'clf__solv...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3, 3), 'clf__solv...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3, 3), 'clf__solv...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (2, 2), 'clf__solv...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (2, 2), 'clf__solv...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (2, 2), 'clf__solv...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (2,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (2, 2), 'clf__solv...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (2, 2), 'clf__solv...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (2,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (2, 2), 'clf__solv...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (2,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (2,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (2,), 'clf__solver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_recall</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_recall</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_recall</th>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_recall</th>\n",
       "      <td>0.565079</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.531746</td>\n",
       "      <td>0.515873</td>\n",
       "      <td>0.548413</td>\n",
       "      <td>0.468254</td>\n",
       "      <td>0.531746</td>\n",
       "      <td>0.563492</td>\n",
       "      <td>0.501587</td>\n",
       "      <td>0.499206</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.563492</td>\n",
       "      <td>0.580952</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.549206</td>\n",
       "      <td>0.515873</td>\n",
       "      <td>0.565079</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_recall</th>\n",
       "      <td>0.138305</td>\n",
       "      <td>0.062904</td>\n",
       "      <td>0.09976</td>\n",
       "      <td>0.011224</td>\n",
       "      <td>0.019473</td>\n",
       "      <td>0.062492</td>\n",
       "      <td>0.09976</td>\n",
       "      <td>0.048924</td>\n",
       "      <td>0.072253</td>\n",
       "      <td>0.052176</td>\n",
       "      <td>0.062904</td>\n",
       "      <td>0.107069</td>\n",
       "      <td>0.041148</td>\n",
       "      <td>0.050992</td>\n",
       "      <td>0.052931</td>\n",
       "      <td>0.040468</td>\n",
       "      <td>0.100289</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_recall</th>\n",
       "      <td>24</td>\n",
       "      <td>96</td>\n",
       "      <td>76</td>\n",
       "      <td>113</td>\n",
       "      <td>53</td>\n",
       "      <td>216</td>\n",
       "      <td>76</td>\n",
       "      <td>37</td>\n",
       "      <td>128</td>\n",
       "      <td>153</td>\n",
       "      <td>202</td>\n",
       "      <td>36</td>\n",
       "      <td>15</td>\n",
       "      <td>236</td>\n",
       "      <td>48</td>\n",
       "      <td>113</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>512</td>\n",
       "      <td>521</td>\n",
       "      <td>557</td>\n",
       "      <td>609</td>\n",
       "      <td>602</td>\n",
       "      <td>548</td>\n",
       "      <td>529</td>\n",
       "      <td>577</td>\n",
       "      <td>570</td>\n",
       "      <td>536</td>\n",
       "      <td>524</td>\n",
       "      <td>576</td>\n",
       "      <td>578</td>\n",
       "      <td>604</td>\n",
       "      <td>601</td>\n",
       "      <td>603</td>\n",
       "      <td>565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_precision</th>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.37931</td>\n",
       "      <td>0.30303</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.27027</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.292683</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.292683</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_precision</th>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.375</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_precision</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.37931</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_precision</th>\n",
       "      <td>0.443678</td>\n",
       "      <td>0.396307</td>\n",
       "      <td>0.383065</td>\n",
       "      <td>0.389644</td>\n",
       "      <td>0.375668</td>\n",
       "      <td>0.432492</td>\n",
       "      <td>0.378196</td>\n",
       "      <td>0.363567</td>\n",
       "      <td>0.396062</td>\n",
       "      <td>0.395458</td>\n",
       "      <td>0.40119</td>\n",
       "      <td>0.353721</td>\n",
       "      <td>0.352223</td>\n",
       "      <td>0.406926</td>\n",
       "      <td>0.370577</td>\n",
       "      <td>0.372253</td>\n",
       "      <td>0.355894</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_precision</th>\n",
       "      <td>0.052321</td>\n",
       "      <td>0.031858</td>\n",
       "      <td>0.052985</td>\n",
       "      <td>0.033445</td>\n",
       "      <td>0.054076</td>\n",
       "      <td>0.101179</td>\n",
       "      <td>0.048988</td>\n",
       "      <td>0.068618</td>\n",
       "      <td>0.02332</td>\n",
       "      <td>0.05361</td>\n",
       "      <td>0.021887</td>\n",
       "      <td>0.071688</td>\n",
       "      <td>0.091412</td>\n",
       "      <td>0.065866</td>\n",
       "      <td>0.077438</td>\n",
       "      <td>0.011383</td>\n",
       "      <td>0.045848</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_precision</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>51</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>71</td>\n",
       "      <td>75</td>\n",
       "      <td>9</td>\n",
       "      <td>35</td>\n",
       "      <td>30</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>582</td>\n",
       "      <td>569</td>\n",
       "      <td>568</td>\n",
       "      <td>624</td>\n",
       "      <td>648</td>\n",
       "      <td>586</td>\n",
       "      <td>587</td>\n",
       "      <td>588</td>\n",
       "      <td>524</td>\n",
       "      <td>598</td>\n",
       "      <td>546</td>\n",
       "      <td>593</td>\n",
       "      <td>594</td>\n",
       "      <td>591</td>\n",
       "      <td>590</td>\n",
       "      <td>592</td>\n",
       "      <td>595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_f1</th>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.392157</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.44898</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.350877</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.37037</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.393443</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.393443</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_f1</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.436364</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.45283</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_f1</th>\n",
       "      <td>0.390244</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_f1</th>\n",
       "      <td>0.493347</td>\n",
       "      <td>0.44434</td>\n",
       "      <td>0.443539</td>\n",
       "      <td>0.442731</td>\n",
       "      <td>0.442413</td>\n",
       "      <td>0.4414</td>\n",
       "      <td>0.440988</td>\n",
       "      <td>0.44074</td>\n",
       "      <td>0.438547</td>\n",
       "      <td>0.438205</td>\n",
       "      <td>0.436538</td>\n",
       "      <td>0.434428</td>\n",
       "      <td>0.433882</td>\n",
       "      <td>0.433581</td>\n",
       "      <td>0.432735</td>\n",
       "      <td>0.431676</td>\n",
       "      <td>0.431148</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_f1</th>\n",
       "      <td>0.085669</td>\n",
       "      <td>0.017089</td>\n",
       "      <td>0.067196</td>\n",
       "      <td>0.017271</td>\n",
       "      <td>0.032285</td>\n",
       "      <td>0.068633</td>\n",
       "      <td>0.064761</td>\n",
       "      <td>0.066528</td>\n",
       "      <td>0.016305</td>\n",
       "      <td>0.039824</td>\n",
       "      <td>0.028871</td>\n",
       "      <td>0.085698</td>\n",
       "      <td>0.076436</td>\n",
       "      <td>0.056294</td>\n",
       "      <td>0.03391</td>\n",
       "      <td>0.01539</td>\n",
       "      <td>0.04876</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_f1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>632</td>\n",
       "      <td>633</td>\n",
       "      <td>634</td>\n",
       "      <td>635</td>\n",
       "      <td>636</td>\n",
       "      <td>637</td>\n",
       "      <td>638</td>\n",
       "      <td>639</td>\n",
       "      <td>640</td>\n",
       "      <td>641</td>\n",
       "      <td>642</td>\n",
       "      <td>643</td>\n",
       "      <td>644</td>\n",
       "      <td>645</td>\n",
       "      <td>646</td>\n",
       "      <td>647</td>\n",
       "      <td>648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_roc_auc</th>\n",
       "      <td>0.762281</td>\n",
       "      <td>0.75614</td>\n",
       "      <td>0.650877</td>\n",
       "      <td>0.619079</td>\n",
       "      <td>0.696053</td>\n",
       "      <td>0.646272</td>\n",
       "      <td>0.634868</td>\n",
       "      <td>0.674561</td>\n",
       "      <td>0.738377</td>\n",
       "      <td>0.612939</td>\n",
       "      <td>0.689035</td>\n",
       "      <td>0.718202</td>\n",
       "      <td>0.626535</td>\n",
       "      <td>0.641667</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.739474</td>\n",
       "      <td>0.683772</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_roc_auc</th>\n",
       "      <td>0.855879</td>\n",
       "      <td>0.729246</td>\n",
       "      <td>0.846608</td>\n",
       "      <td>0.776233</td>\n",
       "      <td>0.732196</td>\n",
       "      <td>0.768015</td>\n",
       "      <td>0.745048</td>\n",
       "      <td>0.746945</td>\n",
       "      <td>0.727771</td>\n",
       "      <td>0.755794</td>\n",
       "      <td>0.745048</td>\n",
       "      <td>0.817109</td>\n",
       "      <td>0.759376</td>\n",
       "      <td>0.727981</td>\n",
       "      <td>0.729246</td>\n",
       "      <td>0.816688</td>\n",
       "      <td>0.698272</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_roc_auc</th>\n",
       "      <td>0.705858</td>\n",
       "      <td>0.681416</td>\n",
       "      <td>0.719343</td>\n",
       "      <td>0.749684</td>\n",
       "      <td>0.698062</td>\n",
       "      <td>0.677202</td>\n",
       "      <td>0.795617</td>\n",
       "      <td>0.743152</td>\n",
       "      <td>0.685841</td>\n",
       "      <td>0.683102</td>\n",
       "      <td>0.722925</td>\n",
       "      <td>0.7067</td>\n",
       "      <td>0.679309</td>\n",
       "      <td>0.705647</td>\n",
       "      <td>0.700801</td>\n",
       "      <td>0.660346</td>\n",
       "      <td>0.784239</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_roc_auc</th>\n",
       "      <td>0.774672</td>\n",
       "      <td>0.722267</td>\n",
       "      <td>0.738942</td>\n",
       "      <td>0.714999</td>\n",
       "      <td>0.70877</td>\n",
       "      <td>0.697163</td>\n",
       "      <td>0.725178</td>\n",
       "      <td>0.721553</td>\n",
       "      <td>0.71733</td>\n",
       "      <td>0.683945</td>\n",
       "      <td>0.719003</td>\n",
       "      <td>0.747337</td>\n",
       "      <td>0.688407</td>\n",
       "      <td>0.691765</td>\n",
       "      <td>0.69598</td>\n",
       "      <td>0.738836</td>\n",
       "      <td>0.722095</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_roc_auc</th>\n",
       "      <td>0.061869</td>\n",
       "      <td>0.030903</td>\n",
       "      <td>0.0811</td>\n",
       "      <td>0.068686</td>\n",
       "      <td>0.016585</td>\n",
       "      <td>0.051667</td>\n",
       "      <td>0.067113</td>\n",
       "      <td>0.033264</td>\n",
       "      <td>0.022683</td>\n",
       "      <td>0.058324</td>\n",
       "      <td>0.023035</td>\n",
       "      <td>0.049559</td>\n",
       "      <td>0.054612</td>\n",
       "      <td>0.03658</td>\n",
       "      <td>0.029328</td>\n",
       "      <td>0.063828</td>\n",
       "      <td>0.04434</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_roc_auc</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>9</td>\n",
       "      <td>45</td>\n",
       "      <td>65</td>\n",
       "      <td>99</td>\n",
       "      <td>20</td>\n",
       "      <td>29</td>\n",
       "      <td>38</td>\n",
       "      <td>170</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>145</td>\n",
       "      <td>126</td>\n",
       "      <td>108</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>510</td>\n",
       "      <td>553</td>\n",
       "      <td>558</td>\n",
       "      <td>521</td>\n",
       "      <td>591</td>\n",
       "      <td>523</td>\n",
       "      <td>505</td>\n",
       "      <td>598</td>\n",
       "      <td>599</td>\n",
       "      <td>527</td>\n",
       "      <td>590</td>\n",
       "      <td>587</td>\n",
       "      <td>586</td>\n",
       "      <td>532</td>\n",
       "      <td>559</td>\n",
       "      <td>592</td>\n",
       "      <td>537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34 rows × 648 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              471  \\\n",
       "mean_fit_time                                                            2.167066   \n",
       "std_fit_time                                                              0.22789   \n",
       "mean_score_time                                                          0.011518   \n",
       "std_score_time                                                           0.000134   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         5   \n",
       "param_over__sampling_strategy                                                 0.6   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                            0.6   \n",
       "split1_test_recall                                                       0.714286   \n",
       "split2_test_recall                                                       0.380952   \n",
       "mean_test_recall                                                         0.565079   \n",
       "std_test_recall                                                          0.138305   \n",
       "rank_test_recall                                                               24   \n",
       "split0_test_precision                                                    0.413793   \n",
       "split1_test_precision                                                    0.517241   \n",
       "split2_test_precision                                                         0.4   \n",
       "mean_test_precision                                                      0.443678   \n",
       "std_test_precision                                                       0.052321   \n",
       "rank_test_precision                                                             2   \n",
       "split0_test_f1                                                           0.489796   \n",
       "split1_test_f1                                                                0.6   \n",
       "split2_test_f1                                                           0.390244   \n",
       "mean_test_f1                                                             0.493347   \n",
       "std_test_f1                                                              0.085669   \n",
       "rank_test_f1                                                                    1   \n",
       "split0_test_roc_auc                                                      0.762281   \n",
       "split1_test_roc_auc                                                      0.855879   \n",
       "split2_test_roc_auc                                                      0.705858   \n",
       "mean_test_roc_auc                                                        0.774672   \n",
       "std_test_roc_auc                                                         0.061869   \n",
       "rank_test_roc_auc                                                               1   \n",
       "\n",
       "                                                                              458  \\\n",
       "mean_fit_time                                                            2.734507   \n",
       "std_fit_time                                                             0.649784   \n",
       "mean_score_time                                                          0.014452   \n",
       "std_score_time                                                           0.002173   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         3   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.8   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                       0.571429   \n",
       "split2_test_recall                                                       0.428571   \n",
       "mean_test_recall                                                         0.516667   \n",
       "std_test_recall                                                          0.062904   \n",
       "rank_test_recall                                                               96   \n",
       "split0_test_precision                                                    0.407407   \n",
       "split1_test_precision                                                    0.352941   \n",
       "split2_test_precision                                                    0.428571   \n",
       "mean_test_precision                                                      0.396307   \n",
       "std_test_precision                                                       0.031858   \n",
       "rank_test_precision                                                            13   \n",
       "split0_test_f1                                                           0.468085   \n",
       "split1_test_f1                                                           0.436364   \n",
       "split2_test_f1                                                           0.428571   \n",
       "mean_test_f1                                                              0.44434   \n",
       "std_test_f1                                                              0.017089   \n",
       "rank_test_f1                                                                    2   \n",
       "split0_test_roc_auc                                                       0.75614   \n",
       "split1_test_roc_auc                                                      0.729246   \n",
       "split2_test_roc_auc                                                      0.681416   \n",
       "mean_test_roc_auc                                                        0.722267   \n",
       "std_test_roc_auc                                                         0.030903   \n",
       "rank_test_roc_auc                                                              26   \n",
       "\n",
       "                                                                              480  \\\n",
       "mean_fit_time                                                            2.047373   \n",
       "std_fit_time                                                             0.560389   \n",
       "mean_score_time                                                          0.011492   \n",
       "std_score_time                                                           0.000093   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         6   \n",
       "param_over__sampling_strategy                                                 0.6   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                       0.666667   \n",
       "split2_test_recall                                                       0.428571   \n",
       "mean_test_recall                                                         0.531746   \n",
       "std_test_recall                                                           0.09976   \n",
       "rank_test_recall                                                               76   \n",
       "split0_test_precision                                                    0.322581   \n",
       "split1_test_precision                                                    0.451613   \n",
       "split2_test_precision                                                       0.375   \n",
       "mean_test_precision                                                      0.383065   \n",
       "std_test_precision                                                       0.052985   \n",
       "rank_test_precision                                                            22   \n",
       "split0_test_f1                                                           0.392157   \n",
       "split1_test_f1                                                           0.538462   \n",
       "split2_test_f1                                                                0.4   \n",
       "mean_test_f1                                                             0.443539   \n",
       "std_test_f1                                                              0.067196   \n",
       "rank_test_f1                                                                    3   \n",
       "split0_test_roc_auc                                                      0.650877   \n",
       "split1_test_roc_auc                                                      0.846608   \n",
       "split2_test_roc_auc                                                      0.719343   \n",
       "mean_test_roc_auc                                                        0.738942   \n",
       "std_test_roc_auc                                                           0.0811   \n",
       "rank_test_roc_auc                                                               9   \n",
       "\n",
       "                                                                              453  \\\n",
       "mean_fit_time                                                            1.964402   \n",
       "std_fit_time                                                             0.354595   \n",
       "mean_score_time                                                          0.011541   \n",
       "std_score_time                                                           0.000088   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         3   \n",
       "param_over__sampling_strategy                                                 0.6   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                        0.52381   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.515873   \n",
       "std_test_recall                                                          0.011224   \n",
       "rank_test_recall                                                              113   \n",
       "split0_test_precision                                                    0.434783   \n",
       "split1_test_precision                                                    0.354839   \n",
       "split2_test_precision                                                     0.37931   \n",
       "mean_test_precision                                                      0.389644   \n",
       "std_test_precision                                                       0.033445   \n",
       "rank_test_precision                                                            18   \n",
       "split0_test_f1                                                           0.465116   \n",
       "split1_test_f1                                                           0.423077   \n",
       "split2_test_f1                                                               0.44   \n",
       "mean_test_f1                                                             0.442731   \n",
       "std_test_f1                                                              0.017271   \n",
       "rank_test_f1                                                                    4   \n",
       "split0_test_roc_auc                                                      0.619079   \n",
       "split1_test_roc_auc                                                      0.776233   \n",
       "split2_test_roc_auc                                                      0.749684   \n",
       "mean_test_roc_auc                                                        0.714999   \n",
       "std_test_roc_auc                                                         0.068686   \n",
       "rank_test_roc_auc                                                              45   \n",
       "\n",
       "                                                                              135  \\\n",
       "mean_fit_time                                                            1.852783   \n",
       "std_fit_time                                                             0.095719   \n",
       "mean_score_time                                                           0.01126   \n",
       "std_score_time                                                           0.000666   \n",
       "param_clf__hidden_layer_sizes                                                (2,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         4   \n",
       "param_over__sampling_strategy                                                 0.5   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__hidden_layer_sizes': (2,), 'clf__solver...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                        0.52381   \n",
       "split2_test_recall                                                       0.571429   \n",
       "mean_test_recall                                                         0.548413   \n",
       "std_test_recall                                                          0.019473   \n",
       "rank_test_recall                                                               53   \n",
       "split0_test_precision                                                     0.37931   \n",
       "split1_test_precision                                                        0.44   \n",
       "split2_test_precision                                                    0.307692   \n",
       "mean_test_precision                                                      0.375668   \n",
       "std_test_precision                                                       0.054076   \n",
       "rank_test_precision                                                            27   \n",
       "split0_test_f1                                                            0.44898   \n",
       "split1_test_f1                                                           0.478261   \n",
       "split2_test_f1                                                                0.4   \n",
       "mean_test_f1                                                             0.442413   \n",
       "std_test_f1                                                              0.032285   \n",
       "rank_test_f1                                                                    5   \n",
       "split0_test_roc_auc                                                      0.696053   \n",
       "split1_test_roc_auc                                                      0.732196   \n",
       "split2_test_roc_auc                                                      0.698062   \n",
       "mean_test_roc_auc                                                         0.70877   \n",
       "std_test_roc_auc                                                         0.016585   \n",
       "rank_test_roc_auc                                                              65   \n",
       "\n",
       "                                                                              535  \\\n",
       "mean_fit_time                                                            0.262392   \n",
       "std_fit_time                                                             0.106844   \n",
       "mean_score_time                                                          0.011578   \n",
       "std_score_time                                                           0.000164   \n",
       "param_clf__hidden_layer_sizes                                              (3, 3)   \n",
       "param_clf__solver                                                           lbfgs   \n",
       "param_over__k_neighbors                                                         6   \n",
       "param_over__sampling_strategy                                                 0.6   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__hidden_layer_sizes': (3, 3), 'clf__solv...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                        0.52381   \n",
       "split2_test_recall                                                       0.380952   \n",
       "mean_test_recall                                                         0.468254   \n",
       "std_test_recall                                                          0.062492   \n",
       "rank_test_recall                                                              216   \n",
       "split0_test_precision                                                     0.30303   \n",
       "split1_test_precision                                                        0.55   \n",
       "split2_test_precision                                                    0.444444   \n",
       "mean_test_precision                                                      0.432492   \n",
       "std_test_precision                                                       0.101179   \n",
       "rank_test_precision                                                             4   \n",
       "split0_test_f1                                                           0.377358   \n",
       "split1_test_f1                                                           0.536585   \n",
       "split2_test_f1                                                           0.410256   \n",
       "mean_test_f1                                                               0.4414   \n",
       "std_test_f1                                                              0.068633   \n",
       "rank_test_f1                                                                    6   \n",
       "split0_test_roc_auc                                                      0.646272   \n",
       "split1_test_roc_auc                                                      0.768015   \n",
       "split2_test_roc_auc                                                      0.677202   \n",
       "mean_test_roc_auc                                                        0.697163   \n",
       "std_test_roc_auc                                                         0.051667   \n",
       "rank_test_roc_auc                                                              99   \n",
       "\n",
       "                                                                              626  \\\n",
       "mean_fit_time                                                            2.837131   \n",
       "std_fit_time                                                             0.210817   \n",
       "mean_score_time                                                          0.012101   \n",
       "std_score_time                                                           0.000738   \n",
       "param_clf__hidden_layer_sizes                                              (3, 3)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         4   \n",
       "param_over__sampling_strategy                                                 0.6   \n",
       "param_under__sampling_strategy                                                0.8   \n",
       "params                          {'clf__hidden_layer_sizes': (3, 3), 'clf__solv...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                       0.666667   \n",
       "mean_test_recall                                                         0.531746   \n",
       "std_test_recall                                                           0.09976   \n",
       "rank_test_recall                                                               76   \n",
       "split0_test_precision                                                         0.4   \n",
       "split1_test_precision                                                    0.310345   \n",
       "split2_test_precision                                                    0.424242   \n",
       "mean_test_precision                                                      0.378196   \n",
       "std_test_precision                                                       0.048988   \n",
       "rank_test_precision                                                            25   \n",
       "split0_test_f1                                                           0.444444   \n",
       "split1_test_f1                                                               0.36   \n",
       "split2_test_f1                                                           0.518519   \n",
       "mean_test_f1                                                             0.440988   \n",
       "std_test_f1                                                              0.064761   \n",
       "rank_test_f1                                                                    7   \n",
       "split0_test_roc_auc                                                      0.634868   \n",
       "split1_test_roc_auc                                                      0.745048   \n",
       "split2_test_roc_auc                                                      0.795617   \n",
       "mean_test_roc_auc                                                        0.725178   \n",
       "std_test_roc_auc                                                         0.067113   \n",
       "rank_test_roc_auc                                                              20   \n",
       "\n",
       "                                                                              404  \\\n",
       "mean_fit_time                                                            1.911611   \n",
       "std_fit_time                                                             0.380058   \n",
       "mean_score_time                                                          0.011378   \n",
       "std_score_time                                                           0.000196   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                             sgd   \n",
       "param_over__k_neighbors                                                         3   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.8   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                       0.571429   \n",
       "split2_test_recall                                                       0.619048   \n",
       "mean_test_recall                                                         0.563492   \n",
       "std_test_recall                                                          0.048924   \n",
       "rank_test_recall                                                               37   \n",
       "split0_test_precision                                                     0.27027   \n",
       "split1_test_precision                                                    0.387097   \n",
       "split2_test_precision                                                    0.433333   \n",
       "mean_test_precision                                                      0.363567   \n",
       "std_test_precision                                                       0.068618   \n",
       "rank_test_precision                                                            51   \n",
       "split0_test_f1                                                           0.350877   \n",
       "split1_test_f1                                                           0.461538   \n",
       "split2_test_f1                                                           0.509804   \n",
       "mean_test_f1                                                              0.44074   \n",
       "std_test_f1                                                              0.066528   \n",
       "rank_test_f1                                                                    8   \n",
       "split0_test_roc_auc                                                      0.674561   \n",
       "split1_test_roc_auc                                                      0.746945   \n",
       "split2_test_roc_auc                                                      0.743152   \n",
       "mean_test_roc_auc                                                        0.721553   \n",
       "std_test_roc_auc                                                         0.033264   \n",
       "rank_test_roc_auc                                                              29   \n",
       "\n",
       "                                                                              459  \\\n",
       "mean_fit_time                                                            1.970372   \n",
       "std_fit_time                                                              0.07063   \n",
       "mean_score_time                                                          0.012193   \n",
       "std_score_time                                                           0.000755   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         4   \n",
       "param_over__sampling_strategy                                                 0.5   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                            0.6   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                       0.428571   \n",
       "mean_test_recall                                                         0.501587   \n",
       "std_test_recall                                                          0.072253   \n",
       "rank_test_recall                                                              128   \n",
       "split0_test_precision                                                       0.375   \n",
       "split1_test_precision                                                    0.384615   \n",
       "split2_test_precision                                                    0.428571   \n",
       "mean_test_precision                                                      0.396062   \n",
       "std_test_precision                                                        0.02332   \n",
       "rank_test_precision                                                            14   \n",
       "split0_test_f1                                                           0.461538   \n",
       "split1_test_f1                                                           0.425532   \n",
       "split2_test_f1                                                           0.428571   \n",
       "mean_test_f1                                                             0.438547   \n",
       "std_test_f1                                                              0.016305   \n",
       "rank_test_f1                                                                    9   \n",
       "split0_test_roc_auc                                                      0.738377   \n",
       "split1_test_roc_auc                                                      0.727771   \n",
       "split2_test_roc_auc                                                      0.685841   \n",
       "mean_test_roc_auc                                                         0.71733   \n",
       "std_test_roc_auc                                                         0.022683   \n",
       "rank_test_roc_auc                                                              38   \n",
       "\n",
       "                                                                              318  \\\n",
       "mean_fit_time                                                            2.099764   \n",
       "std_fit_time                                                             0.514306   \n",
       "mean_score_time                                                          0.011546   \n",
       "std_score_time                                                           0.000101   \n",
       "param_clf__hidden_layer_sizes                                              (2, 2)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         6   \n",
       "param_over__sampling_strategy                                                 0.6   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__hidden_layer_sizes': (2, 2), 'clf__solv...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                       0.571429   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.499206   \n",
       "std_test_recall                                                          0.052176   \n",
       "rank_test_recall                                                              153   \n",
       "split0_test_precision                                                        0.45   \n",
       "split1_test_precision                                                    0.413793   \n",
       "split2_test_precision                                                    0.322581   \n",
       "mean_test_precision                                                      0.395458   \n",
       "std_test_precision                                                        0.05361   \n",
       "rank_test_precision                                                            15   \n",
       "split0_test_f1                                                               0.45   \n",
       "split1_test_f1                                                               0.48   \n",
       "split2_test_f1                                                           0.384615   \n",
       "mean_test_f1                                                             0.438205   \n",
       "std_test_f1                                                              0.039824   \n",
       "rank_test_f1                                                                   10   \n",
       "split0_test_roc_auc                                                      0.612939   \n",
       "split1_test_roc_auc                                                      0.755794   \n",
       "split2_test_roc_auc                                                      0.683102   \n",
       "mean_test_roc_auc                                                        0.683945   \n",
       "std_test_roc_auc                                                         0.058324   \n",
       "rank_test_roc_auc                                                             170   \n",
       "\n",
       "                                                                              607  \\\n",
       "mean_fit_time                                                            2.457142   \n",
       "std_fit_time                                                             0.509649   \n",
       "mean_score_time                                                          0.011981   \n",
       "std_score_time                                                           0.000502   \n",
       "param_clf__hidden_layer_sizes                                              (3, 3)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         2   \n",
       "param_over__sampling_strategy                                                 0.6   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__hidden_layer_sizes': (3, 3), 'clf__solv...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                       0.571429   \n",
       "split2_test_recall                                                       0.428571   \n",
       "mean_test_recall                                                         0.483333   \n",
       "std_test_recall                                                          0.062904   \n",
       "rank_test_recall                                                              202   \n",
       "split0_test_precision                                                    0.428571   \n",
       "split1_test_precision                                                         0.4   \n",
       "split2_test_precision                                                       0.375   \n",
       "mean_test_precision                                                       0.40119   \n",
       "std_test_precision                                                       0.021887   \n",
       "rank_test_precision                                                            11   \n",
       "split0_test_f1                                                           0.439024   \n",
       "split1_test_f1                                                           0.470588   \n",
       "split2_test_f1                                                                0.4   \n",
       "mean_test_f1                                                             0.436538   \n",
       "std_test_f1                                                              0.028871   \n",
       "rank_test_f1                                                                   11   \n",
       "split0_test_roc_auc                                                      0.689035   \n",
       "split1_test_roc_auc                                                      0.745048   \n",
       "split2_test_roc_auc                                                      0.722925   \n",
       "mean_test_roc_auc                                                        0.719003   \n",
       "std_test_roc_auc                                                         0.023035   \n",
       "rank_test_roc_auc                                                              35   \n",
       "\n",
       "                                                                              469  \\\n",
       "mean_fit_time                                                            2.269336   \n",
       "std_fit_time                                                             0.157522   \n",
       "mean_score_time                                                          0.011551   \n",
       "std_score_time                                                           0.000044   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         5   \n",
       "param_over__sampling_strategy                                                 0.5   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                       0.714286   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.563492   \n",
       "std_test_recall                                                          0.107069   \n",
       "rank_test_recall                                                               36   \n",
       "split0_test_precision                                                    0.294118   \n",
       "split1_test_precision                                                    0.454545   \n",
       "split2_test_precision                                                      0.3125   \n",
       "mean_test_precision                                                      0.353721   \n",
       "std_test_precision                                                       0.071688   \n",
       "rank_test_precision                                                            71   \n",
       "split0_test_f1                                                            0.37037   \n",
       "split1_test_f1                                                           0.555556   \n",
       "split2_test_f1                                                           0.377358   \n",
       "mean_test_f1                                                             0.434428   \n",
       "std_test_f1                                                              0.085698   \n",
       "rank_test_f1                                                                   12   \n",
       "split0_test_roc_auc                                                      0.718202   \n",
       "split1_test_roc_auc                                                      0.817109   \n",
       "split2_test_roc_auc                                                        0.7067   \n",
       "mean_test_roc_auc                                                        0.747337   \n",
       "std_test_roc_auc                                                         0.049559   \n",
       "rank_test_roc_auc                                                               5   \n",
       "\n",
       "                                                                              365  \\\n",
       "mean_fit_time                                                            0.201277   \n",
       "std_fit_time                                                             0.053225   \n",
       "mean_score_time                                                          0.011491   \n",
       "std_score_time                                                           0.000114   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                           lbfgs   \n",
       "param_over__k_neighbors                                                         5   \n",
       "param_over__sampling_strategy                                                 0.6   \n",
       "param_under__sampling_strategy                                                0.8   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                            0.6   \n",
       "split1_test_recall                                                       0.619048   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.580952   \n",
       "std_test_recall                                                          0.041148   \n",
       "rank_test_recall                                                               15   \n",
       "split0_test_precision                                                    0.285714   \n",
       "split1_test_precision                                                    0.481481   \n",
       "split2_test_precision                                                    0.289474   \n",
       "mean_test_precision                                                      0.352223   \n",
       "std_test_precision                                                       0.091412   \n",
       "rank_test_precision                                                            75   \n",
       "split0_test_f1                                                           0.387097   \n",
       "split1_test_f1                                                           0.541667   \n",
       "split2_test_f1                                                           0.372881   \n",
       "mean_test_f1                                                             0.433882   \n",
       "std_test_f1                                                              0.076436   \n",
       "rank_test_f1                                                                   13   \n",
       "split0_test_roc_auc                                                      0.626535   \n",
       "split1_test_roc_auc                                                      0.759376   \n",
       "split2_test_roc_auc                                                      0.679309   \n",
       "mean_test_roc_auc                                                        0.688407   \n",
       "std_test_roc_auc                                                         0.054612   \n",
       "rank_test_roc_auc                                                             145   \n",
       "\n",
       "                                                                              616  \\\n",
       "mean_fit_time                                                            3.240838   \n",
       "std_fit_time                                                             0.552569   \n",
       "mean_score_time                                                          0.011555   \n",
       "std_score_time                                                           0.000097   \n",
       "param_clf__hidden_layer_sizes                                              (3, 3)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         3   \n",
       "param_over__sampling_strategy                                                 0.6   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__hidden_layer_sizes': (3, 3), 'clf__solv...   \n",
       "split0_test_recall                                                            0.4   \n",
       "split1_test_recall                                                        0.52381   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.466667   \n",
       "std_test_recall                                                          0.050992   \n",
       "rank_test_recall                                                              236   \n",
       "split0_test_precision                                                    0.363636   \n",
       "split1_test_precision                                                         0.5   \n",
       "split2_test_precision                                                    0.357143   \n",
       "mean_test_precision                                                      0.406926   \n",
       "std_test_precision                                                       0.065866   \n",
       "rank_test_precision                                                             9   \n",
       "split0_test_f1                                                           0.380952   \n",
       "split1_test_f1                                                           0.511628   \n",
       "split2_test_f1                                                           0.408163   \n",
       "mean_test_f1                                                             0.433581   \n",
       "std_test_f1                                                              0.056294   \n",
       "rank_test_f1                                                                   14   \n",
       "split0_test_roc_auc                                                      0.641667   \n",
       "split1_test_roc_auc                                                      0.727981   \n",
       "split2_test_roc_auc                                                      0.705647   \n",
       "mean_test_roc_auc                                                        0.691765   \n",
       "std_test_roc_auc                                                          0.03658   \n",
       "rank_test_roc_auc                                                             126   \n",
       "\n",
       "                                                                              514  \\\n",
       "mean_fit_time                                                            0.296037   \n",
       "std_fit_time                                                              0.18885   \n",
       "mean_score_time                                                          0.011626   \n",
       "std_score_time                                                           0.000104   \n",
       "param_clf__hidden_layer_sizes                                              (3, 3)   \n",
       "param_clf__solver                                                           lbfgs   \n",
       "param_over__k_neighbors                                                         4   \n",
       "param_over__sampling_strategy                                                 0.5   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__hidden_layer_sizes': (3, 3), 'clf__solv...   \n",
       "split0_test_recall                                                            0.6   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                       0.571429   \n",
       "mean_test_recall                                                         0.549206   \n",
       "std_test_recall                                                          0.052931   \n",
       "rank_test_recall                                                               48   \n",
       "split0_test_precision                                                    0.292683   \n",
       "split1_test_precision                                                     0.47619   \n",
       "split2_test_precision                                                    0.342857   \n",
       "mean_test_precision                                                      0.370577   \n",
       "std_test_precision                                                       0.077438   \n",
       "rank_test_precision                                                            35   \n",
       "split0_test_f1                                                           0.393443   \n",
       "split1_test_f1                                                            0.47619   \n",
       "split2_test_f1                                                           0.428571   \n",
       "mean_test_f1                                                             0.432735   \n",
       "std_test_f1                                                               0.03391   \n",
       "rank_test_f1                                                                   15   \n",
       "split0_test_roc_auc                                                      0.657895   \n",
       "split1_test_roc_auc                                                      0.729246   \n",
       "split2_test_roc_auc                                                      0.700801   \n",
       "mean_test_roc_auc                                                         0.69598   \n",
       "std_test_roc_auc                                                         0.029328   \n",
       "rank_test_roc_auc                                                             108   \n",
       "\n",
       "                                                                              481  \\\n",
       "mean_fit_time                                                             2.14666   \n",
       "std_fit_time                                                              0.19812   \n",
       "mean_score_time                                                          0.011487   \n",
       "std_score_time                                                           0.000209   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         6   \n",
       "param_over__sampling_strategy                                                 0.6   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                       0.571429   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.515873   \n",
       "std_test_recall                                                          0.040468   \n",
       "rank_test_recall                                                              113   \n",
       "split0_test_precision                                                    0.357143   \n",
       "split1_test_precision                                                       0.375   \n",
       "split2_test_precision                                                    0.384615   \n",
       "mean_test_precision                                                      0.372253   \n",
       "std_test_precision                                                       0.011383   \n",
       "rank_test_precision                                                            30   \n",
       "split0_test_f1                                                           0.416667   \n",
       "split1_test_f1                                                            0.45283   \n",
       "split2_test_f1                                                           0.425532   \n",
       "mean_test_f1                                                             0.431676   \n",
       "std_test_f1                                                               0.01539   \n",
       "rank_test_f1                                                                   16   \n",
       "split0_test_roc_auc                                                      0.739474   \n",
       "split1_test_roc_auc                                                      0.816688   \n",
       "split2_test_roc_auc                                                      0.660346   \n",
       "mean_test_roc_auc                                                        0.738836   \n",
       "std_test_roc_auc                                                         0.063828   \n",
       "rank_test_roc_auc                                                              10   \n",
       "\n",
       "                                                                              434  \\\n",
       "mean_fit_time                                                            1.936174   \n",
       "std_fit_time                                                              0.30956   \n",
       "mean_score_time                                                          0.011514   \n",
       "std_score_time                                                           0.000045   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         1   \n",
       "param_over__sampling_strategy                                                 0.5   \n",
       "param_under__sampling_strategy                                                0.8   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                            0.6   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                       0.666667   \n",
       "mean_test_recall                                                         0.565079   \n",
       "std_test_recall                                                          0.100289   \n",
       "rank_test_recall                                                               26   \n",
       "split0_test_precision                                                    0.292683   \n",
       "split1_test_precision                                                       0.375   \n",
       "split2_test_precision                                                         0.4   \n",
       "mean_test_precision                                                      0.355894   \n",
       "std_test_precision                                                       0.045848   \n",
       "rank_test_precision                                                            65   \n",
       "split0_test_f1                                                           0.393443   \n",
       "split1_test_f1                                                                0.4   \n",
       "split2_test_f1                                                                0.5   \n",
       "mean_test_f1                                                             0.431148   \n",
       "std_test_f1                                                               0.04876   \n",
       "rank_test_f1                                                                   17   \n",
       "split0_test_roc_auc                                                      0.683772   \n",
       "split1_test_roc_auc                                                      0.698272   \n",
       "split2_test_roc_auc                                                      0.784239   \n",
       "mean_test_roc_auc                                                        0.722095   \n",
       "std_test_roc_auc                                                          0.04434   \n",
       "rank_test_roc_auc                                                              28   \n",
       "\n",
       "                                ...  \\\n",
       "mean_fit_time                   ...   \n",
       "std_fit_time                    ...   \n",
       "mean_score_time                 ...   \n",
       "std_score_time                  ...   \n",
       "param_clf__hidden_layer_sizes   ...   \n",
       "param_clf__solver               ...   \n",
       "param_over__k_neighbors         ...   \n",
       "param_over__sampling_strategy   ...   \n",
       "param_under__sampling_strategy  ...   \n",
       "params                          ...   \n",
       "split0_test_recall              ...   \n",
       "split1_test_recall              ...   \n",
       "split2_test_recall              ...   \n",
       "mean_test_recall                ...   \n",
       "std_test_recall                 ...   \n",
       "rank_test_recall                ...   \n",
       "split0_test_precision           ...   \n",
       "split1_test_precision           ...   \n",
       "split2_test_precision           ...   \n",
       "mean_test_precision             ...   \n",
       "std_test_precision              ...   \n",
       "rank_test_precision             ...   \n",
       "split0_test_f1                  ...   \n",
       "split1_test_f1                  ...   \n",
       "split2_test_f1                  ...   \n",
       "mean_test_f1                    ...   \n",
       "std_test_f1                     ...   \n",
       "rank_test_f1                    ...   \n",
       "split0_test_roc_auc             ...   \n",
       "split1_test_roc_auc             ...   \n",
       "split2_test_roc_auc             ...   \n",
       "mean_test_roc_auc               ...   \n",
       "std_test_roc_auc                ...   \n",
       "rank_test_roc_auc               ...   \n",
       "\n",
       "                                                                              186  \\\n",
       "mean_fit_time                                                            0.015199   \n",
       "std_fit_time                                                             0.000076   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                              (2, 2)   \n",
       "param_clf__solver                                                           lbfgs   \n",
       "param_over__k_neighbors                                                         3   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__hidden_layer_sizes': (2, 2), 'clf__solv...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              512   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           582   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  632   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             510   \n",
       "\n",
       "                                                                              178  \\\n",
       "mean_fit_time                                                            0.026251   \n",
       "std_fit_time                                                             0.015456   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                              (2, 2)   \n",
       "param_clf__solver                                                           lbfgs   \n",
       "param_over__k_neighbors                                                         2   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__hidden_layer_sizes': (2, 2), 'clf__solv...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              521   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           569   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  633   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             553   \n",
       "\n",
       "                                                                              177  \\\n",
       "mean_fit_time                                                            0.015776   \n",
       "std_fit_time                                                             0.000554   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                              (2, 2)   \n",
       "param_clf__solver                                                           lbfgs   \n",
       "param_over__k_neighbors                                                         2   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__hidden_layer_sizes': (2, 2), 'clf__solv...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              557   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           568   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  634   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             558   \n",
       "\n",
       "                                                                              141  \\\n",
       "mean_fit_time                                                            0.015556   \n",
       "std_fit_time                                                             0.000153   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                                (2,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         4   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__hidden_layer_sizes': (2,), 'clf__solver...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              609   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           624   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  635   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             521   \n",
       "\n",
       "                                                                              456  \\\n",
       "mean_fit_time                                                            0.016102   \n",
       "std_fit_time                                                             0.000564   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         3   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              602   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           648   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  636   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             591   \n",
       "\n",
       "                                                                              169  \\\n",
       "mean_fit_time                                                            0.015105   \n",
       "std_fit_time                                                             0.000027   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                              (2, 2)   \n",
       "param_clf__solver                                                           lbfgs   \n",
       "param_over__k_neighbors                                                         1   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__hidden_layer_sizes': (2, 2), 'clf__solv...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              548   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           586   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  637   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             523   \n",
       "\n",
       "                                                                              168  \\\n",
       "mean_fit_time                                                            0.015137   \n",
       "std_fit_time                                                             0.000049   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                              (2, 2)   \n",
       "param_clf__solver                                                           lbfgs   \n",
       "param_over__k_neighbors                                                         1   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__hidden_layer_sizes': (2, 2), 'clf__solv...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              529   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           587   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  638   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             505   \n",
       "\n",
       "                                                                              465  \\\n",
       "mean_fit_time                                                            0.015664   \n",
       "std_fit_time                                                             0.000052   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         4   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              577   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           588   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  639   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             598   \n",
       "\n",
       "                                                                              466  \\\n",
       "mean_fit_time                                                            0.015746   \n",
       "std_fit_time                                                             0.000175   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         4   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              570   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           524   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  640   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             599   \n",
       "\n",
       "                                                                              160  \\\n",
       "mean_fit_time                                                            0.015762   \n",
       "std_fit_time                                                             0.000665   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                                (2,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         6   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__hidden_layer_sizes': (2,), 'clf__solver...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              536   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           598   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  641   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             527   \n",
       "\n",
       "                                                                              295  \\\n",
       "mean_fit_time                                                            0.019749   \n",
       "std_fit_time                                                             0.004465   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                              (2, 2)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         3   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__hidden_layer_sizes': (2, 2), 'clf__solv...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              524   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           546   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  642   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             590   \n",
       "\n",
       "                                                                              474  \\\n",
       "mean_fit_time                                                            0.018816   \n",
       "std_fit_time                                                             0.003997   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         5   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              576   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           593   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  643   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             587   \n",
       "\n",
       "                                                                              475  \\\n",
       "mean_fit_time                                                            0.015867   \n",
       "std_fit_time                                                             0.000507   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         5   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              578   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           594   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  644   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             586   \n",
       "\n",
       "                                                                              151  \\\n",
       "mean_fit_time                                                            0.015411   \n",
       "std_fit_time                                                             0.000102   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                                (2,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         5   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__hidden_layer_sizes': (2,), 'clf__solver...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              604   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           591   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  645   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             532   \n",
       "\n",
       "                                                                              150  \\\n",
       "mean_fit_time                                                            0.015846   \n",
       "std_fit_time                                                             0.000286   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                                (2,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         5   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__hidden_layer_sizes': (2,), 'clf__solver...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              601   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           590   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  646   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             559   \n",
       "\n",
       "                                                                              457  \\\n",
       "mean_fit_time                                                            0.016008   \n",
       "std_fit_time                                                             0.000276   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         3   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              603   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           592   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  647   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             592   \n",
       "\n",
       "                                                                              159  \n",
       "mean_fit_time                                                            0.015395  \n",
       "std_fit_time                                                             0.000295  \n",
       "mean_score_time                                                               0.0  \n",
       "std_score_time                                                                0.0  \n",
       "param_clf__hidden_layer_sizes                                                (2,)  \n",
       "param_clf__solver                                                            adam  \n",
       "param_over__k_neighbors                                                         6  \n",
       "param_over__sampling_strategy                                                 0.8  \n",
       "param_under__sampling_strategy                                                0.6  \n",
       "params                          {'clf__hidden_layer_sizes': (2,), 'clf__solver...  \n",
       "split0_test_recall                                                            NaN  \n",
       "split1_test_recall                                                            NaN  \n",
       "split2_test_recall                                                            NaN  \n",
       "mean_test_recall                                                              NaN  \n",
       "std_test_recall                                                               NaN  \n",
       "rank_test_recall                                                              565  \n",
       "split0_test_precision                                                         NaN  \n",
       "split1_test_precision                                                         NaN  \n",
       "split2_test_precision                                                         NaN  \n",
       "mean_test_precision                                                           NaN  \n",
       "std_test_precision                                                            NaN  \n",
       "rank_test_precision                                                           595  \n",
       "split0_test_f1                                                                NaN  \n",
       "split1_test_f1                                                                NaN  \n",
       "split2_test_f1                                                                NaN  \n",
       "mean_test_f1                                                                  NaN  \n",
       "std_test_f1                                                                   NaN  \n",
       "rank_test_f1                                                                  648  \n",
       "split0_test_roc_auc                                                           NaN  \n",
       "split1_test_roc_auc                                                           NaN  \n",
       "split2_test_roc_auc                                                           NaN  \n",
       "mean_test_roc_auc                                                             NaN  \n",
       "std_test_roc_auc                                                              NaN  \n",
       "rank_test_roc_auc                                                             537  \n",
       "\n",
       "[34 rows x 648 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MLP with grid search for parameters, testing on 5-fold CV with shuffling\n",
    "\n",
    "pipeline = IMBLpipeline([\n",
    "    ('over', SMOTE()),\n",
    "    ('under', RandomUnderSampler()),\n",
    "    ('clf', MLPClassifier(solver='lbfgs', max_iter=5000))\n",
    "])\n",
    "\n",
    "parameters = {'over__k_neighbors': range(1,7),\n",
    "              'over__sampling_strategy': [0.5, 0.6, 0.8], # probaj poveča ovresampling do 0.9\n",
    "              'under__sampling_strategy': [0.6, 0.7, 0.8],\n",
    "              'clf__hidden_layer_sizes': [(2, ), (2, 2), (3,), (3,3)],\n",
    "              'clf__solver': ['lbfgs', 'sgd', 'adam']\n",
    "             }\n",
    "nfolds = 3\n",
    "scores = ['recall', \"precision\", 'f1', 'roc_auc']\n",
    "gscv_NN = GridSearchCV(pipeline, \n",
    "                    parameters, \n",
    "                    scoring=scores,\n",
    "                    cv=StratifiedKFold(n_splits=nfolds, shuffle=True),\n",
    "                    n_jobs= -1, \n",
    "                    return_train_score=False, \n",
    "                    verbose=1, \n",
    "                    refit= \"f1\")\n",
    "resultsGSCV = gscv_NN.fit(Xsc, y)\n",
    "\n",
    "# Get the classifier name from the pipeline\n",
    "classifier_name = resultsGSCV.best_estimator_.named_steps['clf'].__class__.__name__\n",
    "    \n",
    "# save the best estimator\n",
    "save_best_estimator(gscv_NN, classifier_name, notebook_name)\n",
    "\n",
    "results = pd.DataFrame(resultsGSCV.cv_results_)\n",
    "display(results.sort_values(by=[f'rank_test_f1']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "017735d1-2fe5-4b37-b40a-a45fb91b8be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.91      0.88      0.90       147\n",
      "         poz       0.44      0.54      0.48        26\n",
      "\n",
      "    accuracy                           0.83       173\n",
      "   macro avg       0.68      0.71      0.69       173\n",
      "weighted avg       0.84      0.83      0.83       173\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on test data\n",
    "nn_clf = gscv_NN.best_estimator_.steps[2][1]\n",
    "\n",
    "# # Load the best estimator from the saved pickle file (replace with acctual file name)\n",
    "# pickle_file_name = \"models/timestamp/classifier_name-notebook_name.pkl\"\n",
    "# nn_clf = load_best_estimator(pickle_file_name).steps[2][1]\n",
    "\n",
    "# Evaluation of NN on test set\n",
    "X_eval_ohe = pd.get_dummies(X_eval, columns=[\"month\"])\n",
    "scaler = StandardScaler().fit(Xohe)\n",
    "X_eval_sc = scaler.transform(X_eval_ohe)\n",
    "X_eval_sc = pd.DataFrame(X_eval_sc, columns=Xohe.columns)\n",
    "y_pred = nn_clf.predict(X_eval_sc)\n",
    "NN_classification_report = classification_report(y_eval, y_pred)\n",
    "\n",
    "# Create classification report as dictionary\n",
    "NN_report_dict = classification_report(y_eval, y_pred, output_dict=True)\n",
    "\n",
    "print(classification_report(y_eval, y_pred, target_names=[\"neg\", \"poz\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ba333f-3931-4bf7-ae33-757478d69a30",
   "metadata": {},
   "source": [
    "#### Feature Importance (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67f68acd-b343-48c6-a372-d46ed52e6951",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Permutation Importance MLP')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feature importance of model (MLP)  (no cross-validation!)\n",
    "\n",
    "fig, (ax2) = plt.subplots(1, 1, figsize=(10,9))\n",
    "plt.subplots_adjust(wspace=3)\n",
    "\n",
    "# Get feature importance with Permutation Based Feature Importance (randomly shuffles each feature and compute the \n",
    "# change in the model’s performance. The features which impact the performance the most are the most important one).\n",
    "perm_importance = permutation_importance(nn_clf, Xsc, y)\n",
    "perm_sorted_idx = perm_importance.importances_mean.argsort()\n",
    "x2 = Xsc.columns[perm_sorted_idx]\n",
    "y2 = perm_importance.importances_mean[perm_sorted_idx]\n",
    "ax2.barh(x2, y2)\n",
    "ax2.set_title(\"Permutation Importance MLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09700f24-b683-41ee-85b1-1c957b12325a",
   "metadata": {},
   "source": [
    "#### Feature importance with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ec901eb-8260-445e-8b66-919df83ffd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Xsc.copy()\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, stratify=y, test_size=0.25)\n",
    "\n",
    "MLP = MLPClassifier(hidden_layer_sizes=(3,3), solver='lbfgs', max_iter=5000)\n",
    "NN_model = MLP.fit(X.to_numpy(), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902cad31-dc72-4a66-b376-2ca9ea8e0e80",
   "metadata": {},
   "source": [
    "First, visualize the impact of all features on both classes in one chart. We are using KernelExplainer but simpler general Explainer should be also tested once the SHAP code fixes all bugs.\n",
    "\n",
    "**Note: SHAP explanations change between runs because of sampling and probably other random factors!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "241febf0-4e3c-40b1-b0ff-a0d4d4d99787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # explain the model's predictions using SHAP\n",
    "# import shap\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# shap.initjs()\n",
    "\n",
    "# explainer = shap.KernelExplainer(model.predict_proba, shap.sample(X_eval_sc,20))\n",
    "# shap_values = explainer.shap_values(X_eval_sc, nsamples=50)\n",
    "# shap.summary_plot(shap_values, X_eval_sc, max_display=len(X.columns), class_names=le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9123988d-5f60-4676-8f57-cccbbff97cbe",
   "metadata": {},
   "source": [
    "Now for each class separately. We observe the impact of features on the returned model's probability for a given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "385ff88c-0dce-4fb5-a974-fec0a9b91d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f9a3f81fc648dca3cc381f28d2a7bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=5.990e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=5.724e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=3.123e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=2.190e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=7.441e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=5.990e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=5.724e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=3.123e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=2.190e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=7.441e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.293e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.434e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.358e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.428e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.403e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=6.799e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.293e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.434e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.358e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.428e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.403e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=6.799e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.106e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=5.529e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=4.274e-07, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=5.990e-07, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=2.916e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.628e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 36 iterations, alpha=2.259e-05, previous alpha=1.803e-05, with an active set of 21 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.490e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.675e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=8.050e-07, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.490e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.675e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=8.050e-07, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.037e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.627e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 22 iterations, alpha=1.844e-04, previous alpha=7.957e-05, with an active set of 19 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.730e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.045e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.251e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.173e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.812e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=7.610e-05, with an active set of 16 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 20 iterations, alpha=6.757e-05, previous alpha=6.754e-05, with an active set of 17 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.173e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.812e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=7.610e-05, with an active set of 16 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 20 iterations, alpha=6.757e-05, previous alpha=6.754e-05, with an active set of 17 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=4.336e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=3.422e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.108e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 28 iterations, alpha=2.472e-05, previous alpha=1.072e-05, with an active set of 23 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.592e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.920e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.686e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 18 iterations, alpha=2.194e-04, previous alpha=1.657e-04, with an active set of 17 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.592e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.920e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.686e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 18 iterations, alpha=2.194e-04, previous alpha=1.657e-04, with an active set of 17 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.689e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.214e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=6.039e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.777e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.890e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.887e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.662e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.662e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 29 iterations, alpha=1.657e-04, previous alpha=1.131e-04, with an active set of 20 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.291e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.461e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.303e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 20 iterations, alpha=2.023e-04, previous alpha=1.771e-04, with an active set of 17 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=7.363e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.284e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.177e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 28 iterations, alpha=1.153e-05, previous alpha=7.395e-06, with an active set of 23 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=7.363e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.284e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.177e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 28 iterations, alpha=1.153e-05, previous alpha=7.395e-06, with an active set of 23 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.413e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.629e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.496e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.413e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.629e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.496e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.216e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.388e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.269e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 18 iterations, alpha=9.583e-05, previous alpha=9.519e-05, with an active set of 15 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=7.137e-05, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=6.942e-05, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=6.252e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 27 iterations, alpha=4.847e-05, previous alpha=3.708e-05, with an active set of 20 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.401e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.267e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=9.604e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.813e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.401e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.267e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=9.604e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.813e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=9.094e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 39 iterations, alpha=8.586e-06, previous alpha=8.179e-06, with an active set of 24 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=9.094e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 39 iterations, alpha=8.586e-06, previous alpha=8.179e-06, with an active set of 24 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.381e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.664e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 29 iterations, alpha=6.580e-05, previous alpha=3.618e-05, with an active set of 22 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.381e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.664e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 29 iterations, alpha=6.580e-05, previous alpha=3.618e-05, with an active set of 22 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.338e-05, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.490e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.280e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 29 iterations, alpha=1.727e-05, previous alpha=1.019e-05, with an active set of 20 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=5.181e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.674e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.977e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 20 iterations, alpha=1.641e-04, previous alpha=1.380e-04, with an active set of 19 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=5.181e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.674e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.977e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 20 iterations, alpha=1.641e-04, previous alpha=1.380e-04, with an active set of 19 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=9.401e-05, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.809e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.657e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 38 iterations, alpha=1.489e-05, previous alpha=1.140e-05, with an active set of 21 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=9.401e-05, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.809e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.657e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 38 iterations, alpha=1.489e-05, previous alpha=1.140e-05, with an active set of 21 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.426e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=3.839e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.309e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.487e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.375e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 17 iterations, alpha=1.587e-04, previous alpha=1.372e-04, with an active set of 16 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.426e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=3.839e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.309e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.487e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.375e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 17 iterations, alpha=1.587e-04, previous alpha=1.372e-04, with an active set of 16 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.486e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.198e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=7.408e-05, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=6.997e-05, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=3.704e-05, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 27 iterations, alpha=3.334e-05, previous alpha=3.177e-05, with an active set of 18 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.807e-05, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.251e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=5.545e-07, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.154e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=5.144e-05, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=4.467e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=3.077e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.994e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=5.210e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.657e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.918e-07, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=3.599e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=9.472e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=6.061e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=3.599e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=9.472e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=6.061e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.614e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 21 iterations, alpha=1.069e-04, previous alpha=8.719e-05, with an active set of 18 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.530e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 20 iterations, alpha=1.098e-04, previous alpha=8.462e-05, with an active set of 17 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=3.579e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=2.038e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.666e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 24 iterations, alpha=1.673e-05, previous alpha=1.463e-05, with an active set of 21 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=3.579e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=2.038e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.666e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 24 iterations, alpha=1.673e-05, previous alpha=1.463e-05, with an active set of 21 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.111e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=4.205e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.436e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 31 iterations, alpha=3.801e-05, previous alpha=2.951e-05, with an active set of 24 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.213e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.090e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.687e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.823e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 15 iterations, alpha=2.957e-04, previous alpha=2.304e-04, with an active set of 14 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=8.643e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.883e-06, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.121e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=4.064e-07, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.883e-06, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.121e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=4.064e-07, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x914.4 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "explainer = shap.KernelExplainer(NN_model.predict_proba, shap.sample(Xsc,50)) #morda treba zmanjšat število, ali brez sample in samo X_eval\n",
    "shap_values = explainer.shap_values(Xsc, nsamples=50)\n",
    "classid = 1\n",
    "shap.summary_plot(shap_values[classid], Xsc, max_display=len(X.columns), class_names=le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3cac8790-80dc-44b5-9e13-18d2050b196e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f84e62ecc24d7b8ba9224186b4b283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=8.870e-05, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=5.115e-05, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 29 iterations, alpha=2.100e-05, previous alpha=1.457e-05, with an active set of 22 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.695e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=6.341e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 21 iterations, alpha=1.079e-04, previous alpha=2.981e-05, with an active set of 20 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.695e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=6.341e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 21 iterations, alpha=1.079e-04, previous alpha=2.981e-05, with an active set of 20 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.242e-05, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=4.462e-06, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.419e-06, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.365e-06, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=9.274e-07, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.284e-05, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=3.682e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.866e-05, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.449e-05, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=5.978e-06, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.595e-06, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.537e-06, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.252e-06, with an active set of 21 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=7.483e-07, with an active set of 21 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.469e-07, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.331e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=7.633e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=6.575e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=2.069e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 26 iterations, alpha=3.503e-05, previous alpha=9.337e-06, with an active set of 23 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.331e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=7.633e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=6.575e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=2.069e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 26 iterations, alpha=3.503e-05, previous alpha=9.337e-06, with an active set of 23 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=5.013e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.161e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.065e-06, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.271e-07, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=5.962e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=4.995e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 26 iterations, alpha=4.960e-05, previous alpha=2.792e-05, with an active set of 21 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.682e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.881e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=3.384e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=6.003e-07, with an active set of 23 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=4.441e-07, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.245e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.938e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=9.665e-05, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=5.711e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 29 iterations, alpha=5.838e-05, previous alpha=5.483e-05, with an active set of 20 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.938e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=9.665e-05, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=5.711e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 29 iterations, alpha=5.838e-05, previous alpha=5.483e-05, with an active set of 20 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=5.854e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.566e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 26 iterations, alpha=3.479e-05, previous alpha=2.267e-05, with an active set of 21 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=5.854e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.566e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 26 iterations, alpha=3.479e-05, previous alpha=2.267e-05, with an active set of 21 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=4.483e-05, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=4.085e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.229e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=7.148e-06, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.545e-06, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=8.560e-07, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.411e-07, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=4.431e-05, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=3.487e-07, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.062e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.383e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.062e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.383e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.192e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.392e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.382e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=9.469e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.071e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.605e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.192e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.392e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.382e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=9.469e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.071e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.605e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.650e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=5.266e-07, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=5.402e-05, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=5.042e-07, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 540x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Try dependence contribution plot\n",
    "explainer = shap.KernelExplainer(NN_model.predict_proba, shap.sample(X_eval_sc,50))\n",
    "shap_values = explainer.shap_values(X_eval_sc, nsamples=50)\n",
    "shap.dependence_plot('salinity', shap_values[1], X_eval_sc,) #interaction_index=\"salinity\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38930dca-d69a-4393-a88b-6fc019d83c57",
   "metadata": {},
   "source": [
    "Example intepretation: The fact this slopes upward says the higher the soca flow, the higher the model's prediction is for poz/neg. The spread suggests that other features must interact with Soca flow. \n",
    "In general, high Soca flow increases the chance of poz/neg. But if the sea temp is moderate or low, that trend reverses and even high soca flow does not increase preditions of poz/neg as the sea temp is too low.\n",
    "https://www.kaggle.com/code/dansbecker/advanced-uses-of-shap-values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e17d179-7316-4026-b266-1bc7ef4662b9",
   "metadata": {},
   "source": [
    "Now let's explain the prediction of a single instance. We will show the explanation of the bigger predicted probability to see why the model decided as it did. But in practice we could be interested only in the explanation of the probability of the positive prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d07af4e-262b-45e6-beab-41bdf773fb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real value: 1, \n",
      "predicted: 1, \n",
      "predicted probs: [0.10649498 0.89350502]\n",
      "Explanation for prediction: class=1, p=0.8935050170550933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X has feature names, but MLPClassifier was fitted without feature names\n",
      "X has feature names, but MLPClassifier was fitted without feature names\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a18e8cd1acd40a8bc6dbb82cd09c80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div id='iANWD4LNBWYIHSCFWY6DB'>\n",
       "<div style='color: #900; text-align: center;'>\n",
       "  <b>Visualization omitted, Javascript library not loaded!</b><br>\n",
       "  Have you run `initjs()` in this notebook? If this notebook was from another\n",
       "  user you must also trust this notebook (File -> Trust notebook). If you are viewing\n",
       "  this notebook on github the Javascript has been stripped for security. If you are using\n",
       "  JupyterLab this error is because a JupyterLab extension has not yet been written.\n",
       "</div></div>\n",
       " <script>\n",
       "   if (window.SHAP) SHAP.ReactDom.render(\n",
       "    SHAP.React.createElement(SHAP.AdditiveForceVisualizer, {\"outNames\": [\"f(x)\"], \"baseValue\": 0.24437436204224738, \"outValue\": 0.8935050170550932, \"link\": \"identity\", \"featureNames\": [\"DSP\", \"Dinophysis caudata\", \"Dinophysis fortii\", \"Phalacroma rotundatum\", \"Dinophysis sacculus\", \"Dinophysis tripos\", \"sun [h]\", \"air temp\", \"wind strength\", \"precipitation\", \"Chl-a\", \"salinity\", \"T\", \"DIN\", \"PO4-P\", \"Soca\", \"month_1\", \"month_2\", \"month_3\", \"month_4\", \"month_5\", \"month_6\", \"month_7\", \"month_8\", \"month_9\", \"month_10\", \"month_11\", \"month_12\"], \"features\": {\"0\": {\"effect\": -0.025809809980838094, \"value\": 50.0}, \"1\": {\"effect\": 0.026247471429035288, \"value\": 0.0}, \"5\": {\"effect\": 0.08041074987218218, \"value\": 30.0}, \"6\": {\"effect\": 0.2256073894276464, \"value\": 42.99999999999999}, \"7\": {\"effect\": 0.09768654564726859, \"value\": 6.8428571428571425}, \"8\": {\"effect\": 0.04835459506610054, \"value\": 2.2333333333333334}, \"9\": {\"effect\": 0.021912056823144566, \"value\": 158.79999999999998}, \"10\": {\"effect\": 0.062114355876266625, \"value\": 4.24}, \"11\": {\"effect\": 0.17070008473350615, \"value\": 31.89}, \"12\": {\"effect\": -0.026781735507380208, \"value\": 14.55}, \"13\": {\"effect\": 0.16441932836467366, \"value\": 10.379999905824656}, \"14\": {\"effect\": -0.027306490501771537, \"value\": 0.00999999977648258}, \"16\": {\"effect\": 0.020831413693590303, \"value\": 0.0}, \"19\": {\"effect\": 0.04659215183850299, \"value\": 0.0}, \"20\": {\"effect\": -0.07651223262487192, \"value\": 0.0}, \"23\": {\"effect\": -0.03849581668131168, \"value\": 0.0}, \"25\": {\"effect\": 0.05317160604453508, \"value\": 0.0}, \"26\": {\"effect\": 0.029104972291253528, \"value\": 0.0}, \"27\": {\"effect\": -0.20311598079868654, \"value\": 1.0}}, \"plot_cmap\": \"RdBu\", \"labelMargin\": 20}),\n",
       "    document.getElementById('iANWD4LNBWYIHSCFWY6DB')\n",
       "  );\n",
       "</script>"
      ],
      "text/plain": [
       "<shap.plots._force.AdditiveForceVisualizer at 0x1467602b0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instanceID = 10\n",
    "instance = X.iloc[[instanceID]]\n",
    "display_instance = X_display.iloc[[instanceID]]\n",
    "\n",
    "prediction = NN_model.predict(instance)[0]\n",
    "prediction_probs = NN_model.predict_proba(instance)[0]\n",
    "print(f'real value: {y[instanceID]}, \\npredicted: {prediction}, \\npredicted probs: {prediction_probs}')\n",
    "max_p_id = prediction_probs.argmax()  # we will show the explanation of the bigger predicted probability\n",
    "print(f'Explanation for prediction: class={max_p_id}, p={prediction_probs.max()}')\n",
    "\n",
    "explainer = shap.KernelExplainer(NN_model.predict_proba, shap.sample(X, 50))\n",
    "shap_values = explainer.shap_values(instance, nsamples=500)\n",
    "shap.force_plot(explainer.expected_value[max_p_id], shap_values[max_p_id], features=display_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4908630-eea9-4167-83d9-5511dd820733",
   "metadata": {},
   "source": [
    "Show the mean values of features as it may help understanding this particular instance data in the plot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "642977c5-c5e3-4c96-b81f-c9419b084dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DSP</th>\n",
       "      <td>102.700503</td>\n",
       "      <td>195.860294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dinophysis caudata</th>\n",
       "      <td>25.537688</td>\n",
       "      <td>40.543382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dinophysis fortii</th>\n",
       "      <td>23.725829</td>\n",
       "      <td>72.827941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phalacroma rotundatum</th>\n",
       "      <td>16.438492</td>\n",
       "      <td>13.510294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dinophysis sacculus</th>\n",
       "      <td>24.474070</td>\n",
       "      <td>52.823529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dinophysis tripos</th>\n",
       "      <td>4.902513</td>\n",
       "      <td>8.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sun [h]</th>\n",
       "      <td>165.661647</td>\n",
       "      <td>152.191912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>air temp</th>\n",
       "      <td>17.422122</td>\n",
       "      <td>16.944328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wind strength</th>\n",
       "      <td>2.997017</td>\n",
       "      <td>2.921674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precipitation</th>\n",
       "      <td>56.557731</td>\n",
       "      <td>75.321324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chl-a</th>\n",
       "      <td>0.791451</td>\n",
       "      <td>0.965165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salinity</th>\n",
       "      <td>37.476320</td>\n",
       "      <td>35.692370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T</th>\n",
       "      <td>20.112537</td>\n",
       "      <td>20.241136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIN</th>\n",
       "      <td>2.921092</td>\n",
       "      <td>3.868770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PO4-P</th>\n",
       "      <td>0.058539</td>\n",
       "      <td>0.076768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soca</th>\n",
       "      <td>3208.868147</td>\n",
       "      <td>3703.893351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_1</th>\n",
       "      <td>0.015060</td>\n",
       "      <td>0.022059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_2</th>\n",
       "      <td>0.021084</td>\n",
       "      <td>0.022059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_3</th>\n",
       "      <td>0.028112</td>\n",
       "      <td>0.014706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_4</th>\n",
       "      <td>0.066265</td>\n",
       "      <td>0.014706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_5</th>\n",
       "      <td>0.089357</td>\n",
       "      <td>0.102941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_6</th>\n",
       "      <td>0.115462</td>\n",
       "      <td>0.088235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_7</th>\n",
       "      <td>0.125502</td>\n",
       "      <td>0.132353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_8</th>\n",
       "      <td>0.126506</td>\n",
       "      <td>0.080882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_9</th>\n",
       "      <td>0.143574</td>\n",
       "      <td>0.227941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_10</th>\n",
       "      <td>0.138554</td>\n",
       "      <td>0.176471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_11</th>\n",
       "      <td>0.091365</td>\n",
       "      <td>0.095588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_12</th>\n",
       "      <td>0.039157</td>\n",
       "      <td>0.022059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               neg          pos\n",
       "DSP                     102.700503   195.860294\n",
       "Dinophysis caudata       25.537688    40.543382\n",
       "Dinophysis fortii        23.725829    72.827941\n",
       "Phalacroma rotundatum    16.438492    13.510294\n",
       "Dinophysis sacculus      24.474070    52.823529\n",
       "Dinophysis tripos         4.902513     8.235294\n",
       "sun [h]                 165.661647   152.191912\n",
       "air temp                 17.422122    16.944328\n",
       "wind strength             2.997017     2.921674\n",
       "precipitation            56.557731    75.321324\n",
       "Chl-a                     0.791451     0.965165\n",
       "salinity                 37.476320    35.692370\n",
       "T                        20.112537    20.241136\n",
       "DIN                       2.921092     3.868770\n",
       "PO4-P                     0.058539     0.076768\n",
       "Soca                   3208.868147  3703.893351\n",
       "month_1                   0.015060     0.022059\n",
       "month_2                   0.021084     0.022059\n",
       "month_3                   0.028112     0.014706\n",
       "month_4                   0.066265     0.014706\n",
       "month_5                   0.089357     0.102941\n",
       "month_6                   0.115462     0.088235\n",
       "month_7                   0.125502     0.132353\n",
       "month_8                   0.126506     0.080882\n",
       "month_9                   0.143574     0.227941\n",
       "month_10                  0.138554     0.176471\n",
       "month_11                  0.091365     0.095588\n",
       "month_12                  0.039157     0.022059"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.get_dummies(data, columns=[\"month\"])\n",
    "\n",
    "\n",
    "pd.DataFrame([data[data['lipophylic_toxins']=='neg'].mean(), data[data['lipophylic_toxins']=='poz'].mean()], index=['neg','pos']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9f3aca-b6f0-4ab4-bd20-b828301bfdb2",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8273fc8d-8b18-4f8b-8c19-448fb5843226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Class</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.871951</td>\n",
       "      <td>0.972789</td>\n",
       "      <td>0.919614</td>\n",
       "      <td>147.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>poz</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Support Vector Mashines</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.697095</td>\n",
       "      <td>147.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Support Vector Mashines</td>\n",
       "      <td>poz</td>\n",
       "      <td>0.202532</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.304762</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.895161</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>0.819188</td>\n",
       "      <td>147.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>poz</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.346667</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.940741</td>\n",
       "      <td>0.863946</td>\n",
       "      <td>0.900709</td>\n",
       "      <td>147.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>poz</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.914894</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>147.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>poz</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Classifier Class  Precision    Recall  F1-score  Support\n",
       "0      Logistic Regression   neg   0.871951  0.972789  0.919614    147.0\n",
       "1      Logistic Regression   poz   0.555556  0.192308  0.285714     26.0\n",
       "2  Support Vector Mashines   neg   0.893617  0.571429  0.697095    147.0\n",
       "3  Support Vector Mashines   poz   0.202532  0.615385  0.304762     26.0\n",
       "4            Decision Tree   neg   0.895161  0.755102  0.819188    147.0\n",
       "5            Decision Tree   poz   0.265306  0.500000  0.346667     26.0\n",
       "6            Random Forest   neg   0.940741  0.863946  0.900709    147.0\n",
       "7            Random Forest   poz   0.473684  0.692308  0.562500     26.0\n",
       "8           Neural Network   neg   0.914894  0.877551  0.895833    147.0\n",
       "9           Neural Network   poz   0.437500  0.538462  0.482759     26.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to extract the metrics from the classification report dictionary\n",
    "def extract_metrics(report_dict):\n",
    "    metrics = {}\n",
    "    for class_label in report_dict:\n",
    "        if class_label in ('accuracy', 'macro avg', 'weighted avg'):\n",
    "            continue\n",
    "        metrics[class_label] = {\n",
    "            'precision': report_dict[class_label]['precision'],\n",
    "            'recall': report_dict[class_label]['recall'],\n",
    "            'f1-score': report_dict[class_label]['f1-score'],\n",
    "            'support': report_dict[class_label]['support']\n",
    "        }\n",
    "    return metrics\n",
    "\n",
    "# Extract the metrics for each classifier\n",
    "lr_metrics = extract_metrics(lr_report_dict)\n",
    "SVM_metrics = extract_metrics(SVM_report_dict)\n",
    "DT_metrics = extract_metrics(DT_report_dict)\n",
    "RF_metrics = extract_metrics(RF_report_dict)\n",
    "NN_metrics = extract_metrics(NN_report_dict)\n",
    "\n",
    "# Create a dictionary to store the metrics for each classifier\n",
    "classifier_metrics = {\n",
    "    'Logistic Regression': lr_metrics,\n",
    "    'Support Vector Mashines': SVM_metrics,\n",
    "    'Decision Tree': DT_metrics,\n",
    "    'Random Forest': RF_metrics,\n",
    "    'Neural Network': NN_metrics\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a pandas DataFrame\n",
    "summary_df = pd.concat({k: pd.DataFrame(v).transpose() for k, v in classifier_metrics.items()}, axis=0)\n",
    "summary_df.reset_index(inplace=True)\n",
    "summary_df.columns = ['Classifier', 'Class', 'Precision', 'Recall', 'F1-score', 'Support']\n",
    "\n",
    "# Map the original class labels to the new names\n",
    "class_name_mapping = {\n",
    "    '0': 'neg',\n",
    "    '1': 'poz'\n",
    "}\n",
    "summary_df['Class'] = summary_df['Class'].map(class_name_mapping)\n",
    "\n",
    "# Display the summary DataFrame\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1bb76f-cb9e-42b5-941e-72a9176536d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
