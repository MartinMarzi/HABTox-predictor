{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57d854a4-aa15-40da-bd45-4df04e0043ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from dateutil.parser import parse\n",
    "import datetime\n",
    "from dateutil.parser import parse\n",
    "import math\n",
    "from numpy import mean\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold, GridSearchCV, cross_validate, StratifiedKFold\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline as SKLpipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.tree import export_text\n",
    "from dtreeviz.trees import dtreeviz \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as IMBLpipeline\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import ipynbname\n",
    "notebook_name = ipynbname.name()\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 30)\n",
    "pd.set_option(\"display.max_columns\", 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d530319-a99e-4824-982c-5d531a8ac548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DSP                        1\n",
       "Dinophysis caudata         1\n",
       "Dinophysis fortii          1\n",
       "Phalacroma rotundatum      1\n",
       "Dinophysis sacculus        1\n",
       "Dinophysis tripos          1\n",
       "sun [h]                    0\n",
       "air temp                   0\n",
       "wind strength              0\n",
       "precipitation              0\n",
       "Chl-a                    422\n",
       "salinity                  21\n",
       "T                         59\n",
       "SECCHI                   450\n",
       "DIN                      352\n",
       "PO4-P                    349\n",
       "Soca                       0\n",
       "month                      0\n",
       "lipophylic_toxins        320\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read df pickle\n",
    "df_alg = pd.read_pickle(\"objects/df_alg-HAB_preprocessing_5_1\")\n",
    "# data = pd.read_pickle(\"data/preprocessed/hab_org-data-HAB_part2-preprocessing-5_2\")\n",
    "data = pd.read_pickle(\"data/preprocessed/hab_interp_data-HAB_part2-preprocessing-5_2\")\n",
    "\n",
    "data.drop(columns=[\"sampling station\", \"date\"], inplace=True)\n",
    "# data.set_index('date', inplace=True)\n",
    "\n",
    "\n",
    "# slice by station and time\n",
    "# data = data[data[\"sampling station\"] == \"Debeli_rtic\"].loc[\"2008-01-01\" : \"2021-12-31\"]\n",
    "\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4ce4a24-a8cc-4258-9d9b-3b925dc86a52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neg    996\n",
       "NaN    320\n",
       "poz    136\n",
       "Name: lipophylic_toxins, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class distribution\n",
    "data[\"lipophylic_toxins\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9945a3bf-fecf-4702-863e-7d4f489e41a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move month to first place\n",
    "cols = data.columns.tolist()  # Get a list of column names\n",
    "cols = [cols[-2]] + cols[:-2] + [cols[-1]]  # Move the one before the last column to the first position\n",
    "data = data[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e54e5716-91c5-48f2-b201-73f7444bd46c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "month                      0\n",
       "DSP                        1\n",
       "Dinophysis caudata         1\n",
       "Dinophysis fortii          1\n",
       "Phalacroma rotundatum      1\n",
       "Dinophysis sacculus        1\n",
       "Dinophysis tripos          1\n",
       "sun [h]                    0\n",
       "air temp                   0\n",
       "wind strength              0\n",
       "precipitation              0\n",
       "Chl-a                    422\n",
       "salinity                  21\n",
       "T                         59\n",
       "DIN                      352\n",
       "PO4-P                    349\n",
       "Soca                       0\n",
       "lipophylic_toxins        320\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data.drop(columns=[\"Chl-a\",\"PO4-P\",\"DIN\",\"SECCHI\"], inplace=True)\n",
    "data.drop(columns=[\"SECCHI\",  ], inplace=True)#,\"Chl-a\", \"PO4-P\", \"DIN\",\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45d93c2-6bcb-4298-bca9-f268e6330756",
   "metadata": {},
   "source": [
    "# Descriptive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73b54946-f380-4fd8-9d45-1ea70fd7965f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>DSP</th>\n",
       "      <th>Dinophysis caudata</th>\n",
       "      <th>Dinophysis fortii</th>\n",
       "      <th>Phalacroma rotundatum</th>\n",
       "      <th>Dinophysis sacculus</th>\n",
       "      <th>Dinophysis tripos</th>\n",
       "      <th>sun [h]</th>\n",
       "      <th>air temp</th>\n",
       "      <th>wind strength</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>Chl-a</th>\n",
       "      <th>salinity</th>\n",
       "      <th>T</th>\n",
       "      <th>DIN</th>\n",
       "      <th>PO4-P</th>\n",
       "      <th>Soca</th>\n",
       "      <th>lipophylic_toxins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1452.00</td>\n",
       "      <td>1451.00</td>\n",
       "      <td>1451.00</td>\n",
       "      <td>1451.00</td>\n",
       "      <td>1451.00</td>\n",
       "      <td>1451.00</td>\n",
       "      <td>1451.00</td>\n",
       "      <td>1452.00</td>\n",
       "      <td>1452.00</td>\n",
       "      <td>1452.00</td>\n",
       "      <td>1452.00</td>\n",
       "      <td>1030.00</td>\n",
       "      <td>1431.00</td>\n",
       "      <td>1393.00</td>\n",
       "      <td>1100.00</td>\n",
       "      <td>1103.00</td>\n",
       "      <td>1452.00</td>\n",
       "      <td>1132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.38</td>\n",
       "      <td>105.75</td>\n",
       "      <td>24.92</td>\n",
       "      <td>26.15</td>\n",
       "      <td>14.20</td>\n",
       "      <td>28.55</td>\n",
       "      <td>4.50</td>\n",
       "      <td>158.80</td>\n",
       "      <td>16.60</td>\n",
       "      <td>2.99</td>\n",
       "      <td>57.58</td>\n",
       "      <td>0.85</td>\n",
       "      <td>37.08</td>\n",
       "      <td>19.46</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.06</td>\n",
       "      <td>3294.64</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.82</td>\n",
       "      <td>339.89</td>\n",
       "      <td>87.92</td>\n",
       "      <td>160.06</td>\n",
       "      <td>29.93</td>\n",
       "      <td>177.90</td>\n",
       "      <td>41.18</td>\n",
       "      <td>60.63</td>\n",
       "      <td>6.06</td>\n",
       "      <td>0.41</td>\n",
       "      <td>50.71</td>\n",
       "      <td>0.71</td>\n",
       "      <td>25.50</td>\n",
       "      <td>5.33</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2329.62</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.80</td>\n",
       "      <td>-0.82</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>24.13</td>\n",
       "      <td>6.24</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>593.92</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>112.60</td>\n",
       "      <td>12.18</td>\n",
       "      <td>2.74</td>\n",
       "      <td>19.68</td>\n",
       "      <td>0.39</td>\n",
       "      <td>35.85</td>\n",
       "      <td>15.50</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1636.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>161.40</td>\n",
       "      <td>17.44</td>\n",
       "      <td>2.98</td>\n",
       "      <td>44.70</td>\n",
       "      <td>0.68</td>\n",
       "      <td>36.89</td>\n",
       "      <td>20.47</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2580.82</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10.00</td>\n",
       "      <td>90.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>207.30</td>\n",
       "      <td>21.74</td>\n",
       "      <td>3.21</td>\n",
       "      <td>79.00</td>\n",
       "      <td>1.09</td>\n",
       "      <td>37.48</td>\n",
       "      <td>23.91</td>\n",
       "      <td>3.96</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4236.82</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12.00</td>\n",
       "      <td>7630.00</td>\n",
       "      <td>1309.00</td>\n",
       "      <td>4624.00</td>\n",
       "      <td>393.00</td>\n",
       "      <td>4639.00</td>\n",
       "      <td>1139.00</td>\n",
       "      <td>277.80</td>\n",
       "      <td>26.57</td>\n",
       "      <td>5.26</td>\n",
       "      <td>267.70</td>\n",
       "      <td>9.25</td>\n",
       "      <td>999.00</td>\n",
       "      <td>28.37</td>\n",
       "      <td>35.47</td>\n",
       "      <td>3.54</td>\n",
       "      <td>16039.87</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing_values</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>422.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>59.00</td>\n",
       "      <td>352.00</td>\n",
       "      <td>349.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  month      DSP  Dinophysis caudata  Dinophysis fortii  \\\n",
       "count           1452.00  1451.00             1451.00            1451.00   \n",
       "mean               7.38   105.75               24.92              26.15   \n",
       "std                2.82   339.89               87.92             160.06   \n",
       "min                1.00     0.00                0.00               0.00   \n",
       "25%                5.00    10.00                0.00               0.00   \n",
       "50%                8.00    37.00                0.00               0.00   \n",
       "75%               10.00    90.00               13.00              10.00   \n",
       "max               12.00  7630.00             1309.00            4624.00   \n",
       "missing_values     0.00     1.00                1.00               1.00   \n",
       "\n",
       "                Phalacroma rotundatum  Dinophysis sacculus  Dinophysis tripos  \\\n",
       "count                         1451.00              1451.00            1451.00   \n",
       "mean                            14.20                28.55               4.50   \n",
       "std                             29.93               177.90              41.18   \n",
       "min                              0.00                 0.00               0.00   \n",
       "25%                              0.00                 0.00               0.00   \n",
       "50%                             10.00                 0.00               0.00   \n",
       "75%                             20.00                10.00               0.00   \n",
       "max                            393.00              4639.00            1139.00   \n",
       "missing_values                   1.00                 1.00               1.00   \n",
       "\n",
       "                sun [h]  air temp  wind strength  precipitation    Chl-a  \\\n",
       "count           1452.00   1452.00        1452.00        1452.00  1030.00   \n",
       "mean             158.80     16.60           2.99          57.58     0.85   \n",
       "std               60.63      6.06           0.41          50.71     0.71   \n",
       "min               22.80     -0.82           1.48           0.00     0.09   \n",
       "25%              112.60     12.18           2.74          19.68     0.39   \n",
       "50%              161.40     17.44           2.98          44.70     0.68   \n",
       "75%              207.30     21.74           3.21          79.00     1.09   \n",
       "max              277.80     26.57           5.26         267.70     9.25   \n",
       "missing_values     0.00      0.00           0.00           0.00   422.00   \n",
       "\n",
       "                salinity        T      DIN    PO4-P      Soca  \\\n",
       "count            1431.00  1393.00  1100.00  1103.00   1452.00   \n",
       "mean               37.08    19.46     3.29     0.06   3294.64   \n",
       "std                25.50     5.33     3.94     0.16   2329.62   \n",
       "min                24.13     6.24     0.06     0.00    593.92   \n",
       "25%                35.85    15.50     0.87     0.02   1636.00   \n",
       "50%                36.89    20.47     1.98     0.04   2580.82   \n",
       "75%                37.48    23.91     3.96     0.08   4236.82   \n",
       "max               999.00    28.37    35.47     3.54  16039.87   \n",
       "missing_values     21.00    59.00   352.00   349.00      0.00   \n",
       "\n",
       "               lipophylic_toxins  \n",
       "count                       1132  \n",
       "mean                         NaN  \n",
       "std                          NaN  \n",
       "min                          NaN  \n",
       "25%                          NaN  \n",
       "50%                          NaN  \n",
       "75%                          NaN  \n",
       "max                          NaN  \n",
       "missing_values               320  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# describe\n",
    "description = data.describe(include='all').round(2)\n",
    "\n",
    "# Calculate the number of missing values for each column\n",
    "missing_values = data.isna().sum()\n",
    "missing_values.name = 'missing_values'\n",
    "\n",
    "# Append the missing_values row to the description DataFrame\n",
    "description_with_missing = description.append(missing_values)\n",
    "description_with_missing = description_with_missing.drop(['unique', 'top', 'freq'])\n",
    "\n",
    "description_with_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7501e318-06de-4e74-ac3e-04965a6ccd58",
   "metadata": {},
   "source": [
    "## Descriptive analysis by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3c39a42-f94a-476e-9e49-1962461bf7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DSP</th>\n",
       "      <th>Dinophysis caudata</th>\n",
       "      <th>Dinophysis fortii</th>\n",
       "      <th>Phalacroma rotundatum</th>\n",
       "      <th>Dinophysis sacculus</th>\n",
       "      <th>Dinophysis tripos</th>\n",
       "      <th>sun [h]</th>\n",
       "      <th>air temp</th>\n",
       "      <th>wind strength</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>Chl-a</th>\n",
       "      <th>salinity</th>\n",
       "      <th>T</th>\n",
       "      <th>DIN</th>\n",
       "      <th>PO4-P</th>\n",
       "      <th>Soca</th>\n",
       "      <th>neg</th>\n",
       "      <th>poz</th>\n",
       "      <th>poz %</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>January</th>\n",
       "      <td>9.00</td>\n",
       "      <td>1.16</td>\n",
       "      <td>2.96</td>\n",
       "      <td>3.66</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.20</td>\n",
       "      <td>72.62</td>\n",
       "      <td>4.95</td>\n",
       "      <td>2.71</td>\n",
       "      <td>46.53</td>\n",
       "      <td>0.76</td>\n",
       "      <td>36.95</td>\n",
       "      <td>11.07</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4652.16</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>February</th>\n",
       "      <td>12.86</td>\n",
       "      <td>1.02</td>\n",
       "      <td>4.08</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.69</td>\n",
       "      <td>80.19</td>\n",
       "      <td>5.52</td>\n",
       "      <td>3.06</td>\n",
       "      <td>60.66</td>\n",
       "      <td>0.77</td>\n",
       "      <td>37.02</td>\n",
       "      <td>10.22</td>\n",
       "      <td>6.66</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4378.79</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>March</th>\n",
       "      <td>12.38</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.68</td>\n",
       "      <td>3.79</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.64</td>\n",
       "      <td>118.56</td>\n",
       "      <td>7.55</td>\n",
       "      <td>3.30</td>\n",
       "      <td>30.01</td>\n",
       "      <td>0.77</td>\n",
       "      <td>37.17</td>\n",
       "      <td>9.49</td>\n",
       "      <td>4.37</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3257.07</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>April</th>\n",
       "      <td>15.49</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.48</td>\n",
       "      <td>5.24</td>\n",
       "      <td>4.51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>144.63</td>\n",
       "      <td>11.69</td>\n",
       "      <td>3.17</td>\n",
       "      <td>54.90</td>\n",
       "      <td>0.68</td>\n",
       "      <td>36.93</td>\n",
       "      <td>11.68</td>\n",
       "      <td>4.68</td>\n",
       "      <td>0.06</td>\n",
       "      <td>3826.10</td>\n",
       "      <td>66</td>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>May</th>\n",
       "      <td>59.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.97</td>\n",
       "      <td>9.67</td>\n",
       "      <td>34.25</td>\n",
       "      <td>0.15</td>\n",
       "      <td>171.96</td>\n",
       "      <td>16.22</td>\n",
       "      <td>2.89</td>\n",
       "      <td>48.55</td>\n",
       "      <td>0.96</td>\n",
       "      <td>36.57</td>\n",
       "      <td>14.87</td>\n",
       "      <td>3.92</td>\n",
       "      <td>0.07</td>\n",
       "      <td>3864.97</td>\n",
       "      <td>89</td>\n",
       "      <td>14</td>\n",
       "      <td>10.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>June</th>\n",
       "      <td>146.89</td>\n",
       "      <td>29.23</td>\n",
       "      <td>1.49</td>\n",
       "      <td>12.87</td>\n",
       "      <td>93.28</td>\n",
       "      <td>0.09</td>\n",
       "      <td>202.85</td>\n",
       "      <td>20.51</td>\n",
       "      <td>2.83</td>\n",
       "      <td>45.16</td>\n",
       "      <td>1.09</td>\n",
       "      <td>35.60</td>\n",
       "      <td>20.28</td>\n",
       "      <td>3.58</td>\n",
       "      <td>0.13</td>\n",
       "      <td>3436.93</td>\n",
       "      <td>115</td>\n",
       "      <td>12</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>July</th>\n",
       "      <td>237.04</td>\n",
       "      <td>67.07</td>\n",
       "      <td>44.50</td>\n",
       "      <td>19.57</td>\n",
       "      <td>97.25</td>\n",
       "      <td>0.04</td>\n",
       "      <td>227.47</td>\n",
       "      <td>23.31</td>\n",
       "      <td>2.88</td>\n",
       "      <td>38.27</td>\n",
       "      <td>0.75</td>\n",
       "      <td>35.40</td>\n",
       "      <td>23.48</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2473.18</td>\n",
       "      <td>125</td>\n",
       "      <td>18</td>\n",
       "      <td>13.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>August</th>\n",
       "      <td>129.67</td>\n",
       "      <td>55.46</td>\n",
       "      <td>24.23</td>\n",
       "      <td>19.01</td>\n",
       "      <td>22.96</td>\n",
       "      <td>0.08</td>\n",
       "      <td>220.46</td>\n",
       "      <td>23.46</td>\n",
       "      <td>3.03</td>\n",
       "      <td>54.38</td>\n",
       "      <td>0.63</td>\n",
       "      <td>41.92</td>\n",
       "      <td>24.93</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1637.28</td>\n",
       "      <td>126</td>\n",
       "      <td>11</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>September</th>\n",
       "      <td>116.79</td>\n",
       "      <td>28.04</td>\n",
       "      <td>58.58</td>\n",
       "      <td>12.81</td>\n",
       "      <td>3.57</td>\n",
       "      <td>6.04</td>\n",
       "      <td>174.65</td>\n",
       "      <td>20.06</td>\n",
       "      <td>3.11</td>\n",
       "      <td>69.73</td>\n",
       "      <td>0.64</td>\n",
       "      <td>36.59</td>\n",
       "      <td>24.38</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2102.01</td>\n",
       "      <td>143</td>\n",
       "      <td>31</td>\n",
       "      <td>22.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>October</th>\n",
       "      <td>97.76</td>\n",
       "      <td>13.21</td>\n",
       "      <td>35.56</td>\n",
       "      <td>19.39</td>\n",
       "      <td>2.17</td>\n",
       "      <td>19.76</td>\n",
       "      <td>127.60</td>\n",
       "      <td>15.56</td>\n",
       "      <td>3.08</td>\n",
       "      <td>72.07</td>\n",
       "      <td>0.80</td>\n",
       "      <td>36.91</td>\n",
       "      <td>21.70</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2988.28</td>\n",
       "      <td>138</td>\n",
       "      <td>24</td>\n",
       "      <td>17.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>November</th>\n",
       "      <td>80.54</td>\n",
       "      <td>13.81</td>\n",
       "      <td>29.33</td>\n",
       "      <td>19.09</td>\n",
       "      <td>1.11</td>\n",
       "      <td>8.90</td>\n",
       "      <td>82.48</td>\n",
       "      <td>11.35</td>\n",
       "      <td>3.01</td>\n",
       "      <td>74.21</td>\n",
       "      <td>1.20</td>\n",
       "      <td>36.72</td>\n",
       "      <td>17.61</td>\n",
       "      <td>3.09</td>\n",
       "      <td>0.06</td>\n",
       "      <td>5263.97</td>\n",
       "      <td>91</td>\n",
       "      <td>13</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>December</th>\n",
       "      <td>81.96</td>\n",
       "      <td>2.33</td>\n",
       "      <td>51.95</td>\n",
       "      <td>16.06</td>\n",
       "      <td>1.18</td>\n",
       "      <td>4.77</td>\n",
       "      <td>65.97</td>\n",
       "      <td>7.51</td>\n",
       "      <td>2.83</td>\n",
       "      <td>81.32</td>\n",
       "      <td>1.20</td>\n",
       "      <td>36.81</td>\n",
       "      <td>15.56</td>\n",
       "      <td>4.09</td>\n",
       "      <td>0.06</td>\n",
       "      <td>5868.74</td>\n",
       "      <td>39</td>\n",
       "      <td>3</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              DSP  Dinophysis caudata  Dinophysis fortii  \\\n",
       "month                                                      \n",
       "January      9.00                1.16               2.96   \n",
       "February    12.86                1.02               4.08   \n",
       "March       12.38                0.21               0.68   \n",
       "April       15.49                0.97               0.48   \n",
       "May         59.00                6.00               0.97   \n",
       "June       146.89               29.23               1.49   \n",
       "July       237.04               67.07              44.50   \n",
       "August     129.67               55.46              24.23   \n",
       "September  116.79               28.04              58.58   \n",
       "October     97.76               13.21              35.56   \n",
       "November    80.54               13.81              29.33   \n",
       "December    81.96                2.33              51.95   \n",
       "\n",
       "           Phalacroma rotundatum  Dinophysis sacculus  Dinophysis tripos  \\\n",
       "month                                                                      \n",
       "January                     3.66                 0.09               0.20   \n",
       "February                    3.38                 0.20               0.69   \n",
       "March                       3.79                 0.43               0.64   \n",
       "April                       5.24                 4.51               0.00   \n",
       "May                         9.67                34.25               0.15   \n",
       "June                       12.87                93.28               0.09   \n",
       "July                       19.57                97.25               0.04   \n",
       "August                     19.01                22.96               0.08   \n",
       "September                  12.81                 3.57               6.04   \n",
       "October                    19.39                 2.17              19.76   \n",
       "November                   19.09                 1.11               8.90   \n",
       "December                   16.06                 1.18               4.77   \n",
       "\n",
       "           sun [h]  air temp  wind strength  precipitation  Chl-a  salinity  \\\n",
       "month                                                                         \n",
       "January      72.62      4.95           2.71          46.53   0.76     36.95   \n",
       "February     80.19      5.52           3.06          60.66   0.77     37.02   \n",
       "March       118.56      7.55           3.30          30.01   0.77     37.17   \n",
       "April       144.63     11.69           3.17          54.90   0.68     36.93   \n",
       "May         171.96     16.22           2.89          48.55   0.96     36.57   \n",
       "June        202.85     20.51           2.83          45.16   1.09     35.60   \n",
       "July        227.47     23.31           2.88          38.27   0.75     35.40   \n",
       "August      220.46     23.46           3.03          54.38   0.63     41.92   \n",
       "September   174.65     20.06           3.11          69.73   0.64     36.59   \n",
       "October     127.60     15.56           3.08          72.07   0.80     36.91   \n",
       "November     82.48     11.35           3.01          74.21   1.20     36.72   \n",
       "December     65.97      7.51           2.83          81.32   1.20     36.81   \n",
       "\n",
       "               T   DIN  PO4-P     Soca  neg  poz  poz %  \n",
       "month                                                    \n",
       "January    11.07  6.79   0.07  4652.16   15    3    2.2  \n",
       "February   10.22  6.66   0.08  4378.79   21    3    2.2  \n",
       "March       9.49  4.37   0.05  3257.07   28    2    1.5  \n",
       "April      11.68  4.68   0.06  3826.10   66    2    1.5  \n",
       "May        14.87  3.92   0.07  3864.97   89   14   10.3  \n",
       "June       20.28  3.58   0.13  3436.93  115   12    8.8  \n",
       "July       23.48  3.38   0.05  2473.18  125   18   13.2  \n",
       "August     24.93  2.03   0.05  1637.28  126   11    8.1  \n",
       "September  24.38  2.06   0.05  2102.01  143   31   22.8  \n",
       "October    21.70  1.95   0.05  2988.28  138   24   17.6  \n",
       "November   17.61  3.09   0.06  5263.97   91   13    9.6  \n",
       "December   15.56  4.09   0.06  5868.74   39    3    2.2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# table of mean values for each feature by month \n",
    "import calendar\n",
    "\n",
    "grouped_means = data.groupby('month').mean()\n",
    "\n",
    "# Count binary values for the categorical feature grouped by month\n",
    "binary_counts = data.groupby('month')['lipophylic_toxins'].value_counts().unstack()\n",
    "\n",
    "# Calculate the ratio of positive values for each month\n",
    "sum_positive = binary_counts[\"poz\"].sum()\n",
    "positive_ratios = [i for i in (binary_counts[\"poz\"] / sum_positive)] \n",
    "positive_ratios = [round(v * 100, 1) for v in positive_ratios]\n",
    "\n",
    "# Change month names\n",
    "month_names = {i: calendar.month_name[i] for i in range(1, 13)}\n",
    "\n",
    "# Update the index using the month_names dictionary\n",
    "grouped_means.index = grouped_means.index.map(month_names)\n",
    "binary_counts.index = binary_counts.index.map(month_names)\n",
    "\n",
    "# Concatenate the grouped_means and binary_counts DataFrames\n",
    "result = pd.concat([grouped_means, binary_counts], axis=1).round(2)\n",
    "\n",
    "# Add the positive_ratios Series as a new column to the result DataFrame\n",
    "result[\"poz %\"] = positive_ratios\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c941f912-a57c-4caf-ae15-015c0dec5527",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Scikit-learn Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeda4953-b0aa-423e-b844-606188480710",
   "metadata": {},
   "source": [
    "# Data preprocessing for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc307518-a30c-493f-a0a9-888382107aa9",
   "metadata": {},
   "source": [
    "### Removing instances with unlabeled target, label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67f0106d-3b54-4623-8102-48ba8abd2460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class distribution:\n",
      "neg    662\n",
      "poz     88\n",
      "Name: lipophylic_toxins, dtype: int64\n",
      "class encoding: ['neg','poz'] -> [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Prepare for ML in scikit-learn\n",
    "# labeled and unlabeled part\n",
    "data_l = data[data['lipophylic_toxins'].notnull()]\n",
    "data_ul = data[data['lipophylic_toxins'].isnull()]\n",
    "\n",
    "# Remove missing values\n",
    "data_l = data_l.dropna(how=\"any\")\n",
    "print(f\"class distribution:\")\n",
    "print(data_l[\"lipophylic_toxins\"].value_counts(dropna=False))\n",
    "\n",
    "X = data_l.drop(\"lipophylic_toxins\", axis=1)\n",
    "y = data_l[\"lipophylic_toxins\"]\n",
    "\n",
    "# sklearn lable encoding\n",
    "le = LabelEncoder()\n",
    "le.fit(y)\n",
    "y = le.transform(y)\n",
    "print(f\"class encoding: ['neg','poz'] -> {le.transform(['neg','poz'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4e96a9-b60a-4ac7-9efe-2780a5c9ef12",
   "metadata": {},
   "source": [
    "### Clean instances close to the decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cbf75b-2474-48d0-b650-6c60d21d4829",
   "metadata": {},
   "source": [
    "Clean the dataset by removing samples close to the decision boundary. Because the dataset is heavily imbalanced in favor of clas 0 (neg) we will remove instances from this class whenever finding samples which do not agree “enough” with their neighboorhood. The EditedNearestNeighbours will be used. One other option is to use Tomek links but it is more conservative and was found to perform slightly worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b9e6e31-35f7-41b5-a9b0-a1f1876c9a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: Counter({0: 662, 1: 88})\n",
      "Resampled dataset shape Counter({0: 540, 1: 88})\n",
      "Resampled dataset shape Counter({0: 501, 1: 88})\n",
      "Resampled dataset shape Counter({0: 490, 1: 88})\n",
      "Resampled dataset shape Counter({0: 488, 1: 88})\n",
      "Resampled dataset shape Counter({0: 487, 1: 88})\n",
      "Cannot remove any more samples\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours\n",
    "\n",
    "print(f'Original dataset shape: {Counter(y)}')\n",
    "usmp = EditedNearestNeighbours()\n",
    "lastMajorityCount = Counter(y)[0]\n",
    "for i in range(10):\n",
    "    X_res, y_res = usmp.fit_resample(X, y)\n",
    "    if Counter(y_res)[0] == lastMajorityCount:\n",
    "        print('Cannot remove any more samples')\n",
    "        break\n",
    "    else:\n",
    "        print(f'Resampled dataset shape {Counter(y_res)}')\n",
    "        lastMajorityCount = Counter(y_res)[0]\n",
    "    X = X_res\n",
    "    y = y_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d58759b-b41a-4938-9acb-c736a6016b63",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d406548-0ce8-44bf-9ba2-6311c8c9c2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X, X_eval, y, y_eval = train_test_split(X, y, shuffle=True, stratify=y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e68a779-d7ad-4bf9-b597-594cfbf760e6",
   "metadata": {},
   "source": [
    "# Correlation analysis on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e93f704-b7e9-4383-bf61-a15e4e4080ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Pearson  Spearman\n",
      "DSP                    0.050517  0.112360\n",
      "T                      0.133431  0.108940\n",
      "Phalacroma rotundatum  0.027626  0.078264\n",
      "Chl-a                 -0.081980 -0.067924\n",
      "air temp               0.101362  0.061298\n",
      "month                  0.079418  0.060695\n",
      "Dinophysis caudata     0.159930  0.055140\n",
      "DIN                    0.048785  0.038620\n",
      "PO4-P                 -0.046319 -0.037694\n",
      "Dinophysis sacculus   -0.012564 -0.037541\n",
      "precipitation          0.032200  0.036034\n",
      "wind strength          0.003026  0.035853\n",
      "sun [h]                0.048582  0.035070\n",
      "salinity              -0.055177 -0.025085\n",
      "Dinophysis tripos      0.076581  0.018183\n",
      "Soca                  -0.043240 -0.011008\n",
      "Dinophysis fortii     -0.035430 -0.002735\n"
     ]
    }
   ],
   "source": [
    "# Calculate Pearson correlation between numeric features and binary target variable\n",
    "pearson_correlations = X.corrwith(pd.Series(y), method='pearson')\n",
    "pearson_correlations.name = 'Pearson'\n",
    "\n",
    "# Calculate Spearman rank correlation between numeric features and binary target variable\n",
    "spearman_correlations = X.corrwith(pd.Series(y), method='spearman')\n",
    "spearman_correlations.name = 'Spearman'\n",
    "\n",
    "# Combine the correlation values into a single dataframe\n",
    "corr_df = pd.concat([pearson_correlations, spearman_correlations], axis=1)\n",
    "\n",
    "# Create a new column for absolute Spearman correlation values\n",
    "corr_df['Spearman_abs'] = corr_df['Spearman'].abs()\n",
    "\n",
    "# Sort the dataframe by absolute Spearman correlation values\n",
    "corr_df_sorted = corr_df.sort_values(by=['Spearman_abs'], ascending=False)\n",
    "\n",
    "# Drop the absolute Spearman correlation column and return the sorted dataframe\n",
    "corr_df_ranked = corr_df_sorted.drop(columns=['Spearman_abs'])\n",
    "\n",
    "print(corr_df_ranked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b146cf-97fb-4a9b-8e33-dac52f91b48c",
   "metadata": {},
   "source": [
    "## Logistic regression and p-value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89623bf-dd24-405f-9c3d-c53725a26763",
   "metadata": {},
   "source": [
    "We use the coef_pval() method to calculate the p-values for each coefficient in the logistic regression model, and create a dataframe with the logistic regression coefficients and corresponding p-values. Finally, we rank the features by their absolute logistic regression coefficients and print the resulting dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dc96193-51d8-41c2-8019-e653f50e2f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.342655\n",
      "         Iterations 8\n",
      "                       Coefficient  P-value\n",
      "PO4-P                        3.269    0.306\n",
      "wind strength               -0.950    0.034\n",
      "Chl-a                        0.420    0.133\n",
      "salinity                    -0.164    0.119\n",
      "DIN                          0.080    0.136\n",
      "month                       -0.056    0.546\n",
      "Dinophysis tripos            0.041    0.028\n",
      "Phalacroma rotundatum       -0.029    0.079\n",
      "T                            0.012    0.872\n",
      "precipitation                0.009    0.015\n",
      "sun [h]                     -0.008    0.304\n",
      "air temp                     0.008    0.930\n",
      "Dinophysis fortii            0.007    0.463\n",
      "Dinophysis sacculus          0.001    0.921\n",
      "DSP                         -0.001    0.929\n",
      "Soca                        -0.000    0.001\n",
      "Dinophysis caudata           0.000    0.999\n"
     ]
    }
   ],
   "source": [
    "# logistic regression and p-value\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Fit logistic regression model\n",
    "logit_model = sm.Logit(y, sm.add_constant(X))\n",
    "result = logit_model.fit()\n",
    "\n",
    "# Calculate p-values for the logistic regression coefficients\n",
    "p_values = result.pvalues[1:]\n",
    "\n",
    "# Create a dataframe with the logistic regression coefficients and corresponding p-values\n",
    "coef_df = pd.DataFrame({'Coefficient': result.params[1:], 'P-value': p_values})\n",
    "\n",
    "# Add feature names as the index\n",
    "coef_df.index = X.columns\n",
    "\n",
    "# Rank the features by absolute logistic regression coefficients\n",
    "coef_df['Absolute Coefficient'] = coef_df['Coefficient'].abs()\n",
    "coef_df_sorted = coef_df.sort_values(by=['Absolute Coefficient'], ascending=False)\n",
    "coef_df_ranked = coef_df_sorted.drop(columns=['Absolute Coefficient'])\n",
    "\n",
    "print(coef_df_ranked.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17e63f2-39c7-4ab8-bb0e-22e01991f9e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afe5386-abb2-4fac-bb14-3e35e9124faa",
   "metadata": {},
   "source": [
    "## Baseline model - logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f171e07e-333c-4786-8814-efec9ee7009f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.87      0.97      0.92       147\n",
      "         poz       0.56      0.19      0.29        26\n",
      "\n",
      "    accuracy                           0.86       173\n",
      "   macro avg       0.71      0.58      0.60       173\n",
      "weighted avg       0.82      0.86      0.82       173\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
     ]
    }
   ],
   "source": [
    "# baseline model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Fit logistic regression model using scikit-learn\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X, y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logreg.predict(X_eval)\n",
    "\n",
    "# Evaluation on test data\n",
    "y_pred = logreg.predict(X_eval)\n",
    "lr_classification_report = classification_report(y_eval, y_pred)\n",
    "\n",
    "# Create classification report as dictionary\n",
    "lr_report_dict = classification_report(y_eval, y_pred, output_dict=True)\n",
    "\n",
    "print(classification_report(y_eval, y_pred, target_names=[\"neg\", \"poz\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "141ebec7-9dcb-4d12-97e5-14a6aef85da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save best estimator\n",
    "\n",
    "def save_best_estimator(grid_search_cv, classifier_name, notebook_name):\n",
    "    # Get the best estimator from the GridSearchCV object\n",
    "    best_estimator = grid_search_cv.best_estimator_\n",
    "\n",
    "    # Get the current date and time as a string\n",
    "    timestamp = datetime.datetime.now().strftime('%d%m%Y_%H%M')\n",
    "    \n",
    "    # Construct the file name with the classifier name, notebook name, and timestamp\n",
    "    pickle_file_name = f\"objects/estimators/{classifier_name}-{notebook_name}-{timestamp}.pkl\"\n",
    "    \n",
    "    # Save the best estimator to the file\n",
    "    with open(pickle_file_name, 'wb') as f:\n",
    "        pickle.dump(best_estimator, f)\n",
    "\n",
    "    print(f\"Best estimator saved as: {pickle_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3996100-0bd8-447a-a1b9-3a090bd55cab",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "907477fc-ebae-4d60-a61f-8bacc5693a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'objects/estimators/SVC-HAB_modelling_5_7-30042023_1004.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m classifier_name \u001b[38;5;241m=\u001b[39m resultsGSCV\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclf\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# save the best estimator\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[43msave_best_estimator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgscv_svm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnotebook_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(resultsGSCV\u001b[38;5;241m.\u001b[39mcv_results_)\n\u001b[1;32m     36\u001b[0m display(results\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrank_test_f1\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mtranspose())\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36msave_best_estimator\u001b[0;34m(grid_search_cv, classifier_name, notebook_name)\u001b[0m\n\u001b[1;32m     11\u001b[0m pickle_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjects/estimators/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclassifier_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnotebook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Save the best estimator to the file\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpickle_file_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     15\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(best_estimator, f)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest estimator saved as: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpickle_file_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'objects/estimators/SVC-HAB_modelling_5_7-30042023_1004.pkl'"
     ]
    }
   ],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "pipeline = IMBLpipeline([\n",
    "    ('smt', SMOTE()), \n",
    "    ('under', RandomUnderSampler()), \n",
    "    ('clf', SVC())\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "            'clf__C': [0.1, 1, 10],  \n",
    "            # 'clf__kernel': ['linear', 'rbf', 'poly'],\n",
    "            'clf__class_weight': ['balanced', None],\n",
    "            'smt__sampling_strategy': [ 0.2, 0.3, 0.4],\n",
    "            'under__sampling_strategy': [0.5, 0.6, 0.7],\n",
    "            'smt__k_neighbors': [1, 3, 5]\n",
    "             }\n",
    "nfolds = 3\n",
    "scores = ['recall', 'precision', 'f1', 'roc_auc', 'recall_weighted']\n",
    "gscv_svm = GridSearchCV(pipeline, \n",
    "                    parameters, \n",
    "                    scoring=scores,\n",
    "                    cv=StratifiedKFold(n_splits=nfolds, shuffle=True),\n",
    "                    return_train_score=False, \n",
    "                    verbose=1, \n",
    "                    refit=\"f1\",\n",
    "                    n_jobs=-1)\n",
    "resultsGSCV = gscv_svm.fit(X, y)\n",
    "\n",
    "# Get the classifier name from the pipeline\n",
    "classifier_name = resultsGSCV.best_estimator_.named_steps['clf'].__class__.__name__\n",
    "    \n",
    "# save the best estimator\n",
    "save_best_estimator(gscv_svm, classifier_name, notebook_name)\n",
    "\n",
    "results = pd.DataFrame(resultsGSCV.cv_results_)\n",
    "display(results.sort_values(by=[f'rank_test_f1']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd040b2-10bc-4183-a99a-ede567383e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on test data\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = gscv_svm.best_estimator_.steps[2][1].predict(X_eval)\n",
    "SVM_classification_report = classification_report(y_eval, y_pred)\n",
    "\n",
    "# Create classification report as dictionary\n",
    "SVM_report_dict = classification_report(y_eval, y_pred, output_dict=True)\n",
    "\n",
    "print(classification_report(y_eval, y_pred, target_names=[\"neg\", \"poz\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1735f33-e739-4f9a-bb3c-d87f98d21466",
   "metadata": {},
   "source": [
    "## Decision Tree Model (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf64c198-6799-4e17-838e-e52009105996",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "pipeline = IMBLpipeline([\n",
    "    ('smt', SMOTE()), \n",
    "    ('under', RandomUnderSampler()), \n",
    "    ('clf', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "            'clf__max_depth': [2,3,4],\n",
    "            'clf__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "               'clf__class_weight': ['balanced', None],\n",
    "               'smt__sampling_strategy': [ 0.2, 0.3, 0.4],\n",
    "               'under__sampling_strategy': [0.5, 0.6, 0.7],\n",
    "               'smt__k_neighbors': [1, 3, 5]\n",
    "             }\n",
    "nfolds = 3\n",
    "scores = ['recall', 'precision', 'f1', 'roc_auc', 'recall_weighted']\n",
    "gscv_dt = GridSearchCV(pipeline, \n",
    "                    parameters, \n",
    "                    scoring=scores,\n",
    "                    cv=StratifiedKFold(n_splits=nfolds, shuffle=True),\n",
    "                    return_train_score=False, \n",
    "                    verbose=1, \n",
    "                    refit=\"f1\",\n",
    "                    n_jobs=-1)\n",
    "resultsGSCV = gscv_dt.fit(X, y)\n",
    "\n",
    "# Get the classifier name from the pipeline\n",
    "classifier_name = resultsGSCV.best_estimator_.named_steps['clf'].__class__.__name__\n",
    "    \n",
    "# save the best estimator\n",
    "save_best_estimator(gscv_dt, classifier_name, notebook_name)\n",
    "\n",
    "results = pd.DataFrame(resultsGSCV.cv_results_)\n",
    "display(results.sort_values(by=[f'rank_test_f1']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395678b1-57e4-480f-980d-4106ab700c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on test data\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = gscv_dt.best_estimator_.steps[2][1].predict(X_eval)\n",
    "DT_classification_report = classification_report(y_eval, y_pred)\n",
    "\n",
    "# Create classification report as dictionary\n",
    "DT_report_dict = classification_report(y_eval, y_pred, output_dict=True)\n",
    "\n",
    "print(classification_report(y_eval, y_pred, target_names=[\"neg\", \"poz\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158fb422-dd77-4258-b2f8-0f6be3d407db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the model, where \"loaded_estimator\" contains the best estimator saved\n",
    "# with open('best_estimator.pkl', 'rb') as f:\n",
    "#     loaded_estimator = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5024ff2c-4a0c-41da-91cb-8c87eea28b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = gscv_dt.best_estimator_.steps[2][1]\n",
    "viz = dtreeviz(clf, X, y,\n",
    "                target_name=\"target\",\n",
    "                feature_names=X.columns,\n",
    "                class_names=[\"neg\", \"poz\"],\n",
    "             fancy=False,\n",
    "               scale=1.5\n",
    "              )\n",
    "\n",
    "viz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0db8e4c-76d7-4b28-a874-e70f5256bf91",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be1409e-0948-4107-bc34-558b1bb08a35",
   "metadata": {},
   "source": [
    "#### Model evaluation (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8c5f08-1792-4ae9-b2ca-e72b2c24fe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest with grid search for parameters, testing on 5-fold CV with shuffling\n",
    "\n",
    "pipeline = IMBLpipeline([\n",
    "   ('smt', SMOTE()), \n",
    "   ('under', RandomUnderSampler()), \n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "              'clf__n_estimators': [100,300,500],\n",
    "              'clf__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "              'clf__class_weight': ['balanced', 'balanced_subsample', None],\n",
    "              'smt__sampling_strategy': [ 0.2, 0.3, 0.4],\n",
    "              'under__sampling_strategy': [0.5, 0.6, 0.7],\n",
    "              'smt__k_neighbors': [3, 5]\n",
    "             }\n",
    "\n",
    "nfolds = 3\n",
    "scores = ['recall', 'precision', 'f1', 'roc_auc']\n",
    "refit_score = 'f1'\n",
    "gscv_rf = GridSearchCV(pipeline, \n",
    "                    parameters, \n",
    "                    scoring=scores,\n",
    "                    cv=StratifiedKFold(n_splits=nfolds, shuffle=True),\n",
    "                    return_train_score=False, \n",
    "                    verbose=1, \n",
    "                    refit=refit_score,\n",
    "                    n_jobs=-1)\n",
    "resultsGSCV = gscv_rf.fit(X, y)\n",
    "\n",
    "# Get the classifier name from the pipeline\n",
    "classifier_name = resultsGSCV.best_estimator_.named_steps['clf'].__class__.__name__\n",
    "    \n",
    "# save the best estimator\n",
    "save_best_estimator(gscv_rf, classifier_name, notebook_name)\n",
    "\n",
    "results = pd.DataFrame(resultsGSCV.cv_results_)\n",
    "display(results.sort_values(by=[f'rank_test_f1']).transpose())\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6c5dc6-c5f1-4118-9af1-da0c03851db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation RF on test set\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = gscv_rf.best_estimator_.steps[2][1].predict(X_eval)\n",
    "RF_classification_report = classification_report(y_eval, y_pred)\n",
    "\n",
    "# Create classification report as dictionary\n",
    "RF_report_dict = classification_report(y_eval, y_pred, output_dict=True)\n",
    "\n",
    "print(classification_report(y_eval, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f416e105-b46a-497f-8347-aa1f623f305f",
   "metadata": {},
   "source": [
    "Plot the mean ROC curve of the algorithm with best performing parameter selection. We will perform CV once again and plot the ROC curve for each fold and compute and plot the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97807f2-1c2c-404d-9ded-0b4e3a782ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Run classifier with cross-validation and plot ROC curves\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "classifier = resultsGSCV.best_estimator_\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "for i, (train, test) in enumerate(cv.split(X_eval, y_eval)):\n",
    "    classifier.fit(X_eval.iloc[train], y_eval[train])\n",
    "    viz = RocCurveDisplay.from_estimator(\n",
    "        classifier,\n",
    "        X_eval.iloc[test],\n",
    "        y_eval[test],\n",
    "        name=\"fold {}\".format(i),\n",
    "        alpha=0.3,\n",
    "        lw=1,\n",
    "        ax=ax,\n",
    "    )\n",
    "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(viz.roc_auc)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Baseline (random prediction)\", alpha=0.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(\n",
    "    mean_fpr,\n",
    "    mean_tpr,\n",
    "    color=\"b\",\n",
    "    label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n",
    "    lw=2,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax.fill_between(\n",
    "    mean_fpr,\n",
    "    tprs_lower,\n",
    "    tprs_upper,\n",
    "    color=\"grey\",\n",
    "    alpha=0.2,\n",
    "    label=r\"$\\pm$ 1 std. dev.\",\n",
    ")\n",
    "\n",
    "clfname = [str(step[1].__class__.__name__) for step in classifier.steps if step[0]=='clf'][0]\n",
    "ax.set(\n",
    "    xlim=[-0.05, 1.05],\n",
    "    ylim=[-0.05, 1.05],\n",
    "    title=f'{clfname} evaluation (ROC-AUC, {nfolds}-fold CV)',\n",
    ")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ab2296-3bf0-49c5-86ae-843d9a0488db",
   "metadata": {},
   "source": [
    "Plot the mean precision-recall curve. The approach is the same as for the mean ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f497691-967e-4591-afcf-71c9c7ce3377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "# cv = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "# classifier = resultsGSCV.best_estimator_\n",
    "\n",
    "# prs = []\n",
    "# aucs = []\n",
    "# mean_r = np.linspace(0, 1, 100)\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(10,8))\n",
    "# for i, (train, test) in enumerate(cv.split(X_eval, y_eval)):\n",
    "#     classifier.fit(X.iloc[train], y[train])\n",
    "#     viz = PrecisionRecallDisplay.from_estimator(\n",
    "#         classifier,\n",
    "#         X_eval.iloc[test],\n",
    "#         y_eval[test],\n",
    "#         name=\"fold {}\".format(i),\n",
    "#         alpha=0.3,\n",
    "#         lw=1,\n",
    "#         ax=ax,\n",
    "#     )\n",
    "#     interp_pr = np.interp(mean_r, viz.recall[::-1], viz.precision[::-1])\n",
    "#     prs.append(interp_pr)\n",
    "\n",
    "# mean_p = np.mean(prs, axis=0)\n",
    "# ax.plot(\n",
    "#     mean_r,\n",
    "#     mean_p,\n",
    "#     color=\"b\",\n",
    "#     label=f\"mean\",\n",
    "#     lw=2,\n",
    "#     alpha=0.8,\n",
    "# )\n",
    "# ax.legend(loc=\"lower left\")\n",
    "# clfname = [str(step[1].__class__.__name__) for step in classifier.steps if step[0]=='clf'][0]\n",
    "# ax.set(\n",
    "#     # xlim=[-0.05, 1.05],\n",
    "#     # ylim=[-0.05, 1.05],\n",
    "#     title=f'{clfname} evaluation (precision-recall, {nfolds}-fold CV)')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e419d1-f53e-4a68-a922-6ae77cac13e9",
   "metadata": {},
   "source": [
    "#### Feature importance (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2cfb82-7f43-466c-b492-90b58e7c6966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance of model (best RandomForest from gridsearch) with three methods!\n",
    "\n",
    "fig, (ax2) = plt.subplots(1, 1, figsize=(10,9))\n",
    "plt.subplots_adjust(wspace=1.1)\n",
    "\n",
    "rf = gscv_rf.best_estimator_.steps[2][1]\n",
    "\n",
    "# Get feature importance with Permutation Based Feature Importance (randomly shuffles each feature and compute the \n",
    "# change in the model’s performance. The features which impact the performance the most are the most important one).\n",
    "perm_importance = permutation_importance(rf, X, y)\n",
    "perm_sorted_idx = perm_importance.importances_mean.argsort()\n",
    "x2 = X.columns[perm_sorted_idx]\n",
    "y2 = perm_importance.importances_mean[perm_sorted_idx]\n",
    "ax2.barh(x2, y2)\n",
    "ax2.set_title(\"Permutation Importance Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917caad5-b4d1-4364-a63e-b0f77f687670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probaj enako z X_eval in primerjaj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0547d681-1201-4853-a89a-006a0efddb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get feature importance with SHAP\n",
    "# explainer = shap.TreeExplainer(rf)\n",
    "# shap_values = explainer.shap_values(X)\n",
    "# RF_shap = shap.summary_plot(shap_values, X, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d081269b-9944-4784-abf0-d36ad02267aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probaj enako z X_eval in primerjaj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d84a41-047b-4a58-a46e-689374f7d873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP summary plot\n",
    "explainer = shap.TreeExplainer(rf)\n",
    "shap_values = explainer.shap_values(X)\n",
    "classid = 1\n",
    "shap.summary_plot(shap_values[classid], X, max_display=len(X.columns), class_names=le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ca727-c52f-412b-a499-d6e9cdee947f",
   "metadata": {},
   "source": [
    "### Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc6e6b4-1a36-47c3-92c2-51a1340b79c1",
   "metadata": {},
   "source": [
    "#### Model Evaluation (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70de528-5c6d-428b-a0f4-890707fc436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for NN in scikit_learn\n",
    "\n",
    "# Model evaluation with the pipeline of SMOTE oversampling and undersampling on the training dataset only (within each cross-validation fold)!\n",
    "\n",
    "# one-hot encoding of month feature\n",
    "Xohe = pd.get_dummies(X, columns=[\"month\"])\n",
    "\n",
    "X_display = Xohe.copy()  # *used for SHAP visualization so we can show unscaled values\n",
    "\n",
    "# scalling numeric values for NN\n",
    "scaled_array = StandardScaler().fit_transform(Xohe)\n",
    "Xsc = pd.DataFrame(scaled_array, columns=Xohe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a550fb63-51e5-464b-ae88-7b8579958cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60b9414-2528-450c-b46e-44967eee00a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP with grid search for parameters, testing on 5-fold CV with shuffling\n",
    "\n",
    "pipeline = IMBLpipeline([\n",
    "    ('over', SMOTE()),\n",
    "    ('under', RandomUnderSampler()),\n",
    "    ('clf', MLPClassifier(solver='lbfgs', max_iter=5000))\n",
    "])\n",
    "\n",
    "parameters = {'over__k_neighbors': range(1,7),\n",
    "              'over__sampling_strategy': [0.5, 0.6, 0.8], # probaj poveča ovresampling do 0.9\n",
    "              'under__sampling_strategy': [0.6, 0.7, 0.8],\n",
    "              'clf__hidden_layer_sizes': [(2, ), (2, 2), (3,), (3,3)],\n",
    "              'clf__solver': ['lbfgs', 'sgd', 'adam']\n",
    "             }\n",
    "nfolds = 3\n",
    "scores = ['recall', \"precision\", 'f1', 'roc_auc']\n",
    "gscv_NN = GridSearchCV(pipeline, \n",
    "                    parameters, \n",
    "                    scoring=scores,\n",
    "                    cv=StratifiedKFold(n_splits=nfolds, shuffle=True),\n",
    "                    n_jobs= -1, \n",
    "                    return_train_score=False, \n",
    "                    verbose=1, \n",
    "                    refit= \"f1\")\n",
    "resultsGSCV = gscv_NN.fit(Xsc, y)\n",
    "\n",
    "# Get the classifier name from the pipeline\n",
    "classifier_name = resultsGSCV.best_estimator_.named_steps['clf'].__class__.__name__\n",
    "    \n",
    "# save the best estimator\n",
    "save_best_estimator(gscv_NN, classifier_name, notebook_name)\n",
    "\n",
    "results = pd.DataFrame(resultsGSCV.cv_results_)\n",
    "display(results.sort_values(by=[f'rank_test_f1']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017735d1-2fe5-4b37-b40a-a45fb91b8be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of NN on test set\n",
    "from sklearn.metrics import classification_report\n",
    "X_eval_ohe = pd.get_dummies(X_eval, columns=[\"month\"])\n",
    "scaler = StandardScaler().fit(Xohe)\n",
    "X_eval_sc = scaler.transform(X_eval_ohe)\n",
    "X_eval_sc = pd.DataFrame(X_eval_sc, columns=Xohe.columns)\n",
    "y_pred = gscv_NN.best_estimator_.steps[2][1].predict(X_eval_sc)\n",
    "NN_classification_report = classification_report(y_eval, y_pred)\n",
    "\n",
    "# Create classification report as dictionary\n",
    "NN_report_dict = classification_report(y_eval, y_pred, output_dict=True)\n",
    "\n",
    "print(classification_report(y_eval, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ba333f-3931-4bf7-ae33-757478d69a30",
   "metadata": {},
   "source": [
    "#### Feature Importance (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f68acd-b343-48c6-a372-d46ed52e6951",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature importance of model (MLP)  (no cross-validation!)\n",
    "\n",
    "fig, (ax2) = plt.subplots(1, 1, figsize=(10,9))\n",
    "plt.subplots_adjust(wspace=3)\n",
    "\n",
    "MLP = gscv_NN.best_estimator_.steps[2][1]\n",
    "\n",
    "# Get feature importance with Permutation Based Feature Importance (randomly shuffles each feature and compute the \n",
    "# change in the model’s performance. The features which impact the performance the most are the most important one).\n",
    "perm_importance = permutation_importance(MLP, Xsc, y)\n",
    "perm_sorted_idx = perm_importance.importances_mean.argsort()\n",
    "x2 = Xsc.columns[perm_sorted_idx]\n",
    "y2 = perm_importance.importances_mean[perm_sorted_idx]\n",
    "ax2.barh(x2, y2)\n",
    "ax2.set_title(\"Permutation Importance MLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09700f24-b683-41ee-85b1-1c957b12325a",
   "metadata": {},
   "source": [
    "#### Feature importance with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec901eb-8260-445e-8b66-919df83ffd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Xsc.copy()\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, stratify=y, test_size=0.25)\n",
    "\n",
    "nn = MLPClassifier(hidden_layer_sizes=(3,3), solver='lbfgs', max_iter=5000)\n",
    "model = nn.fit(X.to_numpy(), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902cad31-dc72-4a66-b376-2ca9ea8e0e80",
   "metadata": {},
   "source": [
    "First, visualize the impact of all features on both classes in one chart. We are using KernelExplainer but simpler general Explainer should be also tested once the SHAP code fixes all bugs.\n",
    "\n",
    "**Note: SHAP explanations change between runs because of sampling and probably other random factors!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241febf0-4e3c-40b1-b0ff-a0d4d4d99787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # explain the model's predictions using SHAP\n",
    "# import shap\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# shap.initjs()\n",
    "\n",
    "# explainer = shap.KernelExplainer(model.predict_proba, shap.sample(X_eval_sc,20))\n",
    "# shap_values = explainer.shap_values(X_eval_sc, nsamples=50)\n",
    "# shap.summary_plot(shap_values, X_eval_sc, max_display=len(X.columns), class_names=le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9123988d-5f60-4676-8f57-cccbbff97cbe",
   "metadata": {},
   "source": [
    "Now for each class separately. We observe the impact of features on the returned model's probability for a given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385ff88c-0dce-4fb5-a974-fec0a9b91d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict_proba, shap.sample(Xsc,50)) #morda treba zmanjšat število, ali brez sample in samo X_eval\n",
    "shap_values = explainer.shap_values(Xsc, nsamples=50)\n",
    "classid = 1\n",
    "shap.summary_plot(shap_values[classid], Xsc, max_display=len(X.columns), class_names=le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cac8790-80dc-44b5-9e13-18d2050b196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try dependence contribution plot\n",
    "explainer = shap.KernelExplainer(model.predict_proba, shap.sample(X_eval_sc,50))\n",
    "shap_values = explainer.shap_values(X_eval_sc, nsamples=50)\n",
    "shap.dependence_plot('salinity', shap_values[1], X_eval_sc,) #interaction_index=\"salinity\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38930dca-d69a-4393-a88b-6fc019d83c57",
   "metadata": {},
   "source": [
    "Example intepretation: The fact this slopes upward says the higher the soca flow, the higher the model's prediction is for poz/neg. The spread suggests that other features must interact with Soca flow. \n",
    "In general, high Soca flow increases the chance of poz/neg. But if the sea temp is moderate or low, that trend reverses and even high soca flow does not increase preditions of poz/neg as the sea temp is too low.\n",
    "https://www.kaggle.com/code/dansbecker/advanced-uses-of-shap-values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e17d179-7316-4026-b266-1bc7ef4662b9",
   "metadata": {},
   "source": [
    "Now let's explain the prediction of a single instance. We will show the explanation of the bigger predicted probability to see why the model decided as it did. But in practice we could be interested only in the explanation of the probability of the positive prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d07af4e-262b-45e6-beab-41bdf773fb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "instanceID = 10\n",
    "instance = X.iloc[[instanceID]]\n",
    "display_instance = X_display.iloc[[instanceID]]\n",
    "\n",
    "prediction = model.predict(instance)[0]\n",
    "prediction_probs = model.predict_proba(instance)[0]\n",
    "print(f'real value: {y[instanceID]}, \\npredicted: {prediction}, \\npredicted probs: {prediction_probs}')\n",
    "max_p_id = prediction_probs.argmax()  # we will show the explanation of the bigger predicted probability\n",
    "print(f'Explanation for prediction: class={max_p_id}, p={prediction_probs.max()}')\n",
    "\n",
    "explainer = shap.KernelExplainer(model.predict_proba, shap.sample(X, 50))\n",
    "shap_values = explainer.shap_values(instance, nsamples=500)\n",
    "shap.force_plot(explainer.expected_value[max_p_id], shap_values[max_p_id], features=display_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4908630-eea9-4167-83d9-5511dd820733",
   "metadata": {},
   "source": [
    "Show the mean values of features as it may help understanding this particular instance data in the plot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642977c5-c5e3-4c96-b81f-c9419b084dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=[\"month\"])\n",
    "\n",
    "\n",
    "pd.DataFrame([data[data['lipophylic_toxins']=='neg'].mean(), data[data['lipophylic_toxins']=='poz'].mean()], index=['neg','pos']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9f3aca-b6f0-4ab4-bd20-b828301bfdb2",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8273fc8d-8b18-4f8b-8c19-448fb5843226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract the metrics from the classification report dictionary\n",
    "def extract_metrics(report_dict):\n",
    "    metrics = {}\n",
    "    for class_label in report_dict:\n",
    "        if class_label in ('accuracy', 'macro avg', 'weighted avg'):\n",
    "            continue\n",
    "        metrics[class_label] = {\n",
    "            'precision': report_dict[class_label]['precision'],\n",
    "            'recall': report_dict[class_label]['recall'],\n",
    "            'f1-score': report_dict[class_label]['f1-score'],\n",
    "            'support': report_dict[class_label]['support']\n",
    "        }\n",
    "    return metrics\n",
    "\n",
    "# Extract the metrics for each classifier\n",
    "lr_metrics = extract_metrics(lr_report_dict)\n",
    "SVM_metrics = extract_metrics(SVM_report_dict)\n",
    "DT_metrics = extract_metrics(DT_report_dict)\n",
    "RF_metrics = extract_metrics(RF_report_dict)\n",
    "NN_metrics = extract_metrics(NN_report_dict)\n",
    "\n",
    "# Create a dictionary to store the metrics for each classifier\n",
    "classifier_metrics = {\n",
    "    'Logistic Regression': lr_metrics,\n",
    "    'Support Vector Mashines': SVM_metrics,\n",
    "    'Decision Tree': DT_metrics,\n",
    "    'Random Forest': RF_metrics,\n",
    "    'Neural Network': NN_metrics\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a pandas DataFrame\n",
    "summary_df = pd.concat({k: pd.DataFrame(v).transpose() for k, v in classifier_metrics.items()}, axis=0)\n",
    "summary_df.reset_index(inplace=True)\n",
    "summary_df.columns = ['Classifier', 'Class', 'Precision', 'Recall', 'F1-score', 'Support']\n",
    "\n",
    "# Display the summary DataFrame\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edb6f9a-7038-4f5a-9a98-914a7d0e7557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table of prediction results\n",
    "RF_recall = round(RF_recall_best_k[1], 2)\n",
    "RF_auc = round(RF_auc_best_k[1], 2)\n",
    "MLP_recall = round(MLP_recall_best_k[1], 2)\n",
    "MLP_auc = round(MLP_auc_best_k[1], 2)\n",
    "\n",
    "summary = pd.DataFrame(\n",
    "    [\n",
    "        (\n",
    "            \"RF\",\n",
    "            RF_recall_score,\n",
    "            RF_auc_score,\n",
    "        ),\n",
    "        (\n",
    "            \"MLP\",\n",
    "            MLP_recall_score,\n",
    "            MLP_auc_score,\n",
    "        ),\n",
    "        (\n",
    "            \"RF (smote)\",\n",
    "            RF_recall,\n",
    "            RF_auc,\n",
    "        ),\n",
    "        (\n",
    "            \"MLP (smote)\",\n",
    "            MLP_recall,\n",
    "            MLP_auc,\n",
    "        ),\n",
    "        (\n",
    "            \"Decision tree (J48)*\",\n",
    "            0.56,\n",
    "            0.18,\n",
    "        ),\n",
    "    ],\n",
    "    columns=(\"Model\", \"Recall\", \"ROC AUC\"),\n",
    ").set_index(\"Model\")\n",
    "\n",
    "print(\"Table summarising the prediction results of the used classifiers, both with and without SMOTE resampling:\\n\")\n",
    "summary.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c712a49-d47a-456a-9905-50a6502a4be2",
   "metadata": {},
   "source": [
    "As can be seen resampling with SMOTE helped to improve the results substantially, especially when calculating recall. The highest recall and ROC AUC was achieved with Random Forest with the re-sampled data. Both recall and ROC AUC suggest Random Forest as beeing the better classifier for this particular problem. Recall is a crucial metric as it gives indication of what fraction of true positive instances have been predicted. Since the models predict toxins in seashells (food) it is crucial that as few positives as possible are missed.\n",
    "\n",
    "Due to the use of SMOTE resampling (upsampling and downsampling) in combinaiton with cross-validation it was curcial to do the resampling within each fold to avoid data lekeage and validate on original (unsampled) data. In addition, I have optimised the model with regard to the k-values of SMOTE, all of which brought along some complexity. So for the parameter tuning of Random Forest and MLP various parameter settings have been tried  and the model with best performing settings has been chosen.\n",
    "\n",
    "The decision tree J48 algorithm was run within Weka on a slightly different dataset (missing values were not removed to use as many instances as possible, cross validation was 10-fold as opposed to 3-fold due to a higher dataset etc.) thus this results are not directly comparable but were provided as a reference to give an indication of the performance of this algorithm. \n",
    "\n",
    "As can be seen in the feature importance bar plots above, similar features were on the top despite using two different classification algorithms and two different feature ranking methods. If we consider just the three highest-ranking features of each of the feature ranking methods for both algortihms (RF and MLP) the features that overlap are DSP, DSP_like, ASP, Dinophysis fortii and Dinophysis caudata. These can be shown to the domain experts for validation and interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d9c9a9-8f66-45d7-9767-8555adcd5d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
