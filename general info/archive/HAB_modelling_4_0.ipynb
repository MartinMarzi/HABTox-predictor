{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57d854a4-aa15-40da-bd45-4df04e0043ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil.parser import parse\n",
    "pd.set_option(\"display.max_rows\", 15)\n",
    "pd.set_option(\"display.max_columns\", 15)\n",
    "import datetime\n",
    "from dateutil.parser import parse\n",
    "import math\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d530319-a99e-4824-982c-5d531a8ac548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read df pickle\n",
    "df_alg = pd.read_pickle(\"objects/df_alg\")\n",
    "df_cons = pd.read_pickle(\"objects/df_cons4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d116f4c-ab3e-4f4f-b159-fed440e78707",
   "metadata": {},
   "source": [
    "# Modelling and Preliminary Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6f69c3-3202-4942-bddd-06d8f2dc6ce6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49316e37-8b23-48d7-ba37-be56ca18e203",
   "metadata": {},
   "source": [
    "Due to the needs for specific algorithms the modelling has now been carried out in Weka and below are two selected models and evaluation scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc4b94-862e-4c7e-9e29-c4dc3a9ad0e7",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier: J48 (C4.5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f70c5a8-ef0c-47a7-a872-acf3a7e1f8fc",
   "metadata": {},
   "source": [
    "![J48](Weka_results/Seminar2/tree.J48-C0.25-M4-10F.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d43cc2a-fdc5-4ee6-94a4-538050ea8e65",
   "metadata": {},
   "source": [
    "![DecisionTree](Weka_results/Seminar2/tree1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889ea905-bdaf-43fc-b1bb-9e493828172b",
   "metadata": {},
   "source": [
    "![tree](Weka_results/Seminar2/tree2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fcf87c-247d-4128-9608-979d84040271",
   "metadata": {},
   "source": [
    "### Ensemble Classifier: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da593608-25e2-4f79-b1e4-44fbc801df2d",
   "metadata": {},
   "source": [
    "![randomForest](Weka_results/Seminar2/RandomForest1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf0f1ef-7d6b-4e0f-a77c-7cd7ee58db32",
   "metadata": {},
   "source": [
    "![randomForest](Weka_results/Seminar2/RandomForest2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c941f912-a57c-4caf-ae15-015c0dec5527",
   "metadata": {},
   "source": [
    "# Scikit-learn Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c34f7a2c-0b63-47a7-a418-f9d63968b58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>PSP</th>\n",
       "      <th>DSP</th>\n",
       "      <th>DSP_like</th>\n",
       "      <th>ASP</th>\n",
       "      <th>Dinophysis caudata</th>\n",
       "      <th>Dinophysis fortii</th>\n",
       "      <th>...</th>\n",
       "      <th>NO3-N</th>\n",
       "      <th>PO4-P</th>\n",
       "      <th>SiO3-Si</th>\n",
       "      <th>O2</th>\n",
       "      <th>pH</th>\n",
       "      <th>Soca</th>\n",
       "      <th>lipophylic_toxins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>May</td>\n",
       "      <td>1206.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.73</td>\n",
       "      <td>0.15</td>\n",
       "      <td>4.91</td>\n",
       "      <td>230.449997</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1471.231</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>May</td>\n",
       "      <td>4188.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>3.02</td>\n",
       "      <td>250.100006</td>\n",
       "      <td>8.15</td>\n",
       "      <td>1471.231</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>June</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.82</td>\n",
       "      <td>274.209991</td>\n",
       "      <td>8.15</td>\n",
       "      <td>3304.045</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>June</td>\n",
       "      <td>324.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.83</td>\n",
       "      <td>275.109985</td>\n",
       "      <td>8.22</td>\n",
       "      <td>2263.119</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>June</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1873.218</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>December</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.69</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9.21</td>\n",
       "      <td>212.506997</td>\n",
       "      <td>8.17</td>\n",
       "      <td>4071.444</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>December</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5.26</td>\n",
       "      <td>217.990202</td>\n",
       "      <td>8.17</td>\n",
       "      <td>4071.444</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>January</td>\n",
       "      <td>20.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.73</td>\n",
       "      <td>0.07</td>\n",
       "      <td>6.58</td>\n",
       "      <td>237.135858</td>\n",
       "      <td>8.17</td>\n",
       "      <td>1709.376</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>January</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.04</td>\n",
       "      <td>3.68</td>\n",
       "      <td>228.221284</td>\n",
       "      <td>8.19</td>\n",
       "      <td>1709.376</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>January</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.04</td>\n",
       "      <td>3.26</td>\n",
       "      <td>229.621844</td>\n",
       "      <td>8.17</td>\n",
       "      <td>1709.376</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1323 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date     PSP   DSP  DSP_like     ASP  Dinophysis caudata  \\\n",
       "0          May  1206.0  68.0       NaN     NaN                27.0   \n",
       "3          May  4188.0  17.0       NaN     NaN                 8.0   \n",
       "6         June     0.0  27.0       NaN     NaN                16.0   \n",
       "7         June   324.0  23.0       NaN     NaN                 3.0   \n",
       "8         June     0.0  20.0       NaN     NaN                 8.0   \n",
       "...        ...     ...   ...       ...     ...                 ...   \n",
       "1519  December     0.0  20.0       0.0  2500.0                 0.0   \n",
       "1517  December     0.0   0.0       0.0  2100.0                 0.0   \n",
       "1522   January    20.0  40.0       0.0   200.0                10.0   \n",
       "1521   January     0.0   0.0       0.0  2100.0                 0.0   \n",
       "1520   January     0.0  10.0       0.0  1000.0                 0.0   \n",
       "\n",
       "      Dinophysis fortii  ...  NO3-N  PO4-P  SiO3-Si          O2    pH  \\\n",
       "0                   0.0  ...   6.73   0.15     4.91  230.449997  8.00   \n",
       "3                   0.0  ...   1.12   0.12     3.02  250.100006  8.15   \n",
       "6                   3.0  ...   1.23   0.22     1.82  274.209991  8.15   \n",
       "7                   0.0  ...   2.52   0.09     0.83  275.109985  8.22   \n",
       "8                   0.0  ...    NaN    NaN      NaN         NaN   NaN   \n",
       "...                 ...  ...    ...    ...      ...         ...   ...   \n",
       "1519               10.0  ...   4.69   0.10     9.21  212.506997  8.17   \n",
       "1517                0.0  ...   2.34   0.01     5.26  217.990202  8.17   \n",
       "1522               20.0  ...   2.73   0.07     6.58  237.135858  8.17   \n",
       "1521                0.0  ...   1.26   0.04     3.68  228.221284  8.19   \n",
       "1520                0.0  ...   1.23   0.04     3.26  229.621844  8.17   \n",
       "\n",
       "          Soca  lipophylic_toxins  \n",
       "0     1471.231                NaN  \n",
       "3     1471.231                NaN  \n",
       "6     3304.045                NaN  \n",
       "7     2263.119                NaN  \n",
       "8     1873.218                NaN  \n",
       "...        ...                ...  \n",
       "1519  4071.444                neg  \n",
       "1517  4071.444                neg  \n",
       "1522  1709.376                NaN  \n",
       "1521  1709.376                NaN  \n",
       "1520  1709.376                NaN  \n",
       "\n",
       "[1323 rows x 27 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change date to month only\n",
    "df_cons[\"date\"] = df_alg[\"date\"].dt.month_name()\n",
    "\n",
    "# Create df for the sckiti-learn; Remove sampling station, sampling depth, sampling method\n",
    "df_cons_SL = df_cons.drop(columns=[\"sampling station\", \"sampling depth\", \"sampling method\"])\n",
    "df_cons_SL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "068db70e-c3ad-40bd-97f9-0e7284f5e428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neg    232\n",
       "poz     22\n",
       "Name: lipophylic_toxins, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing for NN in scikit_learn\n",
    "# one-hot encoding of date feature\n",
    "df_cons_SL = pd.get_dummies(df_cons_SL, columns=[\"date\"])\n",
    "\n",
    "# Remove missing values\n",
    "df_cons_SL = df_cons_SL.dropna(how=\"any\").copy()\n",
    "\n",
    "# Class distribution\n",
    "df_cons_SL[\"lipophylic_toxins\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91e9a2db-7d7e-4c8d-b618-30fb2f038c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class labels pre-transform: ['neg', 'poz']\n",
      "Class labels post-transform: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# [Continuation...] Preprocessing for NN in scikit_learn\n",
    "X = df_cons_SL.copy().drop(\"lipophylic_toxins\", axis=1)\n",
    "y = df_cons_SL[\"lipophylic_toxins\"]\n",
    "\n",
    "# sklearn lable encoding\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y)\n",
    "print(f\"Class labels pre-transform: {list(le.classes_)}\")\n",
    "y = le.transform(y)\n",
    "print(f\"Class labels post-transform: {np.unique(y)}\")\n",
    "\n",
    "# scalling numeric values\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaled_array = StandardScaler().fit_transform(X)\n",
    "X = pd.DataFrame(scaled_array, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0af6cad3-fc92-481a-8c54-b96321a0b9b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fix class imbalance with Synthetic Minority Oversampling Technique - SMOTE – and undersampling!\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munder_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomUnderSampler\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "# Fix class imbalance with Synthetic Minority Oversampling Technique - SMOTE – and undersampling!\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# summarize class distribution\n",
    "counter = Counter(y)\n",
    "print('Original dataset shape: %s' % Counter(y))\n",
    "\n",
    "# define SMOTE pipeline (oversampling instances with poz (minority class) and undersampling those with neg label)\n",
    "over = SMOTE(sampling_strategy=0.5)\n",
    "under = RandomUnderSampler(sampling_strategy=0.8)\n",
    "steps = [('o', over), ('u', under)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "# transform the dataset\n",
    "X_res, y_res = pipeline.fit_resample(X, y)\n",
    "\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y_res)\n",
    "print('Resampled dataset shape: %s' % Counter(y_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17e63f2-39c7-4ab8-bb0e-22e01991f9e5",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0db8e4c-76d7-4b28-a874-e70f5256bf91",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be1409e-0948-4107-bc34-558b1bb08a35",
   "metadata": {},
   "source": [
    "#### Model evaluation (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81adc9f8-e3a5-4c45-945c-3a51295636be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation with the pipeline of SMOTE oversampling and undersampling on the training dataset only (within each cross-validation fold)!\n",
    "# Evaluate k (SMOTE) parameter. \n",
    "\n",
    "from numpy import mean\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Cross-validation of model with ROC AUC with SMOTE pipeline  \n",
    "\n",
    "\n",
    "# Find best performing k-value for SMOTE\n",
    "k_values = list(range(1,11))\n",
    "RF_auc_best_k = (_, 0)\n",
    "for k in k_values:   \n",
    "    # define pipeline\n",
    "    cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=None, max_features=None)\n",
    "    over = SMOTE(sampling_strategy=0.5, k_neighbors=k)\n",
    "    under = RandomUnderSampler(sampling_strategy=0.8)\n",
    "    steps = [('over', over), ('under', under), ('clf', clf)]\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    scores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "    score = mean(scores)\n",
    "    print('> k=%d, Mean ROC AUC: %.3f' % (k, score))\n",
    "    if score > RF_auc_best_k[1]:\n",
    "        RF_auc_best_k = (k, score)\n",
    "\n",
    "print(f\">>Best k-value: k={RF_auc_best_k[0]} with Mean ROC AUC on resampled dataset: {round(RF_auc_best_k[1], 2)}\")  \n",
    "\n",
    "# Cross-validation of model with ROC AUC without SMOTE pipeline  \n",
    "scores = cross_val_score(clf, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "RF_auc_score = mean(scores)\n",
    "print(f\">>Mean ROC AUC on unsampled dataset: {round(RF_auc_score, 2)}\\n\")\n",
    "\n",
    "# Cross-validation of model with Recall with SMOTE pipeline\n",
    "# Find best performing k-value for SMOTE\n",
    "RF_recall_best_k = (_, 0)\n",
    "for k in k_values:\n",
    "    scores = cross_val_score(pipeline, X, y, scoring='recall', cv=cv, n_jobs=-1)\n",
    "    score = mean(scores)\n",
    "    print('> k=%d, Mean Recall: %.3f' % (k, score))\n",
    "    if score > RF_recall_best_k[1]:\n",
    "        RF_recall_best_k = (k, score)\n",
    "\n",
    "print(f\">>Best k-value: k={RF_recall_best_k[0]} with Mean Recall on resampled dataset: {round(RF_recall_best_k[1], 2)}\")   \n",
    "\n",
    "# Cross-validation of model with Recall without SMOTE pipeline  \n",
    "scores = cross_val_score(clf, X, y, scoring='recall', cv=cv, n_jobs=-1)\n",
    "RF_recall_score = mean(scores)\n",
    "print(f\">>Mean Recall on unsampled dataset: {round(RF_recall_score, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e419d1-f53e-4a68-a922-6ae77cac13e9",
   "metadata": {},
   "source": [
    "#### Feature importance (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2cfb82-7f43-466c-b492-90b58e7c6966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance of model (RandomForest) with three methods!\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "from matplotlib import pyplot as plt\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "plt.subplots_adjust(wspace=1.3)\n",
    "\n",
    "# Split data and fit model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=None, max_features=None)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance with Gini importance (mean decreased impurity)\n",
    "# print(rf.feature_importances_)\n",
    "gini_sorted_idx = rf.feature_importances_.argsort()\n",
    "x1 = X.columns[gini_sorted_idx]\n",
    "y1 = rf.feature_importances_[gini_sorted_idx]\n",
    "ax1.barh(x1, y1)\n",
    "ax1.set_title(\"Gini Feature Importance\")\n",
    "\n",
    "# Get feature importance with Permutation Based Feature Importance (randomly shuffles each feature and compute the \n",
    "# change in the model’s performance. The features which impact the performance the most are the most important one).\n",
    "perm_importance = permutation_importance(rf, X_test, y_test)\n",
    "perm_sorted_idx = perm_importance.importances_mean.argsort()\n",
    "x2 = X.columns[perm_sorted_idx]\n",
    "y2 = perm_importance.importances_mean[perm_sorted_idx]\n",
    "ax2.barh(x2, y2)\n",
    "ax2.set_title(\"Permutation Importance Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0547d681-1201-4853-a89a-006a0efddb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance with SHAP\n",
    "explainer = shap.TreeExplainer(rf)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "RF_shap = shap.summary_plot(shap_values, X_test, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ca727-c52f-412b-a499-d6e9cdee947f",
   "metadata": {},
   "source": [
    "### Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc6e6b4-1a36-47c3-92c2-51a1340b79c1",
   "metadata": {},
   "source": [
    "#### Model Evaluation (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70de528-5c6d-428b-a0f4-890707fc436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation with the pipeline of SMOTE oversampling and undersampling on the training dataset only (within each cross-validation fold)!\n",
    "# Evaluate k (SMOTE) parameter. \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Cross-validation of model with ROC AUC with SMOTE pipeline  \n",
    "# define pipeline\n",
    "clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(20,10), max_iter=3000, random_state=1)\n",
    "over = SMOTE(sampling_strategy=0.5, k_neighbors=k)\n",
    "under = RandomUnderSampler(sampling_strategy=0.8)\n",
    "steps = [('over', over), ('under', under), ('clf', clf)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n",
    "\n",
    "# Find best performing k-value for SMOTE\n",
    "k_values = list(range(1,11))\n",
    "MLP_auc_best_k = (_, 0)\n",
    "for k in k_values:\n",
    "    # evaluate pipeline\n",
    "    scores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "    score = mean(scores)\n",
    "    print('> k=%d, Mean ROC AUC: %.3f' % (k, score))\n",
    "    if score > MLP_auc_best_k[1]:\n",
    "        MLP_auc_best_k = (k, score)\n",
    "\n",
    "print(f\">>Best k: k={MLP_auc_best_k[0]} with Mean ROC AUC on resampled dataset: {round(MLP_auc_best_k[1], 2)}\")  \n",
    "\n",
    "# Cross-validation of model with ROC AUC without SMOTE pipeline  \n",
    "scores = cross_val_score(clf, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "MLP_auc_score = mean(scores)\n",
    "print(f\">>Mean ROC AUC on unsampled dataset: {round(MLP_auc_score, 2)}\\n\")\n",
    "\n",
    "# Cross-validation of model with Recall with SMOTE pipeline\n",
    "# Find best performing k-value for SMOTE\n",
    "MLP_recall_best_k = (_, 0)\n",
    "for k in k_values:\n",
    "    # evaluate pipeline\n",
    "    scores = cross_val_score(pipeline, X, y, scoring='recall', cv=cv, n_jobs=-1)\n",
    "    score = mean(scores)\n",
    "    print('> k=%d, Mean Recall: %.3f' % (k, score))\n",
    "    if score > MLP_recall_best_k[1]:\n",
    "        MLP_recall_best_k = (k, score)\n",
    "\n",
    "print(f\">>Best k: k={MLP_recall_best_k[0]} with Mean Recall on unsampled dataset: {round(MLP_recall_best_k[1], 2)}\")  \n",
    "\n",
    "# Cross-validation of model with Recall without SMOTE pipeline  \n",
    "scores = cross_val_score(clf, X, y, scoring='recall', cv=cv, n_jobs=-1)\n",
    "MLP_recall_score = mean(scores)\n",
    "print(f\">>Mean Recall on unsampled dataset: {round(MLP_recall_score, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6fde10-21d5-46f4-ba80-1e97576ee06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "NN = MLPClassifier(solver='lbfgs', max_iter=3000)\n",
    "parameters = {\"hidden_layer_sizes\": [(3,), (3, 3), (3, 3, 3)]}\n",
    "gs = GridSearchCV(NN, parameters, scoring=\"roc_auc\", cv=3) # \"recall\"\n",
    "gs.fit(X_res, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a859cff-3f5c-4b51-a7d5-e218ec84b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc62c99-c2aa-451e-90ca-a46ea80a22f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ba333f-3931-4bf7-ae33-757478d69a30",
   "metadata": {},
   "source": [
    "#### Feature Importance (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f68acd-b343-48c6-a372-d46ed52e6951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance of model (RandomForest) with three methods (no cross-validation!)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "from matplotlib import pyplot as plt\n",
    "fig, (ax2) = plt.subplots(1, 1)\n",
    "plt.subplots_adjust(wspace=1.3)\n",
    "\n",
    "# Split data and fit model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "MLP = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(20,10), max_iter=3000, random_state=1)\n",
    "MLP.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance with Permutation Based Feature Importance (randomly shuffles each feature and compute the \n",
    "# change in the model’s performance. The features which impact the performance the most are the most important one).\n",
    "perm_importance = permutation_importance(MLP, X_test, y_test)\n",
    "perm_sorted_idx = perm_importance.importances_mean.argsort()\n",
    "x2 = X.columns[perm_sorted_idx]\n",
    "y2 = perm_importance.importances_mean[perm_sorted_idx]\n",
    "ax2.barh(x2, y2)\n",
    "ax2.set_title(\"Permutation Importance MLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ecb324-cec5-4dd6-9614-84fb0d24a3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain the model's predictions using SHAP\n",
    "import shap\n",
    "# explainer = shap.KernelExplainer(clf.predict_proba, X_train)\n",
    "explainer = shap.KernelExplainer(MLP.predict_proba, shap.sample(pd.DataFrame(X_train, columns=X.columns), 100))\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905e54ff-7f73-43c9-b5ee-0fe11ec4ca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the first prediction's explanation (Shapley value is the average contribution of features which are predicting in different situation).\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9f3aca-b6f0-4ab4-bd20-b828301bfdb2",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edb6f9a-7038-4f5a-9a98-914a7d0e7557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table of prediction results\n",
    "RF_recall = round(RF_recall_best_k[1], 2)\n",
    "RF_auc = round(RF_auc_best_k[1], 2)\n",
    "MLP_recall = round(MLP_recall_best_k[1], 2)\n",
    "MLP_auc = round(MLP_auc_best_k[1], 2)\n",
    "\n",
    "summary = pd.DataFrame(\n",
    "    [\n",
    "        (\n",
    "            \"RF\",\n",
    "            RF_recall_score,\n",
    "            RF_auc_score,\n",
    "        ),\n",
    "        (\n",
    "            \"MLP\",\n",
    "            MLP_recall_score,\n",
    "            MLP_auc_score,\n",
    "        ),\n",
    "        (\n",
    "            \"RF (smote)\",\n",
    "            RF_recall,\n",
    "            RF_auc,\n",
    "        ),\n",
    "        (\n",
    "            \"MLP (smote)\",\n",
    "            MLP_recall,\n",
    "            MLP_auc,\n",
    "        ),\n",
    "        (\n",
    "            \"Decision tree (J48)*\",\n",
    "            0.56,\n",
    "            0.18,\n",
    "        ),\n",
    "    ],\n",
    "    columns=(\"Model\", \"Recall\", \"ROC AUC\"),\n",
    ").set_index(\"Model\")\n",
    "\n",
    "print(\"Table summarising the prediction results of the used classifiers, both with and without SMOTE resampling:\\n\")\n",
    "summary.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c712a49-d47a-456a-9905-50a6502a4be2",
   "metadata": {},
   "source": [
    "As can be seen resampling with SMOTE helped to improve the results substantially, especially when calculating recall. The highest recall and ROC AUC was achieved with Random Forest with the re-sampled data. Both recall and ROC AUC suggest Random Forest as beeing the better classifier for this particular problem. Recall is a crucial metric as it gives indication of what fraction of true positive instances have been predicted. Since the models predict toxins in seashells (food) it is crucial that as few positives as possible are missed.\n",
    "\n",
    "Due to the use of SMOTE resampling (upsampling and downsampling) in combinaiton with cross-validation it was curcial to do the resampling within each fold to avoid data lekeage and validate on original (unsampled) data. In addition, I have optimised the model with regard to the k-values of SMOTE, all of which brought along some complexity. So for the parameter tuning of Random Forest and MLP various parameter settings have been tried  and the model with best performing settings has been chosen.\n",
    "\n",
    "The decision tree J48 algorithm was run within Weka on a slightly different dataset (missing values were not removed to use as many instances as possible, cross validation was 10-fold as opposed to 3-fold due to a higher dataset etc.) thus this results are not directly comparable but were provided as a reference to give an indication of the performance of this algorithm. \n",
    "\n",
    "As can be seen in the feature importance bar plots above, similar features were on the top despite using two different classification algorithms and two different feature ranking methods. If we consider just the three highest-ranking features of each of the feature ranking methods for both algortihms (RF and MLP) the features that overlap are DSP, DSP_like, ASP, Dinophysis fortii and Dinophysis caudata. These can be shown to the domain experts for validation and interpretation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
