{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57d854a4-aa15-40da-bd45-4df04e0043ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from dateutil.parser import parse\n",
    "import datetime\n",
    "from dateutil.parser import parse\n",
    "import math\n",
    "from numpy import mean\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold, GridSearchCV, cross_validate, StratifiedKFold\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline as SKLpipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.tree import export_text\n",
    "from dtreeviz.trees import dtreeviz \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as IMBLpipeline\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import ipynbname\n",
    "notebook_name = ipynbname.name()\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 30)\n",
    "pd.set_option(\"display.max_columns\", 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d530319-a99e-4824-982c-5d531a8ac548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DSP                        1\n",
       "Dinophysis caudata         1\n",
       "Dinophysis fortii          1\n",
       "Phalacroma rotundatum      1\n",
       "Dinophysis sacculus        1\n",
       "Dinophysis tripos          1\n",
       "sun [h]                    0\n",
       "air temp                   0\n",
       "wind strength              0\n",
       "precipitation              0\n",
       "Chl-a                    422\n",
       "salinity                  21\n",
       "T                         59\n",
       "SECCHI                   450\n",
       "DIN                      352\n",
       "PO4-P                    349\n",
       "Soca                       0\n",
       "month                      0\n",
       "lipophylic_toxins        320\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read df pickle\n",
    "df_alg = pd.read_pickle(\"objects/df_alg-HAB_preprocessing_5_1\")\n",
    "# data = pd.read_pickle(\"data/preprocessed/hab_org-data-HAB_part2-preprocessing-5_2\")\n",
    "data = pd.read_pickle(\"data/preprocessed/hab_interp_data-HAB_part2-preprocessing-5_2\")\n",
    "\n",
    "data.drop(columns=[\"sampling station\", \"date\"], inplace=True)\n",
    "# data.set_index('date', inplace=True)\n",
    "\n",
    "\n",
    "# slice by station and time\n",
    "# data = data[data[\"sampling station\"] == \"Debeli_rtic\"].loc[\"2008-01-01\" : \"2021-12-31\"]\n",
    "\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4ce4a24-a8cc-4258-9d9b-3b925dc86a52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neg    996\n",
       "NaN    320\n",
       "poz    136\n",
       "Name: lipophylic_toxins, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class distribution\n",
    "data[\"lipophylic_toxins\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9945a3bf-fecf-4702-863e-7d4f489e41a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move month to first place\n",
    "cols = data.columns.tolist()  # Get a list of column names\n",
    "cols = [cols[-2]] + cols[:-2] + [cols[-1]]  # Move the one before the last column to the first position\n",
    "data = data[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e54e5716-91c5-48f2-b201-73f7444bd46c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "month                      0\n",
       "DSP                        1\n",
       "Dinophysis caudata         1\n",
       "Dinophysis fortii          1\n",
       "Phalacroma rotundatum      1\n",
       "Dinophysis sacculus        1\n",
       "Dinophysis tripos          1\n",
       "sun [h]                    0\n",
       "air temp                   0\n",
       "wind strength              0\n",
       "precipitation              0\n",
       "Chl-a                    422\n",
       "salinity                  21\n",
       "T                         59\n",
       "DIN                      352\n",
       "PO4-P                    349\n",
       "Soca                       0\n",
       "lipophylic_toxins        320\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data.drop(columns=[\"Chl-a\",\"PO4-P\",\"DIN\",\"SECCHI\"], inplace=True)\n",
    "data.drop(columns=[\"SECCHI\",  ], inplace=True)#,\"Chl-a\", \"PO4-P\", \"DIN\",\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45d93c2-6bcb-4298-bca9-f268e6330756",
   "metadata": {},
   "source": [
    "# Descriptive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73b54946-f380-4fd8-9d45-1ea70fd7965f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>DSP</th>\n",
       "      <th>Dinophysis caudata</th>\n",
       "      <th>Dinophysis fortii</th>\n",
       "      <th>Phalacroma rotundatum</th>\n",
       "      <th>Dinophysis sacculus</th>\n",
       "      <th>Dinophysis tripos</th>\n",
       "      <th>sun [h]</th>\n",
       "      <th>air temp</th>\n",
       "      <th>wind strength</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>Chl-a</th>\n",
       "      <th>salinity</th>\n",
       "      <th>T</th>\n",
       "      <th>DIN</th>\n",
       "      <th>PO4-P</th>\n",
       "      <th>Soca</th>\n",
       "      <th>lipophylic_toxins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1452.00</td>\n",
       "      <td>1451.00</td>\n",
       "      <td>1451.00</td>\n",
       "      <td>1451.00</td>\n",
       "      <td>1451.00</td>\n",
       "      <td>1451.00</td>\n",
       "      <td>1451.00</td>\n",
       "      <td>1452.00</td>\n",
       "      <td>1452.00</td>\n",
       "      <td>1452.00</td>\n",
       "      <td>1452.00</td>\n",
       "      <td>1030.00</td>\n",
       "      <td>1431.00</td>\n",
       "      <td>1393.00</td>\n",
       "      <td>1100.00</td>\n",
       "      <td>1103.00</td>\n",
       "      <td>1452.00</td>\n",
       "      <td>1132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.38</td>\n",
       "      <td>105.75</td>\n",
       "      <td>24.92</td>\n",
       "      <td>26.15</td>\n",
       "      <td>14.20</td>\n",
       "      <td>28.55</td>\n",
       "      <td>4.50</td>\n",
       "      <td>158.80</td>\n",
       "      <td>16.60</td>\n",
       "      <td>2.99</td>\n",
       "      <td>57.58</td>\n",
       "      <td>0.85</td>\n",
       "      <td>37.08</td>\n",
       "      <td>19.46</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.06</td>\n",
       "      <td>3294.64</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.82</td>\n",
       "      <td>339.89</td>\n",
       "      <td>87.92</td>\n",
       "      <td>160.06</td>\n",
       "      <td>29.93</td>\n",
       "      <td>177.90</td>\n",
       "      <td>41.18</td>\n",
       "      <td>60.63</td>\n",
       "      <td>6.06</td>\n",
       "      <td>0.41</td>\n",
       "      <td>50.71</td>\n",
       "      <td>0.71</td>\n",
       "      <td>25.50</td>\n",
       "      <td>5.33</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2329.62</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.80</td>\n",
       "      <td>-0.82</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>24.13</td>\n",
       "      <td>6.24</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>593.92</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>112.60</td>\n",
       "      <td>12.18</td>\n",
       "      <td>2.74</td>\n",
       "      <td>19.68</td>\n",
       "      <td>0.39</td>\n",
       "      <td>35.85</td>\n",
       "      <td>15.50</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1636.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>161.40</td>\n",
       "      <td>17.44</td>\n",
       "      <td>2.98</td>\n",
       "      <td>44.70</td>\n",
       "      <td>0.68</td>\n",
       "      <td>36.89</td>\n",
       "      <td>20.47</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2580.82</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10.00</td>\n",
       "      <td>90.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>207.30</td>\n",
       "      <td>21.74</td>\n",
       "      <td>3.21</td>\n",
       "      <td>79.00</td>\n",
       "      <td>1.09</td>\n",
       "      <td>37.48</td>\n",
       "      <td>23.91</td>\n",
       "      <td>3.96</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4236.82</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12.00</td>\n",
       "      <td>7630.00</td>\n",
       "      <td>1309.00</td>\n",
       "      <td>4624.00</td>\n",
       "      <td>393.00</td>\n",
       "      <td>4639.00</td>\n",
       "      <td>1139.00</td>\n",
       "      <td>277.80</td>\n",
       "      <td>26.57</td>\n",
       "      <td>5.26</td>\n",
       "      <td>267.70</td>\n",
       "      <td>9.25</td>\n",
       "      <td>999.00</td>\n",
       "      <td>28.37</td>\n",
       "      <td>35.47</td>\n",
       "      <td>3.54</td>\n",
       "      <td>16039.87</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing_values</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>422.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>59.00</td>\n",
       "      <td>352.00</td>\n",
       "      <td>349.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  month      DSP  Dinophysis caudata  Dinophysis fortii  \\\n",
       "count           1452.00  1451.00             1451.00            1451.00   \n",
       "mean               7.38   105.75               24.92              26.15   \n",
       "std                2.82   339.89               87.92             160.06   \n",
       "min                1.00     0.00                0.00               0.00   \n",
       "25%                5.00    10.00                0.00               0.00   \n",
       "50%                8.00    37.00                0.00               0.00   \n",
       "75%               10.00    90.00               13.00              10.00   \n",
       "max               12.00  7630.00             1309.00            4624.00   \n",
       "missing_values     0.00     1.00                1.00               1.00   \n",
       "\n",
       "                Phalacroma rotundatum  Dinophysis sacculus  Dinophysis tripos  \\\n",
       "count                         1451.00              1451.00            1451.00   \n",
       "mean                            14.20                28.55               4.50   \n",
       "std                             29.93               177.90              41.18   \n",
       "min                              0.00                 0.00               0.00   \n",
       "25%                              0.00                 0.00               0.00   \n",
       "50%                             10.00                 0.00               0.00   \n",
       "75%                             20.00                10.00               0.00   \n",
       "max                            393.00              4639.00            1139.00   \n",
       "missing_values                   1.00                 1.00               1.00   \n",
       "\n",
       "                sun [h]  air temp  wind strength  precipitation    Chl-a  \\\n",
       "count           1452.00   1452.00        1452.00        1452.00  1030.00   \n",
       "mean             158.80     16.60           2.99          57.58     0.85   \n",
       "std               60.63      6.06           0.41          50.71     0.71   \n",
       "min               22.80     -0.82           1.48           0.00     0.09   \n",
       "25%              112.60     12.18           2.74          19.68     0.39   \n",
       "50%              161.40     17.44           2.98          44.70     0.68   \n",
       "75%              207.30     21.74           3.21          79.00     1.09   \n",
       "max              277.80     26.57           5.26         267.70     9.25   \n",
       "missing_values     0.00      0.00           0.00           0.00   422.00   \n",
       "\n",
       "                salinity        T      DIN    PO4-P      Soca  \\\n",
       "count            1431.00  1393.00  1100.00  1103.00   1452.00   \n",
       "mean               37.08    19.46     3.29     0.06   3294.64   \n",
       "std                25.50     5.33     3.94     0.16   2329.62   \n",
       "min                24.13     6.24     0.06     0.00    593.92   \n",
       "25%                35.85    15.50     0.87     0.02   1636.00   \n",
       "50%                36.89    20.47     1.98     0.04   2580.82   \n",
       "75%                37.48    23.91     3.96     0.08   4236.82   \n",
       "max               999.00    28.37    35.47     3.54  16039.87   \n",
       "missing_values     21.00    59.00   352.00   349.00      0.00   \n",
       "\n",
       "               lipophylic_toxins  \n",
       "count                       1132  \n",
       "mean                         NaN  \n",
       "std                          NaN  \n",
       "min                          NaN  \n",
       "25%                          NaN  \n",
       "50%                          NaN  \n",
       "75%                          NaN  \n",
       "max                          NaN  \n",
       "missing_values               320  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# describe\n",
    "description = data.describe(include='all').round(2)\n",
    "\n",
    "# Calculate the number of missing values for each column\n",
    "missing_values = data.isna().sum()\n",
    "missing_values.name = 'missing_values'\n",
    "\n",
    "# Append the missing_values row to the description DataFrame\n",
    "description_with_missing = description.append(missing_values)\n",
    "description_with_missing = description_with_missing.drop(['unique', 'top', 'freq'])\n",
    "\n",
    "description_with_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7501e318-06de-4e74-ac3e-04965a6ccd58",
   "metadata": {},
   "source": [
    "## Descriptive analysis by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3c39a42-f94a-476e-9e49-1962461bf7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DSP</th>\n",
       "      <th>Dinophysis caudata</th>\n",
       "      <th>Dinophysis fortii</th>\n",
       "      <th>Phalacroma rotundatum</th>\n",
       "      <th>Dinophysis sacculus</th>\n",
       "      <th>Dinophysis tripos</th>\n",
       "      <th>sun [h]</th>\n",
       "      <th>air temp</th>\n",
       "      <th>wind strength</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>Chl-a</th>\n",
       "      <th>salinity</th>\n",
       "      <th>T</th>\n",
       "      <th>DIN</th>\n",
       "      <th>PO4-P</th>\n",
       "      <th>Soca</th>\n",
       "      <th>neg</th>\n",
       "      <th>poz</th>\n",
       "      <th>poz %</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>January</th>\n",
       "      <td>9.00</td>\n",
       "      <td>1.16</td>\n",
       "      <td>2.96</td>\n",
       "      <td>3.66</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.20</td>\n",
       "      <td>72.62</td>\n",
       "      <td>4.95</td>\n",
       "      <td>2.71</td>\n",
       "      <td>46.53</td>\n",
       "      <td>0.76</td>\n",
       "      <td>36.95</td>\n",
       "      <td>11.07</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4652.16</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>February</th>\n",
       "      <td>12.86</td>\n",
       "      <td>1.02</td>\n",
       "      <td>4.08</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.69</td>\n",
       "      <td>80.19</td>\n",
       "      <td>5.52</td>\n",
       "      <td>3.06</td>\n",
       "      <td>60.66</td>\n",
       "      <td>0.77</td>\n",
       "      <td>37.02</td>\n",
       "      <td>10.22</td>\n",
       "      <td>6.66</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4378.79</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>March</th>\n",
       "      <td>12.38</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.68</td>\n",
       "      <td>3.79</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.64</td>\n",
       "      <td>118.56</td>\n",
       "      <td>7.55</td>\n",
       "      <td>3.30</td>\n",
       "      <td>30.01</td>\n",
       "      <td>0.77</td>\n",
       "      <td>37.17</td>\n",
       "      <td>9.49</td>\n",
       "      <td>4.37</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3257.07</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>April</th>\n",
       "      <td>15.49</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.48</td>\n",
       "      <td>5.24</td>\n",
       "      <td>4.51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>144.63</td>\n",
       "      <td>11.69</td>\n",
       "      <td>3.17</td>\n",
       "      <td>54.90</td>\n",
       "      <td>0.68</td>\n",
       "      <td>36.93</td>\n",
       "      <td>11.68</td>\n",
       "      <td>4.68</td>\n",
       "      <td>0.06</td>\n",
       "      <td>3826.10</td>\n",
       "      <td>66</td>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>May</th>\n",
       "      <td>59.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.97</td>\n",
       "      <td>9.67</td>\n",
       "      <td>34.25</td>\n",
       "      <td>0.15</td>\n",
       "      <td>171.96</td>\n",
       "      <td>16.22</td>\n",
       "      <td>2.89</td>\n",
       "      <td>48.55</td>\n",
       "      <td>0.96</td>\n",
       "      <td>36.57</td>\n",
       "      <td>14.87</td>\n",
       "      <td>3.92</td>\n",
       "      <td>0.07</td>\n",
       "      <td>3864.97</td>\n",
       "      <td>89</td>\n",
       "      <td>14</td>\n",
       "      <td>10.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>June</th>\n",
       "      <td>146.89</td>\n",
       "      <td>29.23</td>\n",
       "      <td>1.49</td>\n",
       "      <td>12.87</td>\n",
       "      <td>93.28</td>\n",
       "      <td>0.09</td>\n",
       "      <td>202.85</td>\n",
       "      <td>20.51</td>\n",
       "      <td>2.83</td>\n",
       "      <td>45.16</td>\n",
       "      <td>1.09</td>\n",
       "      <td>35.60</td>\n",
       "      <td>20.28</td>\n",
       "      <td>3.58</td>\n",
       "      <td>0.13</td>\n",
       "      <td>3436.93</td>\n",
       "      <td>115</td>\n",
       "      <td>12</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>July</th>\n",
       "      <td>237.04</td>\n",
       "      <td>67.07</td>\n",
       "      <td>44.50</td>\n",
       "      <td>19.57</td>\n",
       "      <td>97.25</td>\n",
       "      <td>0.04</td>\n",
       "      <td>227.47</td>\n",
       "      <td>23.31</td>\n",
       "      <td>2.88</td>\n",
       "      <td>38.27</td>\n",
       "      <td>0.75</td>\n",
       "      <td>35.40</td>\n",
       "      <td>23.48</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2473.18</td>\n",
       "      <td>125</td>\n",
       "      <td>18</td>\n",
       "      <td>13.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>August</th>\n",
       "      <td>129.67</td>\n",
       "      <td>55.46</td>\n",
       "      <td>24.23</td>\n",
       "      <td>19.01</td>\n",
       "      <td>22.96</td>\n",
       "      <td>0.08</td>\n",
       "      <td>220.46</td>\n",
       "      <td>23.46</td>\n",
       "      <td>3.03</td>\n",
       "      <td>54.38</td>\n",
       "      <td>0.63</td>\n",
       "      <td>41.92</td>\n",
       "      <td>24.93</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1637.28</td>\n",
       "      <td>126</td>\n",
       "      <td>11</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>September</th>\n",
       "      <td>116.79</td>\n",
       "      <td>28.04</td>\n",
       "      <td>58.58</td>\n",
       "      <td>12.81</td>\n",
       "      <td>3.57</td>\n",
       "      <td>6.04</td>\n",
       "      <td>174.65</td>\n",
       "      <td>20.06</td>\n",
       "      <td>3.11</td>\n",
       "      <td>69.73</td>\n",
       "      <td>0.64</td>\n",
       "      <td>36.59</td>\n",
       "      <td>24.38</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2102.01</td>\n",
       "      <td>143</td>\n",
       "      <td>31</td>\n",
       "      <td>22.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>October</th>\n",
       "      <td>97.76</td>\n",
       "      <td>13.21</td>\n",
       "      <td>35.56</td>\n",
       "      <td>19.39</td>\n",
       "      <td>2.17</td>\n",
       "      <td>19.76</td>\n",
       "      <td>127.60</td>\n",
       "      <td>15.56</td>\n",
       "      <td>3.08</td>\n",
       "      <td>72.07</td>\n",
       "      <td>0.80</td>\n",
       "      <td>36.91</td>\n",
       "      <td>21.70</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2988.28</td>\n",
       "      <td>138</td>\n",
       "      <td>24</td>\n",
       "      <td>17.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>November</th>\n",
       "      <td>80.54</td>\n",
       "      <td>13.81</td>\n",
       "      <td>29.33</td>\n",
       "      <td>19.09</td>\n",
       "      <td>1.11</td>\n",
       "      <td>8.90</td>\n",
       "      <td>82.48</td>\n",
       "      <td>11.35</td>\n",
       "      <td>3.01</td>\n",
       "      <td>74.21</td>\n",
       "      <td>1.20</td>\n",
       "      <td>36.72</td>\n",
       "      <td>17.61</td>\n",
       "      <td>3.09</td>\n",
       "      <td>0.06</td>\n",
       "      <td>5263.97</td>\n",
       "      <td>91</td>\n",
       "      <td>13</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>December</th>\n",
       "      <td>81.96</td>\n",
       "      <td>2.33</td>\n",
       "      <td>51.95</td>\n",
       "      <td>16.06</td>\n",
       "      <td>1.18</td>\n",
       "      <td>4.77</td>\n",
       "      <td>65.97</td>\n",
       "      <td>7.51</td>\n",
       "      <td>2.83</td>\n",
       "      <td>81.32</td>\n",
       "      <td>1.20</td>\n",
       "      <td>36.81</td>\n",
       "      <td>15.56</td>\n",
       "      <td>4.09</td>\n",
       "      <td>0.06</td>\n",
       "      <td>5868.74</td>\n",
       "      <td>39</td>\n",
       "      <td>3</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              DSP  Dinophysis caudata  Dinophysis fortii  \\\n",
       "month                                                      \n",
       "January      9.00                1.16               2.96   \n",
       "February    12.86                1.02               4.08   \n",
       "March       12.38                0.21               0.68   \n",
       "April       15.49                0.97               0.48   \n",
       "May         59.00                6.00               0.97   \n",
       "June       146.89               29.23               1.49   \n",
       "July       237.04               67.07              44.50   \n",
       "August     129.67               55.46              24.23   \n",
       "September  116.79               28.04              58.58   \n",
       "October     97.76               13.21              35.56   \n",
       "November    80.54               13.81              29.33   \n",
       "December    81.96                2.33              51.95   \n",
       "\n",
       "           Phalacroma rotundatum  Dinophysis sacculus  Dinophysis tripos  \\\n",
       "month                                                                      \n",
       "January                     3.66                 0.09               0.20   \n",
       "February                    3.38                 0.20               0.69   \n",
       "March                       3.79                 0.43               0.64   \n",
       "April                       5.24                 4.51               0.00   \n",
       "May                         9.67                34.25               0.15   \n",
       "June                       12.87                93.28               0.09   \n",
       "July                       19.57                97.25               0.04   \n",
       "August                     19.01                22.96               0.08   \n",
       "September                  12.81                 3.57               6.04   \n",
       "October                    19.39                 2.17              19.76   \n",
       "November                   19.09                 1.11               8.90   \n",
       "December                   16.06                 1.18               4.77   \n",
       "\n",
       "           sun [h]  air temp  wind strength  precipitation  Chl-a  salinity  \\\n",
       "month                                                                         \n",
       "January      72.62      4.95           2.71          46.53   0.76     36.95   \n",
       "February     80.19      5.52           3.06          60.66   0.77     37.02   \n",
       "March       118.56      7.55           3.30          30.01   0.77     37.17   \n",
       "April       144.63     11.69           3.17          54.90   0.68     36.93   \n",
       "May         171.96     16.22           2.89          48.55   0.96     36.57   \n",
       "June        202.85     20.51           2.83          45.16   1.09     35.60   \n",
       "July        227.47     23.31           2.88          38.27   0.75     35.40   \n",
       "August      220.46     23.46           3.03          54.38   0.63     41.92   \n",
       "September   174.65     20.06           3.11          69.73   0.64     36.59   \n",
       "October     127.60     15.56           3.08          72.07   0.80     36.91   \n",
       "November     82.48     11.35           3.01          74.21   1.20     36.72   \n",
       "December     65.97      7.51           2.83          81.32   1.20     36.81   \n",
       "\n",
       "               T   DIN  PO4-P     Soca  neg  poz  poz %  \n",
       "month                                                    \n",
       "January    11.07  6.79   0.07  4652.16   15    3    2.2  \n",
       "February   10.22  6.66   0.08  4378.79   21    3    2.2  \n",
       "March       9.49  4.37   0.05  3257.07   28    2    1.5  \n",
       "April      11.68  4.68   0.06  3826.10   66    2    1.5  \n",
       "May        14.87  3.92   0.07  3864.97   89   14   10.3  \n",
       "June       20.28  3.58   0.13  3436.93  115   12    8.8  \n",
       "July       23.48  3.38   0.05  2473.18  125   18   13.2  \n",
       "August     24.93  2.03   0.05  1637.28  126   11    8.1  \n",
       "September  24.38  2.06   0.05  2102.01  143   31   22.8  \n",
       "October    21.70  1.95   0.05  2988.28  138   24   17.6  \n",
       "November   17.61  3.09   0.06  5263.97   91   13    9.6  \n",
       "December   15.56  4.09   0.06  5868.74   39    3    2.2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# table of mean values for each feature by month \n",
    "import calendar\n",
    "\n",
    "grouped_means = data.groupby('month').mean()\n",
    "\n",
    "# Count binary values for the categorical feature grouped by month\n",
    "binary_counts = data.groupby('month')['lipophylic_toxins'].value_counts().unstack()\n",
    "\n",
    "# Calculate the ratio of positive values for each month\n",
    "sum_positive = binary_counts[\"poz\"].sum()\n",
    "positive_ratios = [i for i in (binary_counts[\"poz\"] / sum_positive)] \n",
    "positive_ratios = [round(v * 100, 1) for v in positive_ratios]\n",
    "\n",
    "# Change month names\n",
    "month_names = {i: calendar.month_name[i] for i in range(1, 13)}\n",
    "\n",
    "# Update the index using the month_names dictionary\n",
    "grouped_means.index = grouped_means.index.map(month_names)\n",
    "binary_counts.index = binary_counts.index.map(month_names)\n",
    "\n",
    "# Concatenate the grouped_means and binary_counts DataFrames\n",
    "result = pd.concat([grouped_means, binary_counts], axis=1).round(2)\n",
    "\n",
    "# Add the positive_ratios Series as a new column to the result DataFrame\n",
    "result[\"poz %\"] = positive_ratios\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c941f912-a57c-4caf-ae15-015c0dec5527",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Scikit-learn Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeda4953-b0aa-423e-b844-606188480710",
   "metadata": {},
   "source": [
    "# Data preprocessing for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc307518-a30c-493f-a0a9-888382107aa9",
   "metadata": {},
   "source": [
    "### Removing instances with unlabeled target, label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67f0106d-3b54-4623-8102-48ba8abd2460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class distribution:\n",
      "neg    662\n",
      "poz     88\n",
      "Name: lipophylic_toxins, dtype: int64\n",
      "class encoding: ['neg','poz'] -> [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Prepare for ML in scikit-learn\n",
    "# labeled and unlabeled part\n",
    "data_l = data[data['lipophylic_toxins'].notnull()]\n",
    "data_ul = data[data['lipophylic_toxins'].isnull()]\n",
    "\n",
    "# Remove missing values\n",
    "data_l = data_l.dropna(how=\"any\")\n",
    "print(f\"class distribution:\")\n",
    "print(data_l[\"lipophylic_toxins\"].value_counts(dropna=False))\n",
    "\n",
    "X = data_l.drop(\"lipophylic_toxins\", axis=1)\n",
    "y = data_l[\"lipophylic_toxins\"]\n",
    "\n",
    "# sklearn lable encoding\n",
    "le = LabelEncoder()\n",
    "le.fit(y)\n",
    "y = le.transform(y)\n",
    "print(f\"class encoding: ['neg','poz'] -> {le.transform(['neg','poz'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4e96a9-b60a-4ac7-9efe-2780a5c9ef12",
   "metadata": {},
   "source": [
    "### Clean instances close to the decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cbf75b-2474-48d0-b650-6c60d21d4829",
   "metadata": {},
   "source": [
    "Clean the dataset by removing samples close to the decision boundary. Because the dataset is heavily imbalanced in favor of clas 0 (neg) we will remove instances from this class whenever finding samples which do not agree “enough” with their neighboorhood. The EditedNearestNeighbours will be used. One other option is to use Tomek links but it is more conservative and was found to perform slightly worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b9e6e31-35f7-41b5-a9b0-a1f1876c9a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: Counter({0: 662, 1: 88})\n",
      "Resampled dataset shape Counter({0: 540, 1: 88})\n",
      "Resampled dataset shape Counter({0: 501, 1: 88})\n",
      "Resampled dataset shape Counter({0: 490, 1: 88})\n",
      "Resampled dataset shape Counter({0: 488, 1: 88})\n",
      "Resampled dataset shape Counter({0: 487, 1: 88})\n",
      "Cannot remove any more samples\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours\n",
    "\n",
    "print(f'Original dataset shape: {Counter(y)}')\n",
    "usmp = EditedNearestNeighbours()\n",
    "lastMajorityCount = Counter(y)[0]\n",
    "for i in range(10):\n",
    "    X_res, y_res = usmp.fit_resample(X, y)\n",
    "    if Counter(y_res)[0] == lastMajorityCount:\n",
    "        print('Cannot remove any more samples')\n",
    "        break\n",
    "    else:\n",
    "        print(f'Resampled dataset shape {Counter(y_res)}')\n",
    "        lastMajorityCount = Counter(y_res)[0]\n",
    "    X = X_res\n",
    "    y = y_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d58759b-b41a-4938-9acb-c736a6016b63",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d406548-0ce8-44bf-9ba2-6311c8c9c2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X, X_eval, y, y_eval = train_test_split(X, y, shuffle=True, stratify=y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e68a779-d7ad-4bf9-b597-594cfbf760e6",
   "metadata": {},
   "source": [
    "# Correlation analysis on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e93f704-b7e9-4383-bf61-a15e4e4080ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Pearson  Spearman\n",
      "DSP                    0.050517  0.112360\n",
      "T                      0.133431  0.108940\n",
      "Phalacroma rotundatum  0.027626  0.078264\n",
      "Chl-a                 -0.081980 -0.067924\n",
      "air temp               0.101362  0.061298\n",
      "month                  0.079418  0.060695\n",
      "Dinophysis caudata     0.159930  0.055140\n",
      "DIN                    0.048785  0.038620\n",
      "PO4-P                 -0.046319 -0.037694\n",
      "Dinophysis sacculus   -0.012564 -0.037541\n",
      "precipitation          0.032200  0.036034\n",
      "wind strength          0.003026  0.035853\n",
      "sun [h]                0.048582  0.035070\n",
      "salinity              -0.055177 -0.025085\n",
      "Dinophysis tripos      0.076581  0.018183\n",
      "Soca                  -0.043240 -0.011008\n",
      "Dinophysis fortii     -0.035430 -0.002735\n"
     ]
    }
   ],
   "source": [
    "# Calculate Pearson correlation between numeric features and binary target variable\n",
    "pearson_correlations = X.corrwith(pd.Series(y), method='pearson')\n",
    "pearson_correlations.name = 'Pearson'\n",
    "\n",
    "# Calculate Spearman rank correlation between numeric features and binary target variable\n",
    "spearman_correlations = X.corrwith(pd.Series(y), method='spearman')\n",
    "spearman_correlations.name = 'Spearman'\n",
    "\n",
    "# Combine the correlation values into a single dataframe\n",
    "corr_df = pd.concat([pearson_correlations, spearman_correlations], axis=1)\n",
    "\n",
    "# Create a new column for absolute Spearman correlation values\n",
    "corr_df['Spearman_abs'] = corr_df['Spearman'].abs()\n",
    "\n",
    "# Sort the dataframe by absolute Spearman correlation values\n",
    "corr_df_sorted = corr_df.sort_values(by=['Spearman_abs'], ascending=False)\n",
    "\n",
    "# Drop the absolute Spearman correlation column and return the sorted dataframe\n",
    "corr_df_ranked = corr_df_sorted.drop(columns=['Spearman_abs'])\n",
    "\n",
    "print(corr_df_ranked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b146cf-97fb-4a9b-8e33-dac52f91b48c",
   "metadata": {},
   "source": [
    "## Logistic regression and p-value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89623bf-dd24-405f-9c3d-c53725a26763",
   "metadata": {},
   "source": [
    "We use the coef_pval() method to calculate the p-values for each coefficient in the logistic regression model, and create a dataframe with the logistic regression coefficients and corresponding p-values. Finally, we rank the features by their absolute logistic regression coefficients and print the resulting dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dc96193-51d8-41c2-8019-e653f50e2f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.342655\n",
      "         Iterations 8\n",
      "                       Coefficient  P-value\n",
      "PO4-P                        3.269    0.306\n",
      "wind strength               -0.950    0.034\n",
      "Chl-a                        0.420    0.133\n",
      "salinity                    -0.164    0.119\n",
      "DIN                          0.080    0.136\n",
      "month                       -0.056    0.546\n",
      "Dinophysis tripos            0.041    0.028\n",
      "Phalacroma rotundatum       -0.029    0.079\n",
      "T                            0.012    0.872\n",
      "precipitation                0.009    0.015\n",
      "sun [h]                     -0.008    0.304\n",
      "air temp                     0.008    0.930\n",
      "Dinophysis fortii            0.007    0.463\n",
      "Dinophysis sacculus          0.001    0.921\n",
      "DSP                         -0.001    0.929\n",
      "Soca                        -0.000    0.001\n",
      "Dinophysis caudata           0.000    0.999\n"
     ]
    }
   ],
   "source": [
    "# logistic regression and p-value\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Fit logistic regression model\n",
    "logit_model = sm.Logit(y, sm.add_constant(X))\n",
    "result = logit_model.fit()\n",
    "\n",
    "# Calculate p-values for the logistic regression coefficients\n",
    "p_values = result.pvalues[1:]\n",
    "\n",
    "# Create a dataframe with the logistic regression coefficients and corresponding p-values\n",
    "coef_df = pd.DataFrame({'Coefficient': result.params[1:], 'P-value': p_values})\n",
    "\n",
    "# Add feature names as the index\n",
    "coef_df.index = X.columns\n",
    "\n",
    "# Rank the features by absolute logistic regression coefficients\n",
    "coef_df['Absolute Coefficient'] = coef_df['Coefficient'].abs()\n",
    "coef_df_sorted = coef_df.sort_values(by=['Absolute Coefficient'], ascending=False)\n",
    "coef_df_ranked = coef_df_sorted.drop(columns=['Absolute Coefficient'])\n",
    "\n",
    "print(coef_df_ranked.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17e63f2-39c7-4ab8-bb0e-22e01991f9e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afe5386-abb2-4fac-bb14-3e35e9124faa",
   "metadata": {},
   "source": [
    "## Baseline model - logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f171e07e-333c-4786-8814-efec9ee7009f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.87      0.97      0.92       147\n",
      "         poz       0.56      0.19      0.29        26\n",
      "\n",
      "    accuracy                           0.86       173\n",
      "   macro avg       0.71      0.58      0.60       173\n",
      "weighted avg       0.82      0.86      0.82       173\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
     ]
    }
   ],
   "source": [
    "# baseline model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Fit logistic regression model using scikit-learn\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X, y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logreg.predict(X_eval)\n",
    "\n",
    "# Evaluation on test data\n",
    "y_pred = logreg.predict(X_eval)\n",
    "lr_classification_report = classification_report(y_eval, y_pred)\n",
    "\n",
    "# Create classification report as dictionary\n",
    "lr_report_dict = classification_report(y_eval, y_pred, output_dict=True)\n",
    "\n",
    "print(classification_report(y_eval, y_pred, target_names=[\"neg\", \"poz\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "141ebec7-9dcb-4d12-97e5-14a6aef85da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save best estimator\n",
    "\n",
    "def save_best_estimator(grid_search_cv, classifier_name, notebook_name):\n",
    "    # Get the best estimator from the GridSearchCV object\n",
    "    best_estimator = grid_search_cv.best_estimator_\n",
    "\n",
    "    # Get the current date and time as a string\n",
    "    timestamp = datetime.datetime.now().strftime('%d%m%Y_%H%M')\n",
    "    \n",
    "    # Construct the file name with the classifier name, notebook name, and timestamp\n",
    "    pickle_file_name = f\"objects/estimators/{classifier_name}-{notebook_name}-{timestamp}.pkl\"\n",
    "    \n",
    "    # Save the best estimator to the file\n",
    "    with open(pickle_file_name, 'wb') as f:\n",
    "        pickle.dump(best_estimator, f)\n",
    "\n",
    "    print(f\"Best estimator saved as: {pickle_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3996100-0bd8-447a-a1b9-3a090bd55cab",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "907477fc-ebae-4d60-a61f-8bacc5693a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n",
      "Best estimator saved as: objects/estimators/SVC-HAB_modelling_5_7-10042023_1007.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>111</th>\n",
       "      <th>127</th>\n",
       "      <th>124</th>\n",
       "      <th>132</th>\n",
       "      <th>120</th>\n",
       "      <th>80</th>\n",
       "      <th>113</th>\n",
       "      <th>108</th>\n",
       "      <th>133</th>\n",
       "      <th>129</th>\n",
       "      <th>68</th>\n",
       "      <th>123</th>\n",
       "      <th>130</th>\n",
       "      <th>118</th>\n",
       "      <th>74</th>\n",
       "      <th>12</th>\n",
       "      <th>112</th>\n",
       "      <th>...</th>\n",
       "      <th>43</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>53</th>\n",
       "      <th>34</th>\n",
       "      <th>106</th>\n",
       "      <th>105</th>\n",
       "      <th>104</th>\n",
       "      <th>103</th>\n",
       "      <th>102</th>\n",
       "      <th>99</th>\n",
       "      <th>96</th>\n",
       "      <th>33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>0.021629</td>\n",
       "      <td>0.016569</td>\n",
       "      <td>0.019203</td>\n",
       "      <td>0.023218</td>\n",
       "      <td>0.019881</td>\n",
       "      <td>0.024985</td>\n",
       "      <td>0.021221</td>\n",
       "      <td>0.022221</td>\n",
       "      <td>0.020041</td>\n",
       "      <td>0.018533</td>\n",
       "      <td>0.019372</td>\n",
       "      <td>0.022501</td>\n",
       "      <td>0.019994</td>\n",
       "      <td>0.018112</td>\n",
       "      <td>0.019192</td>\n",
       "      <td>0.02439</td>\n",
       "      <td>0.023815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022967</td>\n",
       "      <td>0.02003</td>\n",
       "      <td>0.024051</td>\n",
       "      <td>0.020768</td>\n",
       "      <td>0.022731</td>\n",
       "      <td>0.020563</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.024515</td>\n",
       "      <td>0.024832</td>\n",
       "      <td>0.023152</td>\n",
       "      <td>0.026251</td>\n",
       "      <td>0.022609</td>\n",
       "      <td>0.022343</td>\n",
       "      <td>0.021059</td>\n",
       "      <td>0.021171</td>\n",
       "      <td>0.022901</td>\n",
       "      <td>0.022465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.002072</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.003507</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>0.003727</td>\n",
       "      <td>0.003551</td>\n",
       "      <td>0.001785</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>0.000865</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.002702</td>\n",
       "      <td>0.001143</td>\n",
       "      <td>0.001309</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.002355</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001739</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.001858</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.002955</td>\n",
       "      <td>0.005527</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>0.004533</td>\n",
       "      <td>0.001931</td>\n",
       "      <td>0.002682</td>\n",
       "      <td>0.002958</td>\n",
       "      <td>0.001243</td>\n",
       "      <td>0.003032</td>\n",
       "      <td>0.000615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.019912</td>\n",
       "      <td>0.015543</td>\n",
       "      <td>0.018874</td>\n",
       "      <td>0.020337</td>\n",
       "      <td>0.017296</td>\n",
       "      <td>0.02087</td>\n",
       "      <td>0.017935</td>\n",
       "      <td>0.017743</td>\n",
       "      <td>0.018676</td>\n",
       "      <td>0.017892</td>\n",
       "      <td>0.016241</td>\n",
       "      <td>0.020621</td>\n",
       "      <td>0.019129</td>\n",
       "      <td>0.014766</td>\n",
       "      <td>0.01665</td>\n",
       "      <td>0.025257</td>\n",
       "      <td>0.018728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023571</td>\n",
       "      <td>0.016795</td>\n",
       "      <td>0.018028</td>\n",
       "      <td>0.016596</td>\n",
       "      <td>0.018346</td>\n",
       "      <td>0.019372</td>\n",
       "      <td>0.017438</td>\n",
       "      <td>0.022126</td>\n",
       "      <td>0.02433</td>\n",
       "      <td>0.019641</td>\n",
       "      <td>0.021128</td>\n",
       "      <td>0.02044</td>\n",
       "      <td>0.021674</td>\n",
       "      <td>0.022188</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.019712</td>\n",
       "      <td>0.020671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.00151</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.002403</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.001578</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0.001992</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.003045</td>\n",
       "      <td>0.002014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002207</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>0.00299</td>\n",
       "      <td>0.002245</td>\n",
       "      <td>0.002393</td>\n",
       "      <td>0.001944</td>\n",
       "      <td>0.00321</td>\n",
       "      <td>0.003264</td>\n",
       "      <td>0.00238</td>\n",
       "      <td>0.002258</td>\n",
       "      <td>0.001471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__C</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__class_weight</th>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_smt__k_neighbors</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_smt__sampling_strategy</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_under__sampling_strategy</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'clf__C': 10, 'clf__class_weight': 'balanced'...</td>\n",
       "      <td>{'clf__C': 10, 'clf__class_weight': 'balanced'...</td>\n",
       "      <td>{'clf__C': 10, 'clf__class_weight': 'balanced'...</td>\n",
       "      <td>{'clf__C': 10, 'clf__class_weight': 'balanced'...</td>\n",
       "      <td>{'clf__C': 10, 'clf__class_weight': 'balanced'...</td>\n",
       "      <td>{'clf__C': 1, 'clf__class_weight': 'balanced',...</td>\n",
       "      <td>{'clf__C': 10, 'clf__class_weight': 'balanced'...</td>\n",
       "      <td>{'clf__C': 10, 'clf__class_weight': 'balanced'...</td>\n",
       "      <td>{'clf__C': 10, 'clf__class_weight': 'balanced'...</td>\n",
       "      <td>{'clf__C': 10, 'clf__class_weight': 'balanced'...</td>\n",
       "      <td>{'clf__C': 1, 'clf__class_weight': 'balanced',...</td>\n",
       "      <td>{'clf__C': 10, 'clf__class_weight': 'balanced'...</td>\n",
       "      <td>{'clf__C': 10, 'clf__class_weight': 'balanced'...</td>\n",
       "      <td>{'clf__C': 10, 'clf__class_weight': 'balanced'...</td>\n",
       "      <td>{'clf__C': 1, 'clf__class_weight': 'balanced',...</td>\n",
       "      <td>{'clf__C': 0.1, 'clf__class_weight': 'balanced...</td>\n",
       "      <td>{'clf__C': 10, 'clf__class_weight': 'balanced'...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'clf__C': 0.1, 'clf__class_weight': None, 'sm...</td>\n",
       "      <td>{'clf__C': 0.1, 'clf__class_weight': None, 'sm...</td>\n",
       "      <td>{'clf__C': 0.1, 'clf__class_weight': None, 'sm...</td>\n",
       "      <td>{'clf__C': 0.1, 'clf__class_weight': None, 'sm...</td>\n",
       "      <td>{'clf__C': 0.1, 'clf__class_weight': None, 'sm...</td>\n",
       "      <td>{'clf__C': 0.1, 'clf__class_weight': None, 'sm...</td>\n",
       "      <td>{'clf__C': 0.1, 'clf__class_weight': None, 'sm...</td>\n",
       "      <td>{'clf__C': 0.1, 'clf__class_weight': None, 'sm...</td>\n",
       "      <td>{'clf__C': 0.1, 'clf__class_weight': None, 'sm...</td>\n",
       "      <td>{'clf__C': 1, 'clf__class_weight': None, 'smt_...</td>\n",
       "      <td>{'clf__C': 1, 'clf__class_weight': None, 'smt_...</td>\n",
       "      <td>{'clf__C': 1, 'clf__class_weight': None, 'smt_...</td>\n",
       "      <td>{'clf__C': 1, 'clf__class_weight': None, 'smt_...</td>\n",
       "      <td>{'clf__C': 1, 'clf__class_weight': None, 'smt_...</td>\n",
       "      <td>{'clf__C': 1, 'clf__class_weight': None, 'smt_...</td>\n",
       "      <td>{'clf__C': 1, 'clf__class_weight': None, 'smt_...</td>\n",
       "      <td>{'clf__C': 0.1, 'clf__class_weight': None, 'sm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_recall</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_recall</th>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_recall</th>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_recall</th>\n",
       "      <td>0.451587</td>\n",
       "      <td>0.435714</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.484127</td>\n",
       "      <td>0.484921</td>\n",
       "      <td>0.468254</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.453175</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.436508</td>\n",
       "      <td>0.484127</td>\n",
       "      <td>0.419841</td>\n",
       "      <td>0.484127</td>\n",
       "      <td>0.643651</td>\n",
       "      <td>0.468254</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.468254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_recall</th>\n",
       "      <td>0.058332</td>\n",
       "      <td>0.010102</td>\n",
       "      <td>0.051434</td>\n",
       "      <td>0.040468</td>\n",
       "      <td>0.07429</td>\n",
       "      <td>0.029696</td>\n",
       "      <td>0.030553</td>\n",
       "      <td>0.071172</td>\n",
       "      <td>0.033672</td>\n",
       "      <td>0.048924</td>\n",
       "      <td>0.011224</td>\n",
       "      <td>0.062098</td>\n",
       "      <td>0.117181</td>\n",
       "      <td>0.187076</td>\n",
       "      <td>0.029696</td>\n",
       "      <td>0.01944</td>\n",
       "      <td>0.062492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_recall</th>\n",
       "      <td>37</td>\n",
       "      <td>45</td>\n",
       "      <td>33</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "      <td>33</td>\n",
       "      <td>40</td>\n",
       "      <td>18</td>\n",
       "      <td>55</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_precision</th>\n",
       "      <td>0.209302</td>\n",
       "      <td>0.219512</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.22449</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.189655</td>\n",
       "      <td>0.180328</td>\n",
       "      <td>0.196078</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.196078</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.180328</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.175439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_precision</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.179245</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>0.27027</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_precision</th>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.22449</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.243243</td>\n",
       "      <td>0.243243</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.234043</td>\n",
       "      <td>0.244444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_precision</th>\n",
       "      <td>0.278286</td>\n",
       "      <td>0.281406</td>\n",
       "      <td>0.27938</td>\n",
       "      <td>0.256211</td>\n",
       "      <td>0.245395</td>\n",
       "      <td>0.246914</td>\n",
       "      <td>0.230641</td>\n",
       "      <td>0.253582</td>\n",
       "      <td>0.246441</td>\n",
       "      <td>0.254414</td>\n",
       "      <td>0.232444</td>\n",
       "      <td>0.252717</td>\n",
       "      <td>0.23642</td>\n",
       "      <td>0.215096</td>\n",
       "      <td>0.231746</td>\n",
       "      <td>0.224601</td>\n",
       "      <td>0.235199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_precision</th>\n",
       "      <td>0.049134</td>\n",
       "      <td>0.058557</td>\n",
       "      <td>0.062154</td>\n",
       "      <td>0.073276</td>\n",
       "      <td>0.031182</td>\n",
       "      <td>0.062951</td>\n",
       "      <td>0.036238</td>\n",
       "      <td>0.05373</td>\n",
       "      <td>0.042486</td>\n",
       "      <td>0.049623</td>\n",
       "      <td>0.027675</td>\n",
       "      <td>0.047742</td>\n",
       "      <td>0.026029</td>\n",
       "      <td>0.049937</td>\n",
       "      <td>0.023757</td>\n",
       "      <td>0.041681</td>\n",
       "      <td>0.045492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_precision</th>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>33</td>\n",
       "      <td>39</td>\n",
       "      <td>37</td>\n",
       "      <td>47</td>\n",
       "      <td>35</td>\n",
       "      <td>38</td>\n",
       "      <td>34</td>\n",
       "      <td>45</td>\n",
       "      <td>36</td>\n",
       "      <td>41</td>\n",
       "      <td>61</td>\n",
       "      <td>46</td>\n",
       "      <td>52</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_f1</th>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.295082</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.318841</td>\n",
       "      <td>0.27027</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.271605</td>\n",
       "      <td>0.28169</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.28169</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.271605</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.253165</td>\n",
       "      <td>0.25974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_f1</th>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.355556</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.280702</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.350877</td>\n",
       "      <td>0.340426</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.299213</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_f1</th>\n",
       "      <td>0.385965</td>\n",
       "      <td>0.327273</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>0.34375</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>0.30303</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.31746</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>0.356164</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.31746</td>\n",
       "      <td>0.323529</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_f1</th>\n",
       "      <td>0.339835</td>\n",
       "      <td>0.337886</td>\n",
       "      <td>0.336897</td>\n",
       "      <td>0.325576</td>\n",
       "      <td>0.324141</td>\n",
       "      <td>0.3161</td>\n",
       "      <td>0.315738</td>\n",
       "      <td>0.315121</td>\n",
       "      <td>0.314992</td>\n",
       "      <td>0.314628</td>\n",
       "      <td>0.312711</td>\n",
       "      <td>0.311933</td>\n",
       "      <td>0.311182</td>\n",
       "      <td>0.30932</td>\n",
       "      <td>0.308201</td>\n",
       "      <td>0.307174</td>\n",
       "      <td>0.306535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_f1</th>\n",
       "      <td>0.041315</td>\n",
       "      <td>0.039993</td>\n",
       "      <td>0.042741</td>\n",
       "      <td>0.055789</td>\n",
       "      <td>0.037818</td>\n",
       "      <td>0.043743</td>\n",
       "      <td>0.028117</td>\n",
       "      <td>0.030907</td>\n",
       "      <td>0.029273</td>\n",
       "      <td>0.025537</td>\n",
       "      <td>0.02363</td>\n",
       "      <td>0.046565</td>\n",
       "      <td>0.0319</td>\n",
       "      <td>0.035645</td>\n",
       "      <td>0.015983</td>\n",
       "      <td>0.039168</td>\n",
       "      <td>0.033205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_f1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_roc_auc</th>\n",
       "      <td>0.575877</td>\n",
       "      <td>0.574561</td>\n",
       "      <td>0.576754</td>\n",
       "      <td>0.567982</td>\n",
       "      <td>0.601754</td>\n",
       "      <td>0.517544</td>\n",
       "      <td>0.560088</td>\n",
       "      <td>0.547807</td>\n",
       "      <td>0.596491</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.524561</td>\n",
       "      <td>0.579386</td>\n",
       "      <td>0.561842</td>\n",
       "      <td>0.562719</td>\n",
       "      <td>0.519298</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.567544</td>\n",
       "      <td>...</td>\n",
       "      <td>0.585965</td>\n",
       "      <td>0.582895</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.592544</td>\n",
       "      <td>0.600877</td>\n",
       "      <td>0.54386</td>\n",
       "      <td>0.442982</td>\n",
       "      <td>0.582018</td>\n",
       "      <td>0.639912</td>\n",
       "      <td>0.589035</td>\n",
       "      <td>0.595614</td>\n",
       "      <td>0.623246</td>\n",
       "      <td>0.595614</td>\n",
       "      <td>0.605702</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.639474</td>\n",
       "      <td>0.632018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_roc_auc</th>\n",
       "      <td>0.694901</td>\n",
       "      <td>0.701643</td>\n",
       "      <td>0.680152</td>\n",
       "      <td>0.7067</td>\n",
       "      <td>0.649389</td>\n",
       "      <td>0.621997</td>\n",
       "      <td>0.681837</td>\n",
       "      <td>0.663295</td>\n",
       "      <td>0.740413</td>\n",
       "      <td>0.719343</td>\n",
       "      <td>0.612305</td>\n",
       "      <td>0.73662</td>\n",
       "      <td>0.74252</td>\n",
       "      <td>0.628319</td>\n",
       "      <td>0.618626</td>\n",
       "      <td>0.608512</td>\n",
       "      <td>0.727349</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787189</td>\n",
       "      <td>0.690265</td>\n",
       "      <td>0.785925</td>\n",
       "      <td>0.590813</td>\n",
       "      <td>0.818795</td>\n",
       "      <td>0.783397</td>\n",
       "      <td>0.650232</td>\n",
       "      <td>0.63759</td>\n",
       "      <td>0.801096</td>\n",
       "      <td>0.783397</td>\n",
       "      <td>0.786346</td>\n",
       "      <td>0.741677</td>\n",
       "      <td>0.815424</td>\n",
       "      <td>0.799831</td>\n",
       "      <td>0.788032</td>\n",
       "      <td>0.807417</td>\n",
       "      <td>0.789296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_roc_auc</th>\n",
       "      <td>0.729456</td>\n",
       "      <td>0.671724</td>\n",
       "      <td>0.703329</td>\n",
       "      <td>0.711336</td>\n",
       "      <td>0.675516</td>\n",
       "      <td>0.629161</td>\n",
       "      <td>0.680995</td>\n",
       "      <td>0.672988</td>\n",
       "      <td>0.694058</td>\n",
       "      <td>0.640961</td>\n",
       "      <td>0.61989</td>\n",
       "      <td>0.7126</td>\n",
       "      <td>0.702065</td>\n",
       "      <td>0.689844</td>\n",
       "      <td>0.616941</td>\n",
       "      <td>0.599663</td>\n",
       "      <td>0.659924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.754319</td>\n",
       "      <td>0.839444</td>\n",
       "      <td>0.794353</td>\n",
       "      <td>0.600084</td>\n",
       "      <td>0.755162</td>\n",
       "      <td>0.798146</td>\n",
       "      <td>0.790139</td>\n",
       "      <td>0.673409</td>\n",
       "      <td>0.778761</td>\n",
       "      <td>0.77539</td>\n",
       "      <td>0.757269</td>\n",
       "      <td>0.756005</td>\n",
       "      <td>0.783397</td>\n",
       "      <td>0.802781</td>\n",
       "      <td>0.809945</td>\n",
       "      <td>0.771176</td>\n",
       "      <td>0.758112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_roc_auc</th>\n",
       "      <td>0.666745</td>\n",
       "      <td>0.649309</td>\n",
       "      <td>0.653412</td>\n",
       "      <td>0.662006</td>\n",
       "      <td>0.64222</td>\n",
       "      <td>0.589568</td>\n",
       "      <td>0.640973</td>\n",
       "      <td>0.62803</td>\n",
       "      <td>0.676987</td>\n",
       "      <td>0.647879</td>\n",
       "      <td>0.585586</td>\n",
       "      <td>0.676202</td>\n",
       "      <td>0.668809</td>\n",
       "      <td>0.626961</td>\n",
       "      <td>0.584955</td>\n",
       "      <td>0.57217</td>\n",
       "      <td>0.651606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.709158</td>\n",
       "      <td>0.704201</td>\n",
       "      <td>0.718426</td>\n",
       "      <td>0.59448</td>\n",
       "      <td>0.724945</td>\n",
       "      <td>0.708467</td>\n",
       "      <td>0.627784</td>\n",
       "      <td>0.631005</td>\n",
       "      <td>0.739923</td>\n",
       "      <td>0.71594</td>\n",
       "      <td>0.713077</td>\n",
       "      <td>0.706976</td>\n",
       "      <td>0.731478</td>\n",
       "      <td>0.736105</td>\n",
       "      <td>0.724326</td>\n",
       "      <td>0.739355</td>\n",
       "      <td>0.726475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_roc_auc</th>\n",
       "      <td>0.065784</td>\n",
       "      <td>0.054248</td>\n",
       "      <td>0.055025</td>\n",
       "      <td>0.066512</td>\n",
       "      <td>0.030537</td>\n",
       "      <td>0.051012</td>\n",
       "      <td>0.057196</td>\n",
       "      <td>0.056864</td>\n",
       "      <td>0.059983</td>\n",
       "      <td>0.055741</td>\n",
       "      <td>0.043262</td>\n",
       "      <td>0.069158</td>\n",
       "      <td>0.077419</td>\n",
       "      <td>0.051907</td>\n",
       "      <td>0.046431</td>\n",
       "      <td>0.045283</td>\n",
       "      <td>0.065505</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088138</td>\n",
       "      <td>0.105198</td>\n",
       "      <td>0.101476</td>\n",
       "      <td>0.004025</td>\n",
       "      <td>0.091494</td>\n",
       "      <td>0.116551</td>\n",
       "      <td>0.142612</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.071304</td>\n",
       "      <td>0.089795</td>\n",
       "      <td>0.083903</td>\n",
       "      <td>0.059495</td>\n",
       "      <td>0.096956</td>\n",
       "      <td>0.092217</td>\n",
       "      <td>0.105968</td>\n",
       "      <td>0.07216</td>\n",
       "      <td>0.067994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_roc_auc</th>\n",
       "      <td>69</td>\n",
       "      <td>80</td>\n",
       "      <td>76</td>\n",
       "      <td>73</td>\n",
       "      <td>84</td>\n",
       "      <td>121</td>\n",
       "      <td>85</td>\n",
       "      <td>94</td>\n",
       "      <td>60</td>\n",
       "      <td>81</td>\n",
       "      <td>125</td>\n",
       "      <td>62</td>\n",
       "      <td>68</td>\n",
       "      <td>97</td>\n",
       "      <td>129</td>\n",
       "      <td>146</td>\n",
       "      <td>79</td>\n",
       "      <td>...</td>\n",
       "      <td>39</td>\n",
       "      <td>44</td>\n",
       "      <td>33</td>\n",
       "      <td>114</td>\n",
       "      <td>24</td>\n",
       "      <td>40</td>\n",
       "      <td>96</td>\n",
       "      <td>91</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>37</td>\n",
       "      <td>41</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_recall_weighted</th>\n",
       "      <td>0.664179</td>\n",
       "      <td>0.679104</td>\n",
       "      <td>0.61194</td>\n",
       "      <td>0.552239</td>\n",
       "      <td>0.649254</td>\n",
       "      <td>0.597015</td>\n",
       "      <td>0.58209</td>\n",
       "      <td>0.559701</td>\n",
       "      <td>0.619403</td>\n",
       "      <td>0.626866</td>\n",
       "      <td>0.619403</td>\n",
       "      <td>0.641791</td>\n",
       "      <td>0.626866</td>\n",
       "      <td>0.559701</td>\n",
       "      <td>0.626866</td>\n",
       "      <td>0.559701</td>\n",
       "      <td>0.574627</td>\n",
       "      <td>...</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_recall_weighted</th>\n",
       "      <td>0.776119</td>\n",
       "      <td>0.791045</td>\n",
       "      <td>0.783582</td>\n",
       "      <td>0.783582</td>\n",
       "      <td>0.69403</td>\n",
       "      <td>0.776119</td>\n",
       "      <td>0.723881</td>\n",
       "      <td>0.768657</td>\n",
       "      <td>0.753731</td>\n",
       "      <td>0.776119</td>\n",
       "      <td>0.708955</td>\n",
       "      <td>0.738806</td>\n",
       "      <td>0.746269</td>\n",
       "      <td>0.335821</td>\n",
       "      <td>0.716418</td>\n",
       "      <td>0.716418</td>\n",
       "      <td>0.753731</td>\n",
       "      <td>...</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_recall_weighted</th>\n",
       "      <td>0.738806</td>\n",
       "      <td>0.723881</td>\n",
       "      <td>0.753731</td>\n",
       "      <td>0.686567</td>\n",
       "      <td>0.723881</td>\n",
       "      <td>0.656716</td>\n",
       "      <td>0.641791</td>\n",
       "      <td>0.731343</td>\n",
       "      <td>0.701493</td>\n",
       "      <td>0.701493</td>\n",
       "      <td>0.679104</td>\n",
       "      <td>0.753731</td>\n",
       "      <td>0.649254</td>\n",
       "      <td>0.731343</td>\n",
       "      <td>0.679104</td>\n",
       "      <td>0.656716</td>\n",
       "      <td>0.671642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_recall_weighted</th>\n",
       "      <td>0.726368</td>\n",
       "      <td>0.731343</td>\n",
       "      <td>0.716418</td>\n",
       "      <td>0.674129</td>\n",
       "      <td>0.689055</td>\n",
       "      <td>0.676617</td>\n",
       "      <td>0.649254</td>\n",
       "      <td>0.686567</td>\n",
       "      <td>0.691542</td>\n",
       "      <td>0.701493</td>\n",
       "      <td>0.669154</td>\n",
       "      <td>0.711443</td>\n",
       "      <td>0.674129</td>\n",
       "      <td>0.542289</td>\n",
       "      <td>0.674129</td>\n",
       "      <td>0.644279</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.845771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_recall_weighted</th>\n",
       "      <td>0.046538</td>\n",
       "      <td>0.046003</td>\n",
       "      <td>0.074875</td>\n",
       "      <td>0.094854</td>\n",
       "      <td>0.030669</td>\n",
       "      <td>0.074461</td>\n",
       "      <td>0.058126</td>\n",
       "      <td>0.090992</td>\n",
       "      <td>0.055289</td>\n",
       "      <td>0.060933</td>\n",
       "      <td>0.03723</td>\n",
       "      <td>0.049627</td>\n",
       "      <td>0.051823</td>\n",
       "      <td>0.16194</td>\n",
       "      <td>0.036728</td>\n",
       "      <td>0.064581</td>\n",
       "      <td>0.073204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_recall_weighted</th>\n",
       "      <td>93</td>\n",
       "      <td>92</td>\n",
       "      <td>97</td>\n",
       "      <td>111</td>\n",
       "      <td>107</td>\n",
       "      <td>110</td>\n",
       "      <td>129</td>\n",
       "      <td>108</td>\n",
       "      <td>104</td>\n",
       "      <td>102</td>\n",
       "      <td>117</td>\n",
       "      <td>98</td>\n",
       "      <td>112</td>\n",
       "      <td>159</td>\n",
       "      <td>112</td>\n",
       "      <td>133</td>\n",
       "      <td>118</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40 rows × 162 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              111  \\\n",
       "mean_fit_time                                                            0.021629   \n",
       "std_fit_time                                                             0.002072   \n",
       "mean_score_time                                                          0.019912   \n",
       "std_score_time                                                            0.00151   \n",
       "param_clf__C                                                                   10   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__C': 10, 'clf__class_weight': 'balanced'...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                       0.380952   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.451587   \n",
       "std_test_recall                                                          0.058332   \n",
       "rank_test_recall                                                               37   \n",
       "split0_test_precision                                                    0.209302   \n",
       "split1_test_precision                                                        0.32   \n",
       "split2_test_precision                                                    0.305556   \n",
       "mean_test_precision                                                      0.278286   \n",
       "std_test_precision                                                       0.049134   \n",
       "rank_test_precision                                                            31   \n",
       "split0_test_f1                                                           0.285714   \n",
       "split1_test_f1                                                           0.347826   \n",
       "split2_test_f1                                                           0.385965   \n",
       "mean_test_f1                                                             0.339835   \n",
       "std_test_f1                                                              0.041315   \n",
       "rank_test_f1                                                                    1   \n",
       "split0_test_roc_auc                                                      0.575877   \n",
       "split1_test_roc_auc                                                      0.694901   \n",
       "split2_test_roc_auc                                                      0.729456   \n",
       "mean_test_roc_auc                                                        0.666745   \n",
       "std_test_roc_auc                                                         0.065784   \n",
       "rank_test_roc_auc                                                              69   \n",
       "split0_test_recall_weighted                                              0.664179   \n",
       "split1_test_recall_weighted                                              0.776119   \n",
       "split2_test_recall_weighted                                              0.738806   \n",
       "mean_test_recall_weighted                                                0.726368   \n",
       "std_test_recall_weighted                                                 0.046538   \n",
       "rank_test_recall_weighted                                                      93   \n",
       "\n",
       "                                                                              127  \\\n",
       "mean_fit_time                                                            0.016569   \n",
       "std_fit_time                                                             0.000384   \n",
       "mean_score_time                                                          0.015543   \n",
       "std_score_time                                                           0.000719   \n",
       "param_clf__C                                                                   10   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__C': 10, 'clf__class_weight': 'balanced'...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                       0.428571   \n",
       "mean_test_recall                                                         0.435714   \n",
       "std_test_recall                                                          0.010102   \n",
       "rank_test_recall                                                               45   \n",
       "split0_test_precision                                                    0.219512   \n",
       "split1_test_precision                                                        0.36   \n",
       "split2_test_precision                                                    0.264706   \n",
       "mean_test_precision                                                      0.281406   \n",
       "std_test_precision                                                       0.058557   \n",
       "rank_test_precision                                                            29   \n",
       "split0_test_f1                                                           0.295082   \n",
       "split1_test_f1                                                           0.391304   \n",
       "split2_test_f1                                                           0.327273   \n",
       "mean_test_f1                                                             0.337886   \n",
       "std_test_f1                                                              0.039993   \n",
       "rank_test_f1                                                                    2   \n",
       "split0_test_roc_auc                                                      0.574561   \n",
       "split1_test_roc_auc                                                      0.701643   \n",
       "split2_test_roc_auc                                                      0.671724   \n",
       "mean_test_roc_auc                                                        0.649309   \n",
       "std_test_roc_auc                                                         0.054248   \n",
       "rank_test_roc_auc                                                              80   \n",
       "split0_test_recall_weighted                                              0.679104   \n",
       "split1_test_recall_weighted                                              0.791045   \n",
       "split2_test_recall_weighted                                              0.723881   \n",
       "mean_test_recall_weighted                                                0.731343   \n",
       "std_test_recall_weighted                                                 0.046003   \n",
       "rank_test_recall_weighted                                                      92   \n",
       "\n",
       "                                                                              124  \\\n",
       "mean_fit_time                                                            0.019203   \n",
       "std_fit_time                                                             0.000251   \n",
       "mean_score_time                                                          0.018874   \n",
       "std_score_time                                                           0.000726   \n",
       "param_clf__C                                                                   10   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__C': 10, 'clf__class_weight': 'balanced'...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                       0.380952   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.452381   \n",
       "std_test_recall                                                          0.051434   \n",
       "rank_test_recall                                                               33   \n",
       "split0_test_precision                                                    0.192308   \n",
       "split1_test_precision                                                    0.333333   \n",
       "split2_test_precision                                                      0.3125   \n",
       "mean_test_precision                                                       0.27938   \n",
       "std_test_precision                                                       0.062154   \n",
       "rank_test_precision                                                            30   \n",
       "split0_test_f1                                                           0.277778   \n",
       "split1_test_f1                                                           0.355556   \n",
       "split2_test_f1                                                           0.377358   \n",
       "mean_test_f1                                                             0.336897   \n",
       "std_test_f1                                                              0.042741   \n",
       "rank_test_f1                                                                    3   \n",
       "split0_test_roc_auc                                                      0.576754   \n",
       "split1_test_roc_auc                                                      0.680152   \n",
       "split2_test_roc_auc                                                      0.703329   \n",
       "mean_test_roc_auc                                                        0.653412   \n",
       "std_test_roc_auc                                                         0.055025   \n",
       "rank_test_roc_auc                                                              76   \n",
       "split0_test_recall_weighted                                               0.61194   \n",
       "split1_test_recall_weighted                                              0.783582   \n",
       "split2_test_recall_weighted                                              0.753731   \n",
       "mean_test_recall_weighted                                                0.716418   \n",
       "std_test_recall_weighted                                                 0.074875   \n",
       "rank_test_recall_weighted                                                      97   \n",
       "\n",
       "                                                                              132  \\\n",
       "mean_fit_time                                                            0.023218   \n",
       "std_fit_time                                                             0.003507   \n",
       "mean_score_time                                                          0.020337   \n",
       "std_score_time                                                           0.002403   \n",
       "param_clf__C                                                                   10   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__C': 10, 'clf__class_weight': 'balanced'...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.484127   \n",
       "std_test_recall                                                          0.040468   \n",
       "rank_test_recall                                                               16   \n",
       "split0_test_precision                                                    0.166667   \n",
       "split1_test_precision                                                    0.346154   \n",
       "split2_test_precision                                                    0.255814   \n",
       "mean_test_precision                                                      0.256211   \n",
       "std_test_precision                                                       0.073276   \n",
       "rank_test_precision                                                            33   \n",
       "split0_test_f1                                                               0.25   \n",
       "split1_test_f1                                                           0.382979   \n",
       "split2_test_f1                                                            0.34375   \n",
       "mean_test_f1                                                             0.325576   \n",
       "std_test_f1                                                              0.055789   \n",
       "rank_test_f1                                                                    4   \n",
       "split0_test_roc_auc                                                      0.567982   \n",
       "split1_test_roc_auc                                                        0.7067   \n",
       "split2_test_roc_auc                                                      0.711336   \n",
       "mean_test_roc_auc                                                        0.662006   \n",
       "std_test_roc_auc                                                         0.066512   \n",
       "rank_test_roc_auc                                                              73   \n",
       "split0_test_recall_weighted                                              0.552239   \n",
       "split1_test_recall_weighted                                              0.783582   \n",
       "split2_test_recall_weighted                                              0.686567   \n",
       "mean_test_recall_weighted                                                0.674129   \n",
       "std_test_recall_weighted                                                 0.094854   \n",
       "rank_test_recall_weighted                                                     111   \n",
       "\n",
       "                                                                              120  \\\n",
       "mean_fit_time                                                            0.019881   \n",
       "std_fit_time                                                             0.001825   \n",
       "mean_score_time                                                          0.017296   \n",
       "std_score_time                                                           0.000627   \n",
       "param_clf__C                                                                   10   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__C': 10, 'clf__class_weight': 'balanced'...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                       0.380952   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.484921   \n",
       "std_test_recall                                                           0.07429   \n",
       "rank_test_recall                                                               15   \n",
       "split0_test_precision                                                     0.22449   \n",
       "split1_test_precision                                                    0.222222   \n",
       "split2_test_precision                                                    0.289474   \n",
       "mean_test_precision                                                      0.245395   \n",
       "std_test_precision                                                       0.031182   \n",
       "rank_test_precision                                                            39   \n",
       "split0_test_f1                                                           0.318841   \n",
       "split1_test_f1                                                           0.280702   \n",
       "split2_test_f1                                                           0.372881   \n",
       "mean_test_f1                                                             0.324141   \n",
       "std_test_f1                                                              0.037818   \n",
       "rank_test_f1                                                                    5   \n",
       "split0_test_roc_auc                                                      0.601754   \n",
       "split1_test_roc_auc                                                      0.649389   \n",
       "split2_test_roc_auc                                                      0.675516   \n",
       "mean_test_roc_auc                                                         0.64222   \n",
       "std_test_roc_auc                                                         0.030537   \n",
       "rank_test_roc_auc                                                              84   \n",
       "split0_test_recall_weighted                                              0.649254   \n",
       "split1_test_recall_weighted                                               0.69403   \n",
       "split2_test_recall_weighted                                              0.723881   \n",
       "mean_test_recall_weighted                                                0.689055   \n",
       "std_test_recall_weighted                                                 0.030669   \n",
       "rank_test_recall_weighted                                                     107   \n",
       "\n",
       "                                                                              80   \\\n",
       "mean_fit_time                                                            0.024985   \n",
       "std_fit_time                                                             0.003727   \n",
       "mean_score_time                                                           0.02087   \n",
       "std_score_time                                                           0.001578   \n",
       "param_clf__C                                                                    1   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__C': 1, 'clf__class_weight': 'balanced',...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.468254   \n",
       "std_test_recall                                                          0.029696   \n",
       "rank_test_recall                                                               24   \n",
       "split0_test_precision                                                    0.185185   \n",
       "split1_test_precision                                                    0.333333   \n",
       "split2_test_precision                                                    0.222222   \n",
       "mean_test_precision                                                      0.246914   \n",
       "std_test_precision                                                       0.062951   \n",
       "rank_test_precision                                                            37   \n",
       "split0_test_f1                                                            0.27027   \n",
       "split1_test_f1                                                              0.375   \n",
       "split2_test_f1                                                            0.30303   \n",
       "mean_test_f1                                                               0.3161   \n",
       "std_test_f1                                                              0.043743   \n",
       "rank_test_f1                                                                    6   \n",
       "split0_test_roc_auc                                                      0.517544   \n",
       "split1_test_roc_auc                                                      0.621997   \n",
       "split2_test_roc_auc                                                      0.629161   \n",
       "mean_test_roc_auc                                                        0.589568   \n",
       "std_test_roc_auc                                                         0.051012   \n",
       "rank_test_roc_auc                                                             121   \n",
       "split0_test_recall_weighted                                              0.597015   \n",
       "split1_test_recall_weighted                                              0.776119   \n",
       "split2_test_recall_weighted                                              0.656716   \n",
       "mean_test_recall_weighted                                                0.676617   \n",
       "std_test_recall_weighted                                                 0.074461   \n",
       "rank_test_recall_weighted                                                     110   \n",
       "\n",
       "                                                                              113  \\\n",
       "mean_fit_time                                                            0.021221   \n",
       "std_fit_time                                                             0.003551   \n",
       "mean_score_time                                                          0.017935   \n",
       "std_score_time                                                           0.001498   \n",
       "param_clf__C                                                                   10   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__C': 10, 'clf__class_weight': 'balanced'...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.516667   \n",
       "std_test_recall                                                          0.030553   \n",
       "rank_test_recall                                                                8   \n",
       "split0_test_precision                                                    0.189655   \n",
       "split1_test_precision                                                    0.277778   \n",
       "split2_test_precision                                                     0.22449   \n",
       "mean_test_precision                                                      0.230641   \n",
       "std_test_precision                                                       0.036238   \n",
       "rank_test_precision                                                            47   \n",
       "split0_test_f1                                                           0.282051   \n",
       "split1_test_f1                                                           0.350877   \n",
       "split2_test_f1                                                           0.314286   \n",
       "mean_test_f1                                                             0.315738   \n",
       "std_test_f1                                                              0.028117   \n",
       "rank_test_f1                                                                    7   \n",
       "split0_test_roc_auc                                                      0.560088   \n",
       "split1_test_roc_auc                                                      0.681837   \n",
       "split2_test_roc_auc                                                      0.680995   \n",
       "mean_test_roc_auc                                                        0.640973   \n",
       "std_test_roc_auc                                                         0.057196   \n",
       "rank_test_roc_auc                                                              85   \n",
       "split0_test_recall_weighted                                               0.58209   \n",
       "split1_test_recall_weighted                                              0.723881   \n",
       "split2_test_recall_weighted                                              0.641791   \n",
       "mean_test_recall_weighted                                                0.649254   \n",
       "std_test_recall_weighted                                                 0.058126   \n",
       "rank_test_recall_weighted                                                     129   \n",
       "\n",
       "                                                                              108  \\\n",
       "mean_fit_time                                                            0.022221   \n",
       "std_fit_time                                                             0.001785   \n",
       "mean_score_time                                                          0.017743   \n",
       "std_score_time                                                           0.000538   \n",
       "param_clf__C                                                                   10   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__C': 10, 'clf__class_weight': 'balanced'...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                       0.380952   \n",
       "split2_test_recall                                                       0.428571   \n",
       "mean_test_recall                                                         0.453175   \n",
       "std_test_recall                                                          0.071172   \n",
       "rank_test_recall                                                               29   \n",
       "split0_test_precision                                                    0.180328   \n",
       "split1_test_precision                                                    0.307692   \n",
       "split2_test_precision                                                    0.272727   \n",
       "mean_test_precision                                                      0.253582   \n",
       "std_test_precision                                                        0.05373   \n",
       "rank_test_precision                                                            35   \n",
       "split0_test_f1                                                           0.271605   \n",
       "split1_test_f1                                                           0.340426   \n",
       "split2_test_f1                                                           0.333333   \n",
       "mean_test_f1                                                             0.315121   \n",
       "std_test_f1                                                              0.030907   \n",
       "rank_test_f1                                                                    8   \n",
       "split0_test_roc_auc                                                      0.547807   \n",
       "split1_test_roc_auc                                                      0.663295   \n",
       "split2_test_roc_auc                                                      0.672988   \n",
       "mean_test_roc_auc                                                         0.62803   \n",
       "std_test_roc_auc                                                         0.056864   \n",
       "rank_test_roc_auc                                                              94   \n",
       "split0_test_recall_weighted                                              0.559701   \n",
       "split1_test_recall_weighted                                              0.768657   \n",
       "split2_test_recall_weighted                                              0.731343   \n",
       "mean_test_recall_weighted                                                0.686567   \n",
       "std_test_recall_weighted                                                 0.090992   \n",
       "rank_test_recall_weighted                                                     108   \n",
       "\n",
       "                                                                              133  \\\n",
       "mean_fit_time                                                            0.020041   \n",
       "std_fit_time                                                             0.000628   \n",
       "mean_score_time                                                          0.018676   \n",
       "std_score_time                                                           0.000194   \n",
       "param_clf__C                                                                   10   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__C': 10, 'clf__class_weight': 'balanced'...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                       0.428571   \n",
       "mean_test_recall                                                         0.452381   \n",
       "std_test_recall                                                          0.033672   \n",
       "rank_test_recall                                                               33   \n",
       "split0_test_precision                                                    0.196078   \n",
       "split1_test_precision                                                         0.3   \n",
       "split2_test_precision                                                    0.243243   \n",
       "mean_test_precision                                                      0.246441   \n",
       "std_test_precision                                                       0.042486   \n",
       "rank_test_precision                                                            38   \n",
       "split0_test_f1                                                            0.28169   \n",
       "split1_test_f1                                                           0.352941   \n",
       "split2_test_f1                                                           0.310345   \n",
       "mean_test_f1                                                             0.314992   \n",
       "std_test_f1                                                              0.029273   \n",
       "rank_test_f1                                                                    9   \n",
       "split0_test_roc_auc                                                      0.596491   \n",
       "split1_test_roc_auc                                                      0.740413   \n",
       "split2_test_roc_auc                                                      0.694058   \n",
       "mean_test_roc_auc                                                        0.676987   \n",
       "std_test_roc_auc                                                         0.059983   \n",
       "rank_test_roc_auc                                                              60   \n",
       "split0_test_recall_weighted                                              0.619403   \n",
       "split1_test_recall_weighted                                              0.753731   \n",
       "split2_test_recall_weighted                                              0.701493   \n",
       "mean_test_recall_weighted                                                0.691542   \n",
       "std_test_recall_weighted                                                 0.055289   \n",
       "rank_test_recall_weighted                                                     104   \n",
       "\n",
       "                                                                              129  \\\n",
       "mean_fit_time                                                            0.018533   \n",
       "std_fit_time                                                             0.000865   \n",
       "mean_score_time                                                          0.017892   \n",
       "std_score_time                                                           0.000595   \n",
       "param_clf__C                                                                   10   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__C': 10, 'clf__class_weight': 'balanced'...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                       0.380952   \n",
       "split2_test_recall                                                       0.428571   \n",
       "mean_test_recall                                                         0.436508   \n",
       "std_test_recall                                                          0.048924   \n",
       "rank_test_recall                                                               40   \n",
       "split0_test_precision                                                         0.2   \n",
       "split1_test_precision                                                        0.32   \n",
       "split2_test_precision                                                    0.243243   \n",
       "mean_test_precision                                                      0.254414   \n",
       "std_test_precision                                                       0.049623   \n",
       "rank_test_precision                                                            34   \n",
       "split0_test_f1                                                           0.285714   \n",
       "split1_test_f1                                                           0.347826   \n",
       "split2_test_f1                                                           0.310345   \n",
       "mean_test_f1                                                             0.314628   \n",
       "std_test_f1                                                              0.025537   \n",
       "rank_test_f1                                                                   10   \n",
       "split0_test_roc_auc                                                      0.583333   \n",
       "split1_test_roc_auc                                                      0.719343   \n",
       "split2_test_roc_auc                                                      0.640961   \n",
       "mean_test_roc_auc                                                        0.647879   \n",
       "std_test_roc_auc                                                         0.055741   \n",
       "rank_test_roc_auc                                                              81   \n",
       "split0_test_recall_weighted                                              0.626866   \n",
       "split1_test_recall_weighted                                              0.776119   \n",
       "split2_test_recall_weighted                                              0.701493   \n",
       "mean_test_recall_weighted                                                0.701493   \n",
       "std_test_recall_weighted                                                 0.060933   \n",
       "rank_test_recall_weighted                                                     102   \n",
       "\n",
       "                                                                              68   \\\n",
       "mean_fit_time                                                            0.019372   \n",
       "std_fit_time                                                             0.000472   \n",
       "mean_score_time                                                          0.016241   \n",
       "std_score_time                                                           0.000902   \n",
       "param_clf__C                                                                    1   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__C': 1, 'clf__class_weight': 'balanced',...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.484127   \n",
       "std_test_recall                                                          0.011224   \n",
       "rank_test_recall                                                               18   \n",
       "split0_test_precision                                                    0.196078   \n",
       "split1_test_precision                                                    0.263158   \n",
       "split2_test_precision                                                    0.238095   \n",
       "mean_test_precision                                                      0.232444   \n",
       "std_test_precision                                                       0.027675   \n",
       "rank_test_precision                                                            45   \n",
       "split0_test_f1                                                            0.28169   \n",
       "split1_test_f1                                                           0.338983   \n",
       "split2_test_f1                                                            0.31746   \n",
       "mean_test_f1                                                             0.312711   \n",
       "std_test_f1                                                               0.02363   \n",
       "rank_test_f1                                                                   11   \n",
       "split0_test_roc_auc                                                      0.524561   \n",
       "split1_test_roc_auc                                                      0.612305   \n",
       "split2_test_roc_auc                                                       0.61989   \n",
       "mean_test_roc_auc                                                        0.585586   \n",
       "std_test_roc_auc                                                         0.043262   \n",
       "rank_test_roc_auc                                                             125   \n",
       "split0_test_recall_weighted                                              0.619403   \n",
       "split1_test_recall_weighted                                              0.708955   \n",
       "split2_test_recall_weighted                                              0.679104   \n",
       "mean_test_recall_weighted                                                0.669154   \n",
       "std_test_recall_weighted                                                  0.03723   \n",
       "rank_test_recall_weighted                                                     117   \n",
       "\n",
       "                                                                              123  \\\n",
       "mean_fit_time                                                            0.022501   \n",
       "std_fit_time                                                             0.002702   \n",
       "mean_score_time                                                          0.020621   \n",
       "std_score_time                                                           0.000974   \n",
       "param_clf__C                                                                   10   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__C': 10, 'clf__class_weight': 'balanced'...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                       0.333333   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.419841   \n",
       "std_test_recall                                                          0.062098   \n",
       "rank_test_recall                                                               55   \n",
       "split0_test_precision                                                    0.195652   \n",
       "split1_test_precision                                                        0.25   \n",
       "split2_test_precision                                                      0.3125   \n",
       "mean_test_precision                                                      0.252717   \n",
       "std_test_precision                                                       0.047742   \n",
       "rank_test_precision                                                            36   \n",
       "split0_test_f1                                                           0.272727   \n",
       "split1_test_f1                                                           0.285714   \n",
       "split2_test_f1                                                           0.377358   \n",
       "mean_test_f1                                                             0.311933   \n",
       "std_test_f1                                                              0.046565   \n",
       "rank_test_f1                                                                   12   \n",
       "split0_test_roc_auc                                                      0.579386   \n",
       "split1_test_roc_auc                                                       0.73662   \n",
       "split2_test_roc_auc                                                        0.7126   \n",
       "mean_test_roc_auc                                                        0.676202   \n",
       "std_test_roc_auc                                                         0.069158   \n",
       "rank_test_roc_auc                                                              62   \n",
       "split0_test_recall_weighted                                              0.641791   \n",
       "split1_test_recall_weighted                                              0.738806   \n",
       "split2_test_recall_weighted                                              0.753731   \n",
       "mean_test_recall_weighted                                                0.711443   \n",
       "std_test_recall_weighted                                                 0.049627   \n",
       "rank_test_recall_weighted                                                      98   \n",
       "\n",
       "                                                                              130  \\\n",
       "mean_fit_time                                                            0.019994   \n",
       "std_fit_time                                                             0.001143   \n",
       "mean_score_time                                                          0.019129   \n",
       "std_score_time                                                           0.001992   \n",
       "param_clf__C                                                                   10   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__C': 10, 'clf__class_weight': 'balanced'...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                       0.333333   \n",
       "split2_test_recall                                                       0.619048   \n",
       "mean_test_recall                                                         0.484127   \n",
       "std_test_recall                                                          0.117181   \n",
       "rank_test_recall                                                               18   \n",
       "split0_test_precision                                                         0.2   \n",
       "split1_test_precision                                                    0.259259   \n",
       "split2_test_precision                                                        0.25   \n",
       "mean_test_precision                                                       0.23642   \n",
       "std_test_precision                                                       0.026029   \n",
       "rank_test_precision                                                            41   \n",
       "split0_test_f1                                                           0.285714   \n",
       "split1_test_f1                                                           0.291667   \n",
       "split2_test_f1                                                           0.356164   \n",
       "mean_test_f1                                                             0.311182   \n",
       "std_test_f1                                                                0.0319   \n",
       "rank_test_f1                                                                   13   \n",
       "split0_test_roc_auc                                                      0.561842   \n",
       "split1_test_roc_auc                                                       0.74252   \n",
       "split2_test_roc_auc                                                      0.702065   \n",
       "mean_test_roc_auc                                                        0.668809   \n",
       "std_test_roc_auc                                                         0.077419   \n",
       "rank_test_roc_auc                                                              68   \n",
       "split0_test_recall_weighted                                              0.626866   \n",
       "split1_test_recall_weighted                                              0.746269   \n",
       "split2_test_recall_weighted                                              0.649254   \n",
       "mean_test_recall_weighted                                                0.674129   \n",
       "std_test_recall_weighted                                                 0.051823   \n",
       "rank_test_recall_weighted                                                     112   \n",
       "\n",
       "                                                                              118  \\\n",
       "mean_fit_time                                                            0.018112   \n",
       "std_fit_time                                                             0.001309   \n",
       "mean_score_time                                                          0.014766   \n",
       "std_score_time                                                           0.000753   \n",
       "param_clf__C                                                                   10   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__C': 10, 'clf__class_weight': 'balanced'...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                       0.904762   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.643651   \n",
       "std_test_recall                                                          0.187076   \n",
       "rank_test_recall                                                                1   \n",
       "split0_test_precision                                                    0.180328   \n",
       "split1_test_precision                                                    0.179245   \n",
       "split2_test_precision                                                    0.285714   \n",
       "mean_test_precision                                                      0.215096   \n",
       "std_test_precision                                                       0.049937   \n",
       "rank_test_precision                                                            61   \n",
       "split0_test_f1                                                           0.271605   \n",
       "split1_test_f1                                                           0.299213   \n",
       "split2_test_f1                                                           0.357143   \n",
       "mean_test_f1                                                              0.30932   \n",
       "std_test_f1                                                              0.035645   \n",
       "rank_test_f1                                                                   14   \n",
       "split0_test_roc_auc                                                      0.562719   \n",
       "split1_test_roc_auc                                                      0.628319   \n",
       "split2_test_roc_auc                                                      0.689844   \n",
       "mean_test_roc_auc                                                        0.626961   \n",
       "std_test_roc_auc                                                         0.051907   \n",
       "rank_test_roc_auc                                                              97   \n",
       "split0_test_recall_weighted                                              0.559701   \n",
       "split1_test_recall_weighted                                              0.335821   \n",
       "split2_test_recall_weighted                                              0.731343   \n",
       "mean_test_recall_weighted                                                0.542289   \n",
       "std_test_recall_weighted                                                  0.16194   \n",
       "rank_test_recall_weighted                                                     159   \n",
       "\n",
       "                                                                              74   \\\n",
       "mean_fit_time                                                            0.019192   \n",
       "std_fit_time                                                             0.000832   \n",
       "mean_score_time                                                           0.01665   \n",
       "std_score_time                                                           0.000664   \n",
       "param_clf__C                                                                    1   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__C': 1, 'clf__class_weight': 'balanced',...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.468254   \n",
       "std_test_recall                                                          0.029696   \n",
       "rank_test_recall                                                               24   \n",
       "split0_test_precision                                                         0.2   \n",
       "split1_test_precision                                                    0.257143   \n",
       "split2_test_precision                                                    0.238095   \n",
       "mean_test_precision                                                      0.231746   \n",
       "std_test_precision                                                       0.023757   \n",
       "rank_test_precision                                                            46   \n",
       "split0_test_f1                                                           0.285714   \n",
       "split1_test_f1                                                           0.321429   \n",
       "split2_test_f1                                                            0.31746   \n",
       "mean_test_f1                                                             0.308201   \n",
       "std_test_f1                                                              0.015983   \n",
       "rank_test_f1                                                                   15   \n",
       "split0_test_roc_auc                                                      0.519298   \n",
       "split1_test_roc_auc                                                      0.618626   \n",
       "split2_test_roc_auc                                                      0.616941   \n",
       "mean_test_roc_auc                                                        0.584955   \n",
       "std_test_roc_auc                                                         0.046431   \n",
       "rank_test_roc_auc                                                             129   \n",
       "split0_test_recall_weighted                                              0.626866   \n",
       "split1_test_recall_weighted                                              0.716418   \n",
       "split2_test_recall_weighted                                              0.679104   \n",
       "mean_test_recall_weighted                                                0.674129   \n",
       "std_test_recall_weighted                                                 0.036728   \n",
       "rank_test_recall_weighted                                                     112   \n",
       "\n",
       "                                                                              12   \\\n",
       "mean_fit_time                                                             0.02439   \n",
       "std_fit_time                                                             0.000556   \n",
       "mean_score_time                                                          0.025257   \n",
       "std_score_time                                                           0.003045   \n",
       "param_clf__C                                                                  0.1   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__C': 0.1, 'clf__class_weight': 'balanced...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                              0.5   \n",
       "std_test_recall                                                           0.01944   \n",
       "rank_test_recall                                                               11   \n",
       "split0_test_precision                                                    0.169492   \n",
       "split1_test_precision                                                     0.27027   \n",
       "split2_test_precision                                                    0.234043   \n",
       "mean_test_precision                                                      0.224601   \n",
       "std_test_precision                                                       0.041681   \n",
       "rank_test_precision                                                            52   \n",
       "split0_test_f1                                                           0.253165   \n",
       "split1_test_f1                                                           0.344828   \n",
       "split2_test_f1                                                           0.323529   \n",
       "mean_test_f1                                                             0.307174   \n",
       "std_test_f1                                                              0.039168   \n",
       "rank_test_f1                                                                   16   \n",
       "split0_test_roc_auc                                                      0.508333   \n",
       "split1_test_roc_auc                                                      0.608512   \n",
       "split2_test_roc_auc                                                      0.599663   \n",
       "mean_test_roc_auc                                                         0.57217   \n",
       "std_test_roc_auc                                                         0.045283   \n",
       "rank_test_roc_auc                                                             146   \n",
       "split0_test_recall_weighted                                              0.559701   \n",
       "split1_test_recall_weighted                                              0.716418   \n",
       "split2_test_recall_weighted                                              0.656716   \n",
       "mean_test_recall_weighted                                                0.644279   \n",
       "std_test_recall_weighted                                                 0.064581   \n",
       "rank_test_recall_weighted                                                     133   \n",
       "\n",
       "                                                                              112  \\\n",
       "mean_fit_time                                                            0.023815   \n",
       "std_fit_time                                                             0.002355   \n",
       "mean_score_time                                                          0.018728   \n",
       "std_score_time                                                           0.002014   \n",
       "param_clf__C                                                                   10   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__C': 10, 'clf__class_weight': 'balanced'...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                       0.380952   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.468254   \n",
       "std_test_recall                                                          0.062492   \n",
       "rank_test_recall                                                               24   \n",
       "split0_test_precision                                                    0.175439   \n",
       "split1_test_precision                                                    0.285714   \n",
       "split2_test_precision                                                    0.244444   \n",
       "mean_test_precision                                                      0.235199   \n",
       "std_test_precision                                                       0.045492   \n",
       "rank_test_precision                                                            43   \n",
       "split0_test_f1                                                            0.25974   \n",
       "split1_test_f1                                                           0.326531   \n",
       "split2_test_f1                                                           0.333333   \n",
       "mean_test_f1                                                             0.306535   \n",
       "std_test_f1                                                              0.033205   \n",
       "rank_test_f1                                                                   17   \n",
       "split0_test_roc_auc                                                      0.567544   \n",
       "split1_test_roc_auc                                                      0.727349   \n",
       "split2_test_roc_auc                                                      0.659924   \n",
       "mean_test_roc_auc                                                        0.651606   \n",
       "std_test_roc_auc                                                         0.065505   \n",
       "rank_test_roc_auc                                                              79   \n",
       "split0_test_recall_weighted                                              0.574627   \n",
       "split1_test_recall_weighted                                              0.753731   \n",
       "split2_test_recall_weighted                                              0.671642   \n",
       "mean_test_recall_weighted                                                0.666667   \n",
       "std_test_recall_weighted                                                 0.073204   \n",
       "rank_test_recall_weighted                                                     118   \n",
       "\n",
       "                                ...  \\\n",
       "mean_fit_time                   ...   \n",
       "std_fit_time                    ...   \n",
       "mean_score_time                 ...   \n",
       "std_score_time                  ...   \n",
       "param_clf__C                    ...   \n",
       "param_clf__class_weight         ...   \n",
       "param_smt__k_neighbors          ...   \n",
       "param_smt__sampling_strategy    ...   \n",
       "param_under__sampling_strategy  ...   \n",
       "params                          ...   \n",
       "split0_test_recall              ...   \n",
       "split1_test_recall              ...   \n",
       "split2_test_recall              ...   \n",
       "mean_test_recall                ...   \n",
       "std_test_recall                 ...   \n",
       "rank_test_recall                ...   \n",
       "split0_test_precision           ...   \n",
       "split1_test_precision           ...   \n",
       "split2_test_precision           ...   \n",
       "mean_test_precision             ...   \n",
       "std_test_precision              ...   \n",
       "rank_test_precision             ...   \n",
       "split0_test_f1                  ...   \n",
       "split1_test_f1                  ...   \n",
       "split2_test_f1                  ...   \n",
       "mean_test_f1                    ...   \n",
       "std_test_f1                     ...   \n",
       "rank_test_f1                    ...   \n",
       "split0_test_roc_auc             ...   \n",
       "split1_test_roc_auc             ...   \n",
       "split2_test_roc_auc             ...   \n",
       "mean_test_roc_auc               ...   \n",
       "std_test_roc_auc                ...   \n",
       "rank_test_roc_auc               ...   \n",
       "split0_test_recall_weighted     ...   \n",
       "split1_test_recall_weighted     ...   \n",
       "split2_test_recall_weighted     ...   \n",
       "mean_test_recall_weighted       ...   \n",
       "std_test_recall_weighted        ...   \n",
       "rank_test_recall_weighted       ...   \n",
       "\n",
       "                                                                              43   \\\n",
       "mean_fit_time                                                            0.022967   \n",
       "std_fit_time                                                             0.001739   \n",
       "mean_score_time                                                          0.023571   \n",
       "std_score_time                                                           0.002207   \n",
       "param_clf__C                                                                  0.1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__C': 0.1, 'clf__class_weight': None, 'sm...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              117   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           117   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  117   \n",
       "split0_test_roc_auc                                                      0.585965   \n",
       "split1_test_roc_auc                                                      0.787189   \n",
       "split2_test_roc_auc                                                      0.754319   \n",
       "mean_test_roc_auc                                                        0.709158   \n",
       "std_test_roc_auc                                                         0.088138   \n",
       "rank_test_roc_auc                                                              39   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       6   \n",
       "\n",
       "                                                                              27   \\\n",
       "mean_fit_time                                                             0.02003   \n",
       "std_fit_time                                                             0.000856   \n",
       "mean_score_time                                                          0.016795   \n",
       "std_score_time                                                           0.000811   \n",
       "param_clf__C                                                                  0.1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__C': 0.1, 'clf__class_weight': None, 'sm...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              117   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           117   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  117   \n",
       "split0_test_roc_auc                                                      0.582895   \n",
       "split1_test_roc_auc                                                      0.690265   \n",
       "split2_test_roc_auc                                                      0.839444   \n",
       "mean_test_roc_auc                                                        0.704201   \n",
       "std_test_roc_auc                                                         0.105198   \n",
       "rank_test_roc_auc                                                              44   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       6   \n",
       "\n",
       "                                                                              28   \\\n",
       "mean_fit_time                                                            0.024051   \n",
       "std_fit_time                                                             0.001858   \n",
       "mean_score_time                                                          0.018028   \n",
       "std_score_time                                                           0.001161   \n",
       "param_clf__C                                                                  0.1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__C': 0.1, 'clf__class_weight': None, 'sm...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              117   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           117   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  117   \n",
       "split0_test_roc_auc                                                         0.575   \n",
       "split1_test_roc_auc                                                      0.785925   \n",
       "split2_test_roc_auc                                                      0.794353   \n",
       "mean_test_roc_auc                                                        0.718426   \n",
       "std_test_roc_auc                                                         0.101476   \n",
       "rank_test_roc_auc                                                              33   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       6   \n",
       "\n",
       "                                                                              29   \\\n",
       "mean_fit_time                                                            0.020768   \n",
       "std_fit_time                                                             0.001786   \n",
       "mean_score_time                                                          0.016596   \n",
       "std_score_time                                                           0.001163   \n",
       "param_clf__C                                                                  0.1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__C': 0.1, 'clf__class_weight': None, 'sm...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              117   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           117   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  117   \n",
       "split0_test_roc_auc                                                      0.592544   \n",
       "split1_test_roc_auc                                                      0.590813   \n",
       "split2_test_roc_auc                                                      0.600084   \n",
       "mean_test_roc_auc                                                         0.59448   \n",
       "std_test_roc_auc                                                         0.004025   \n",
       "rank_test_roc_auc                                                             114   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       6   \n",
       "\n",
       "                                                                              30   \\\n",
       "mean_fit_time                                                            0.022731   \n",
       "std_fit_time                                                             0.001509   \n",
       "mean_score_time                                                          0.018346   \n",
       "std_score_time                                                           0.000652   \n",
       "param_clf__C                                                                  0.1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__C': 0.1, 'clf__class_weight': None, 'sm...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              117   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           117   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  117   \n",
       "split0_test_roc_auc                                                      0.600877   \n",
       "split1_test_roc_auc                                                      0.818795   \n",
       "split2_test_roc_auc                                                      0.755162   \n",
       "mean_test_roc_auc                                                        0.724945   \n",
       "std_test_roc_auc                                                         0.091494   \n",
       "rank_test_roc_auc                                                              24   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       6   \n",
       "\n",
       "                                                                              31   \\\n",
       "mean_fit_time                                                            0.020563   \n",
       "std_fit_time                                                             0.000884   \n",
       "mean_score_time                                                          0.019372   \n",
       "std_score_time                                                           0.001052   \n",
       "param_clf__C                                                                  0.1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__C': 0.1, 'clf__class_weight': None, 'sm...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              117   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           117   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  117   \n",
       "split0_test_roc_auc                                                       0.54386   \n",
       "split1_test_roc_auc                                                      0.783397   \n",
       "split2_test_roc_auc                                                      0.798146   \n",
       "mean_test_roc_auc                                                        0.708467   \n",
       "std_test_roc_auc                                                         0.116551   \n",
       "rank_test_roc_auc                                                              40   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       6   \n",
       "\n",
       "                                                                              32   \\\n",
       "mean_fit_time                                                              0.0202   \n",
       "std_fit_time                                                             0.001203   \n",
       "mean_score_time                                                          0.017438   \n",
       "std_score_time                                                           0.000476   \n",
       "param_clf__C                                                                  0.1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__C': 0.1, 'clf__class_weight': None, 'sm...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              117   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           117   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  117   \n",
       "split0_test_roc_auc                                                      0.442982   \n",
       "split1_test_roc_auc                                                      0.650232   \n",
       "split2_test_roc_auc                                                      0.790139   \n",
       "mean_test_roc_auc                                                        0.627784   \n",
       "std_test_roc_auc                                                         0.142612   \n",
       "rank_test_roc_auc                                                              96   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       6   \n",
       "\n",
       "                                                                              53   \\\n",
       "mean_fit_time                                                            0.024515   \n",
       "std_fit_time                                                             0.002955   \n",
       "mean_score_time                                                          0.022126   \n",
       "std_score_time                                                           0.002205   \n",
       "param_clf__C                                                                  0.1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__C': 0.1, 'clf__class_weight': None, 'sm...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              117   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           117   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  117   \n",
       "split0_test_roc_auc                                                      0.582018   \n",
       "split1_test_roc_auc                                                       0.63759   \n",
       "split2_test_roc_auc                                                      0.673409   \n",
       "mean_test_roc_auc                                                        0.631005   \n",
       "std_test_roc_auc                                                           0.0376   \n",
       "rank_test_roc_auc                                                              91   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       6   \n",
       "\n",
       "                                                                              34   \\\n",
       "mean_fit_time                                                            0.024832   \n",
       "std_fit_time                                                             0.005527   \n",
       "mean_score_time                                                           0.02433   \n",
       "std_score_time                                                            0.00299   \n",
       "param_clf__C                                                                  0.1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__C': 0.1, 'clf__class_weight': None, 'sm...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              117   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           117   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  117   \n",
       "split0_test_roc_auc                                                      0.639912   \n",
       "split1_test_roc_auc                                                      0.801096   \n",
       "split2_test_roc_auc                                                      0.778761   \n",
       "mean_test_roc_auc                                                        0.739923   \n",
       "std_test_roc_auc                                                         0.071304   \n",
       "rank_test_roc_auc                                                               4   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       6   \n",
       "\n",
       "                                                                              106  \\\n",
       "mean_fit_time                                                            0.023152   \n",
       "std_fit_time                                                             0.002242   \n",
       "mean_score_time                                                          0.019641   \n",
       "std_score_time                                                           0.002245   \n",
       "param_clf__C                                                                    1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__C': 1, 'clf__class_weight': None, 'smt_...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              117   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           117   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  117   \n",
       "split0_test_roc_auc                                                      0.589035   \n",
       "split1_test_roc_auc                                                      0.783397   \n",
       "split2_test_roc_auc                                                       0.77539   \n",
       "mean_test_roc_auc                                                         0.71594   \n",
       "std_test_roc_auc                                                         0.089795   \n",
       "rank_test_roc_auc                                                              35   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       6   \n",
       "\n",
       "                                                                              105  \\\n",
       "mean_fit_time                                                            0.026251   \n",
       "std_fit_time                                                             0.004533   \n",
       "mean_score_time                                                          0.021128   \n",
       "std_score_time                                                           0.002393   \n",
       "param_clf__C                                                                    1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__C': 1, 'clf__class_weight': None, 'smt_...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              117   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           117   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  117   \n",
       "split0_test_roc_auc                                                      0.595614   \n",
       "split1_test_roc_auc                                                      0.786346   \n",
       "split2_test_roc_auc                                                      0.757269   \n",
       "mean_test_roc_auc                                                        0.713077   \n",
       "std_test_roc_auc                                                         0.083903   \n",
       "rank_test_roc_auc                                                              37   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       6   \n",
       "\n",
       "                                                                              104  \\\n",
       "mean_fit_time                                                            0.022609   \n",
       "std_fit_time                                                             0.001931   \n",
       "mean_score_time                                                           0.02044   \n",
       "std_score_time                                                           0.001944   \n",
       "param_clf__C                                                                    1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__C': 1, 'clf__class_weight': None, 'smt_...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              117   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           117   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  117   \n",
       "split0_test_roc_auc                                                      0.623246   \n",
       "split1_test_roc_auc                                                      0.741677   \n",
       "split2_test_roc_auc                                                      0.756005   \n",
       "mean_test_roc_auc                                                        0.706976   \n",
       "std_test_roc_auc                                                         0.059495   \n",
       "rank_test_roc_auc                                                              41   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       6   \n",
       "\n",
       "                                                                              103  \\\n",
       "mean_fit_time                                                            0.022343   \n",
       "std_fit_time                                                             0.002682   \n",
       "mean_score_time                                                          0.021674   \n",
       "std_score_time                                                            0.00321   \n",
       "param_clf__C                                                                    1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__C': 1, 'clf__class_weight': None, 'smt_...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              117   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           117   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  117   \n",
       "split0_test_roc_auc                                                      0.595614   \n",
       "split1_test_roc_auc                                                      0.815424   \n",
       "split2_test_roc_auc                                                      0.783397   \n",
       "mean_test_roc_auc                                                        0.731478   \n",
       "std_test_roc_auc                                                         0.096956   \n",
       "rank_test_roc_auc                                                              15   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       6   \n",
       "\n",
       "                                                                              102  \\\n",
       "mean_fit_time                                                            0.021059   \n",
       "std_fit_time                                                             0.002958   \n",
       "mean_score_time                                                          0.022188   \n",
       "std_score_time                                                           0.003264   \n",
       "param_clf__C                                                                    1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__C': 1, 'clf__class_weight': None, 'smt_...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              117   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           117   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  117   \n",
       "split0_test_roc_auc                                                      0.605702   \n",
       "split1_test_roc_auc                                                      0.799831   \n",
       "split2_test_roc_auc                                                      0.802781   \n",
       "mean_test_roc_auc                                                        0.736105   \n",
       "std_test_roc_auc                                                         0.092217   \n",
       "rank_test_roc_auc                                                               9   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       6   \n",
       "\n",
       "                                                                              99   \\\n",
       "mean_fit_time                                                            0.021171   \n",
       "std_fit_time                                                             0.001243   \n",
       "mean_score_time                                                          0.016667   \n",
       "std_score_time                                                            0.00238   \n",
       "param_clf__C                                                                    1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__C': 1, 'clf__class_weight': None, 'smt_...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              117   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           117   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  117   \n",
       "split0_test_roc_auc                                                         0.575   \n",
       "split1_test_roc_auc                                                      0.788032   \n",
       "split2_test_roc_auc                                                      0.809945   \n",
       "mean_test_roc_auc                                                        0.724326   \n",
       "std_test_roc_auc                                                         0.105968   \n",
       "rank_test_roc_auc                                                              26   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       6   \n",
       "\n",
       "                                                                              96   \\\n",
       "mean_fit_time                                                            0.022901   \n",
       "std_fit_time                                                             0.003032   \n",
       "mean_score_time                                                          0.019712   \n",
       "std_score_time                                                           0.002258   \n",
       "param_clf__C                                                                    1   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__C': 1, 'clf__class_weight': None, 'smt_...   \n",
       "split0_test_recall                                                            0.0   \n",
       "split1_test_recall                                                            0.0   \n",
       "split2_test_recall                                                            0.0   \n",
       "mean_test_recall                                                              0.0   \n",
       "std_test_recall                                                               0.0   \n",
       "rank_test_recall                                                              117   \n",
       "split0_test_precision                                                         0.0   \n",
       "split1_test_precision                                                         0.0   \n",
       "split2_test_precision                                                         0.0   \n",
       "mean_test_precision                                                           0.0   \n",
       "std_test_precision                                                            0.0   \n",
       "rank_test_precision                                                           117   \n",
       "split0_test_f1                                                                0.0   \n",
       "split1_test_f1                                                                0.0   \n",
       "split2_test_f1                                                                0.0   \n",
       "mean_test_f1                                                                  0.0   \n",
       "std_test_f1                                                                   0.0   \n",
       "rank_test_f1                                                                  117   \n",
       "split0_test_roc_auc                                                      0.639474   \n",
       "split1_test_roc_auc                                                      0.807417   \n",
       "split2_test_roc_auc                                                      0.771176   \n",
       "mean_test_roc_auc                                                        0.739355   \n",
       "std_test_roc_auc                                                          0.07216   \n",
       "rank_test_roc_auc                                                               5   \n",
       "split0_test_recall_weighted                                              0.850746   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.845771   \n",
       "std_test_recall_weighted                                                 0.003518   \n",
       "rank_test_recall_weighted                                                       6   \n",
       "\n",
       "                                                                              33   \n",
       "mean_fit_time                                                            0.022465  \n",
       "std_fit_time                                                             0.000615  \n",
       "mean_score_time                                                          0.020671  \n",
       "std_score_time                                                           0.001471  \n",
       "param_clf__C                                                                  0.1  \n",
       "param_clf__class_weight                                                      None  \n",
       "param_smt__k_neighbors                                                          1  \n",
       "param_smt__sampling_strategy                                                  0.4  \n",
       "param_under__sampling_strategy                                                0.5  \n",
       "params                          {'clf__C': 0.1, 'clf__class_weight': None, 'sm...  \n",
       "split0_test_recall                                                            0.0  \n",
       "split1_test_recall                                                            0.0  \n",
       "split2_test_recall                                                            0.0  \n",
       "mean_test_recall                                                              0.0  \n",
       "std_test_recall                                                               0.0  \n",
       "rank_test_recall                                                              117  \n",
       "split0_test_precision                                                         0.0  \n",
       "split1_test_precision                                                         0.0  \n",
       "split2_test_precision                                                         0.0  \n",
       "mean_test_precision                                                           0.0  \n",
       "std_test_precision                                                            0.0  \n",
       "rank_test_precision                                                           117  \n",
       "split0_test_f1                                                                0.0  \n",
       "split1_test_f1                                                                0.0  \n",
       "split2_test_f1                                                                0.0  \n",
       "mean_test_f1                                                                  0.0  \n",
       "std_test_f1                                                                   0.0  \n",
       "rank_test_f1                                                                  117  \n",
       "split0_test_roc_auc                                                      0.632018  \n",
       "split1_test_roc_auc                                                      0.789296  \n",
       "split2_test_roc_auc                                                      0.758112  \n",
       "mean_test_roc_auc                                                        0.726475  \n",
       "std_test_roc_auc                                                         0.067994  \n",
       "rank_test_roc_auc                                                              22  \n",
       "split0_test_recall_weighted                                              0.850746  \n",
       "split1_test_recall_weighted                                              0.843284  \n",
       "split2_test_recall_weighted                                              0.843284  \n",
       "mean_test_recall_weighted                                                0.845771  \n",
       "std_test_recall_weighted                                                 0.003518  \n",
       "rank_test_recall_weighted                                                       6  \n",
       "\n",
       "[40 rows x 162 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "pipeline = IMBLpipeline([\n",
    "    ('smt', SMOTE()), \n",
    "    ('under', RandomUnderSampler()), \n",
    "    ('clf', SVC())\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "            'clf__C': [0.1, 1, 10],  \n",
    "            # 'clf__kernel': ['linear', 'rbf', 'poly'],\n",
    "            'clf__class_weight': ['balanced', None],\n",
    "            'smt__sampling_strategy': [ 0.2, 0.3, 0.4],\n",
    "            'under__sampling_strategy': [0.5, 0.6, 0.7],\n",
    "            'smt__k_neighbors': [1, 3, 5]\n",
    "             }\n",
    "nfolds = 3\n",
    "scores = ['recall', 'precision', 'f1', 'roc_auc', 'recall_weighted']\n",
    "gscv_svm = GridSearchCV(pipeline, \n",
    "                    parameters, \n",
    "                    scoring=scores,\n",
    "                    cv=StratifiedKFold(n_splits=nfolds, shuffle=True),\n",
    "                    return_train_score=False, \n",
    "                    verbose=1, \n",
    "                    refit=\"f1\",\n",
    "                    n_jobs=-1)\n",
    "resultsGSCV = gscv_svm.fit(X, y)\n",
    "\n",
    "# Get the classifier name from the pipeline\n",
    "classifier_name = resultsGSCV.best_estimator_.named_steps['clf'].__class__.__name__\n",
    "    \n",
    "# save the best estimator\n",
    "save_best_estimator(gscv_svm, classifier_name, notebook_name)\n",
    "\n",
    "results = pd.DataFrame(resultsGSCV.cv_results_)\n",
    "display(results.sort_values(by=[f'rank_test_f1']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecd040b2-10bc-4183-a99a-ede567383e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.89      0.69      0.78       147\n",
      "         poz       0.23      0.54      0.33        26\n",
      "\n",
      "    accuracy                           0.66       173\n",
      "   macro avg       0.56      0.61      0.55       173\n",
      "weighted avg       0.79      0.66      0.71       173\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on test data\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = gscv_svm.best_estimator_.steps[2][1].predict(X_eval)\n",
    "SVM_classification_report = classification_report(y_eval, y_pred)\n",
    "\n",
    "# Create classification report as dictionary\n",
    "SVM_report_dict = classification_report(y_eval, y_pred, output_dict=True)\n",
    "\n",
    "print(classification_report(y_eval, y_pred, target_names=[\"neg\", \"poz\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1735f33-e739-4f9a-bb3c-d87f98d21466",
   "metadata": {},
   "source": [
    "## Decision Tree Model (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf64c198-6799-4e17-838e-e52009105996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 486 candidates, totalling 1458 fits\n",
      "Best estimator saved as: objects/estimators/DecisionTreeClassifier-HAB_modelling_5_7-10042023_1007.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>252</th>\n",
       "      <th>128</th>\n",
       "      <th>319</th>\n",
       "      <th>342</th>\n",
       "      <th>433</th>\n",
       "      <th>276</th>\n",
       "      <th>254</th>\n",
       "      <th>68</th>\n",
       "      <th>232</th>\n",
       "      <th>277</th>\n",
       "      <th>397</th>\n",
       "      <th>381</th>\n",
       "      <th>244</th>\n",
       "      <th>36</th>\n",
       "      <th>442</th>\n",
       "      <th>408</th>\n",
       "      <th>279</th>\n",
       "      <th>...</th>\n",
       "      <th>65</th>\n",
       "      <th>137</th>\n",
       "      <th>485</th>\n",
       "      <th>407</th>\n",
       "      <th>228</th>\n",
       "      <th>226</th>\n",
       "      <th>430</th>\n",
       "      <th>412</th>\n",
       "      <th>191</th>\n",
       "      <th>29</th>\n",
       "      <th>195</th>\n",
       "      <th>170</th>\n",
       "      <th>64</th>\n",
       "      <th>274</th>\n",
       "      <th>463</th>\n",
       "      <th>336</th>\n",
       "      <th>330</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>0.018433</td>\n",
       "      <td>0.018903</td>\n",
       "      <td>0.018499</td>\n",
       "      <td>0.020446</td>\n",
       "      <td>0.018579</td>\n",
       "      <td>0.020675</td>\n",
       "      <td>0.017953</td>\n",
       "      <td>0.018744</td>\n",
       "      <td>0.019812</td>\n",
       "      <td>0.018673</td>\n",
       "      <td>0.018539</td>\n",
       "      <td>0.020448</td>\n",
       "      <td>0.019144</td>\n",
       "      <td>0.017377</td>\n",
       "      <td>0.018533</td>\n",
       "      <td>0.020638</td>\n",
       "      <td>0.018568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019574</td>\n",
       "      <td>0.020686</td>\n",
       "      <td>0.016055</td>\n",
       "      <td>0.018332</td>\n",
       "      <td>0.022093</td>\n",
       "      <td>0.019656</td>\n",
       "      <td>0.019472</td>\n",
       "      <td>0.019176</td>\n",
       "      <td>0.019162</td>\n",
       "      <td>0.018908</td>\n",
       "      <td>0.019968</td>\n",
       "      <td>0.019992</td>\n",
       "      <td>0.018089</td>\n",
       "      <td>0.018518</td>\n",
       "      <td>0.019009</td>\n",
       "      <td>0.019011</td>\n",
       "      <td>0.01879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.002768</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.00015</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.002706</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>0.00138</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.001385</td>\n",
       "      <td>0.002566</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.002313</td>\n",
       "      <td>0.001528</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>0.000658</td>\n",
       "      <td>0.000555</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>0.001894</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>0.00035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.01111</td>\n",
       "      <td>0.011219</td>\n",
       "      <td>0.013217</td>\n",
       "      <td>0.011635</td>\n",
       "      <td>0.012802</td>\n",
       "      <td>0.012218</td>\n",
       "      <td>0.011123</td>\n",
       "      <td>0.011004</td>\n",
       "      <td>0.011532</td>\n",
       "      <td>0.011215</td>\n",
       "      <td>0.011169</td>\n",
       "      <td>0.012775</td>\n",
       "      <td>0.011219</td>\n",
       "      <td>0.012432</td>\n",
       "      <td>0.01098</td>\n",
       "      <td>0.011961</td>\n",
       "      <td>0.011601</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011581</td>\n",
       "      <td>0.012751</td>\n",
       "      <td>0.006884</td>\n",
       "      <td>0.011289</td>\n",
       "      <td>0.01169</td>\n",
       "      <td>0.011816</td>\n",
       "      <td>0.011474</td>\n",
       "      <td>0.010919</td>\n",
       "      <td>0.010783</td>\n",
       "      <td>0.011479</td>\n",
       "      <td>0.011562</td>\n",
       "      <td>0.011447</td>\n",
       "      <td>0.014375</td>\n",
       "      <td>0.01124</td>\n",
       "      <td>0.011236</td>\n",
       "      <td>0.011118</td>\n",
       "      <td>0.011292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.00305</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.002244</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000696</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00023</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.00024</td>\n",
       "      <td>0.003797</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__class_weight</th>\n",
       "      <td>None</td>\n",
       "      <td>balanced</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>balanced</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__criterion</th>\n",
       "      <td>gini</td>\n",
       "      <td>entropy</td>\n",
       "      <td>gini</td>\n",
       "      <td>entropy</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>gini</td>\n",
       "      <td>gini</td>\n",
       "      <td>gini</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>gini</td>\n",
       "      <td>entropy</td>\n",
       "      <td>entropy</td>\n",
       "      <td>gini</td>\n",
       "      <td>gini</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>gini</td>\n",
       "      <td>...</td>\n",
       "      <td>gini</td>\n",
       "      <td>entropy</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>gini</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>gini</td>\n",
       "      <td>gini</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>entropy</td>\n",
       "      <td>entropy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__max_depth</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_smt__k_neighbors</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_smt__sampling_strategy</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_under__sampling_strategy</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_recall</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_recall</th>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_recall</th>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_recall</th>\n",
       "      <td>0.418254</td>\n",
       "      <td>0.435714</td>\n",
       "      <td>0.498413</td>\n",
       "      <td>0.468254</td>\n",
       "      <td>0.515079</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.545238</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.548413</td>\n",
       "      <td>0.481746</td>\n",
       "      <td>0.451587</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.386508</td>\n",
       "      <td>0.531746</td>\n",
       "      <td>0.51746</td>\n",
       "      <td>0.403175</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561905</td>\n",
       "      <td>0.530159</td>\n",
       "      <td>0.385714</td>\n",
       "      <td>0.437302</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.463492</td>\n",
       "      <td>0.336508</td>\n",
       "      <td>0.292063</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.450794</td>\n",
       "      <td>0.384921</td>\n",
       "      <td>0.406349</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0.354762</td>\n",
       "      <td>0.304762</td>\n",
       "      <td>0.160317</td>\n",
       "      <td>0.128571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_recall</th>\n",
       "      <td>0.075701</td>\n",
       "      <td>0.040172</td>\n",
       "      <td>0.090796</td>\n",
       "      <td>0.062492</td>\n",
       "      <td>0.049956</td>\n",
       "      <td>0.215139</td>\n",
       "      <td>0.139416</td>\n",
       "      <td>0.089087</td>\n",
       "      <td>0.019473</td>\n",
       "      <td>0.198219</td>\n",
       "      <td>0.174967</td>\n",
       "      <td>0.070093</td>\n",
       "      <td>0.100572</td>\n",
       "      <td>0.215019</td>\n",
       "      <td>0.202976</td>\n",
       "      <td>0.097228</td>\n",
       "      <td>0.213844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150183</td>\n",
       "      <td>0.133861</td>\n",
       "      <td>0.072008</td>\n",
       "      <td>0.210103</td>\n",
       "      <td>0.131492</td>\n",
       "      <td>0.230722</td>\n",
       "      <td>0.307166</td>\n",
       "      <td>0.108959</td>\n",
       "      <td>0.224972</td>\n",
       "      <td>0.159617</td>\n",
       "      <td>0.306816</td>\n",
       "      <td>0.167924</td>\n",
       "      <td>0.079333</td>\n",
       "      <td>0.09726</td>\n",
       "      <td>0.189999</td>\n",
       "      <td>0.057713</td>\n",
       "      <td>0.118379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_recall</th>\n",
       "      <td>298</td>\n",
       "      <td>272</td>\n",
       "      <td>153</td>\n",
       "      <td>201</td>\n",
       "      <td>129</td>\n",
       "      <td>169</td>\n",
       "      <td>94</td>\n",
       "      <td>42</td>\n",
       "      <td>83</td>\n",
       "      <td>179</td>\n",
       "      <td>231</td>\n",
       "      <td>310</td>\n",
       "      <td>345</td>\n",
       "      <td>102</td>\n",
       "      <td>118</td>\n",
       "      <td>314</td>\n",
       "      <td>145</td>\n",
       "      <td>...</td>\n",
       "      <td>72</td>\n",
       "      <td>105</td>\n",
       "      <td>350</td>\n",
       "      <td>262</td>\n",
       "      <td>304</td>\n",
       "      <td>218</td>\n",
       "      <td>428</td>\n",
       "      <td>465</td>\n",
       "      <td>331</td>\n",
       "      <td>235</td>\n",
       "      <td>353</td>\n",
       "      <td>309</td>\n",
       "      <td>442</td>\n",
       "      <td>390</td>\n",
       "      <td>461</td>\n",
       "      <td>485</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_precision</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.34375</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.30303</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145455</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.218182</td>\n",
       "      <td>0.146341</td>\n",
       "      <td>0.21875</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_precision</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.317073</td>\n",
       "      <td>0.27907</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203704</td>\n",
       "      <td>0.175439</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.183673</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_precision</th>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.207792</td>\n",
       "      <td>0.205479</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.19697</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.246154</td>\n",
       "      <td>0.340426</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.194805</td>\n",
       "      <td>0.229508</td>\n",
       "      <td>0.242857</td>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_precision</th>\n",
       "      <td>0.576797</td>\n",
       "      <td>0.551358</td>\n",
       "      <td>0.451801</td>\n",
       "      <td>0.498347</td>\n",
       "      <td>0.425463</td>\n",
       "      <td>0.5058</td>\n",
       "      <td>0.43739</td>\n",
       "      <td>0.369604</td>\n",
       "      <td>0.388573</td>\n",
       "      <td>0.454675</td>\n",
       "      <td>0.448232</td>\n",
       "      <td>0.549286</td>\n",
       "      <td>0.594136</td>\n",
       "      <td>0.442677</td>\n",
       "      <td>0.447753</td>\n",
       "      <td>0.533147</td>\n",
       "      <td>0.440351</td>\n",
       "      <td>...</td>\n",
       "      <td>0.18565</td>\n",
       "      <td>0.188988</td>\n",
       "      <td>0.232649</td>\n",
       "      <td>0.261169</td>\n",
       "      <td>0.21561</td>\n",
       "      <td>0.194678</td>\n",
       "      <td>0.318021</td>\n",
       "      <td>0.268519</td>\n",
       "      <td>0.241279</td>\n",
       "      <td>0.188055</td>\n",
       "      <td>0.306878</td>\n",
       "      <td>0.235563</td>\n",
       "      <td>0.225368</td>\n",
       "      <td>0.204462</td>\n",
       "      <td>0.289683</td>\n",
       "      <td>0.598039</td>\n",
       "      <td>0.515152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_precision</th>\n",
       "      <td>0.060214</td>\n",
       "      <td>0.057369</td>\n",
       "      <td>0.102301</td>\n",
       "      <td>0.132408</td>\n",
       "      <td>0.112963</td>\n",
       "      <td>0.052965</td>\n",
       "      <td>0.1102</td>\n",
       "      <td>0.105284</td>\n",
       "      <td>0.049991</td>\n",
       "      <td>0.069441</td>\n",
       "      <td>0.134112</td>\n",
       "      <td>0.145102</td>\n",
       "      <td>0.202166</td>\n",
       "      <td>0.12188</td>\n",
       "      <td>0.193945</td>\n",
       "      <td>0.175144</td>\n",
       "      <td>0.043045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028472</td>\n",
       "      <td>0.012439</td>\n",
       "      <td>0.071959</td>\n",
       "      <td>0.08071</td>\n",
       "      <td>0.034275</td>\n",
       "      <td>0.046584</td>\n",
       "      <td>0.049022</td>\n",
       "      <td>0.047213</td>\n",
       "      <td>0.1154</td>\n",
       "      <td>0.03707</td>\n",
       "      <td>0.067373</td>\n",
       "      <td>0.073765</td>\n",
       "      <td>0.072487</td>\n",
       "      <td>0.022706</td>\n",
       "      <td>0.101171</td>\n",
       "      <td>0.296396</td>\n",
       "      <td>0.40881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_precision</th>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>58</td>\n",
       "      <td>26</td>\n",
       "      <td>84</td>\n",
       "      <td>23</td>\n",
       "      <td>73</td>\n",
       "      <td>192</td>\n",
       "      <td>152</td>\n",
       "      <td>54</td>\n",
       "      <td>61</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>68</td>\n",
       "      <td>63</td>\n",
       "      <td>15</td>\n",
       "      <td>69</td>\n",
       "      <td>...</td>\n",
       "      <td>486</td>\n",
       "      <td>484</td>\n",
       "      <td>450</td>\n",
       "      <td>403</td>\n",
       "      <td>466</td>\n",
       "      <td>482</td>\n",
       "      <td>285</td>\n",
       "      <td>384</td>\n",
       "      <td>440</td>\n",
       "      <td>485</td>\n",
       "      <td>310</td>\n",
       "      <td>446</td>\n",
       "      <td>456</td>\n",
       "      <td>475</td>\n",
       "      <td>342</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_f1</th>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.358974</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213333</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.25641</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.196721</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_f1</th>\n",
       "      <td>0.432432</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>...</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.25641</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>0.276923</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_f1</th>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.604651</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.509091</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.651163</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.557377</td>\n",
       "      <td>0.697674</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.542373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>0.319149</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.298851</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.306122</td>\n",
       "      <td>0.341463</td>\n",
       "      <td>0.373626</td>\n",
       "      <td>0.227848</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>0.30303</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_f1</th>\n",
       "      <td>0.48296</td>\n",
       "      <td>0.48226</td>\n",
       "      <td>0.47373</td>\n",
       "      <td>0.46862</td>\n",
       "      <td>0.459628</td>\n",
       "      <td>0.457181</td>\n",
       "      <td>0.455315</td>\n",
       "      <td>0.453205</td>\n",
       "      <td>0.452137</td>\n",
       "      <td>0.447389</td>\n",
       "      <td>0.44601</td>\n",
       "      <td>0.443504</td>\n",
       "      <td>0.440657</td>\n",
       "      <td>0.440611</td>\n",
       "      <td>0.439289</td>\n",
       "      <td>0.437427</td>\n",
       "      <td>0.436466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277732</td>\n",
       "      <td>0.276509</td>\n",
       "      <td>0.274618</td>\n",
       "      <td>0.270434</td>\n",
       "      <td>0.26995</td>\n",
       "      <td>0.269672</td>\n",
       "      <td>0.269551</td>\n",
       "      <td>0.266955</td>\n",
       "      <td>0.26544</td>\n",
       "      <td>0.263753</td>\n",
       "      <td>0.263543</td>\n",
       "      <td>0.263424</td>\n",
       "      <td>0.262097</td>\n",
       "      <td>0.25742</td>\n",
       "      <td>0.240741</td>\n",
       "      <td>0.222399</td>\n",
       "      <td>0.185606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_f1</th>\n",
       "      <td>0.067905</td>\n",
       "      <td>0.012751</td>\n",
       "      <td>0.097174</td>\n",
       "      <td>0.073723</td>\n",
       "      <td>0.07458</td>\n",
       "      <td>0.091534</td>\n",
       "      <td>0.03874</td>\n",
       "      <td>0.103874</td>\n",
       "      <td>0.027413</td>\n",
       "      <td>0.090318</td>\n",
       "      <td>0.154653</td>\n",
       "      <td>0.031593</td>\n",
       "      <td>0.057056</td>\n",
       "      <td>0.082662</td>\n",
       "      <td>0.182717</td>\n",
       "      <td>0.103705</td>\n",
       "      <td>0.089907</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047511</td>\n",
       "      <td>0.030167</td>\n",
       "      <td>0.029298</td>\n",
       "      <td>0.045011</td>\n",
       "      <td>0.018632</td>\n",
       "      <td>0.086738</td>\n",
       "      <td>0.159664</td>\n",
       "      <td>0.063262</td>\n",
       "      <td>0.077575</td>\n",
       "      <td>0.062864</td>\n",
       "      <td>0.089725</td>\n",
       "      <td>0.040445</td>\n",
       "      <td>0.075148</td>\n",
       "      <td>0.042883</td>\n",
       "      <td>0.031966</td>\n",
       "      <td>0.033207</td>\n",
       "      <td>0.153117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_f1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>470</td>\n",
       "      <td>471</td>\n",
       "      <td>472</td>\n",
       "      <td>473</td>\n",
       "      <td>474</td>\n",
       "      <td>475</td>\n",
       "      <td>476</td>\n",
       "      <td>477</td>\n",
       "      <td>478</td>\n",
       "      <td>479</td>\n",
       "      <td>480</td>\n",
       "      <td>481</td>\n",
       "      <td>482</td>\n",
       "      <td>483</td>\n",
       "      <td>484</td>\n",
       "      <td>485</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_roc_auc</th>\n",
       "      <td>0.687719</td>\n",
       "      <td>0.599342</td>\n",
       "      <td>0.60636</td>\n",
       "      <td>0.675439</td>\n",
       "      <td>0.707895</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.704605</td>\n",
       "      <td>0.63114</td>\n",
       "      <td>0.704386</td>\n",
       "      <td>0.654605</td>\n",
       "      <td>0.696272</td>\n",
       "      <td>0.634211</td>\n",
       "      <td>0.553728</td>\n",
       "      <td>0.712719</td>\n",
       "      <td>0.609211</td>\n",
       "      <td>0.641228</td>\n",
       "      <td>0.691886</td>\n",
       "      <td>...</td>\n",
       "      <td>0.477851</td>\n",
       "      <td>0.613596</td>\n",
       "      <td>0.548904</td>\n",
       "      <td>0.607456</td>\n",
       "      <td>0.657018</td>\n",
       "      <td>0.42807</td>\n",
       "      <td>0.625439</td>\n",
       "      <td>0.625439</td>\n",
       "      <td>0.581579</td>\n",
       "      <td>0.524781</td>\n",
       "      <td>0.616447</td>\n",
       "      <td>0.632456</td>\n",
       "      <td>0.458553</td>\n",
       "      <td>0.631798</td>\n",
       "      <td>0.601754</td>\n",
       "      <td>0.635088</td>\n",
       "      <td>0.625439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_roc_auc</th>\n",
       "      <td>0.659503</td>\n",
       "      <td>0.715971</td>\n",
       "      <td>0.742099</td>\n",
       "      <td>0.742099</td>\n",
       "      <td>0.719764</td>\n",
       "      <td>0.55689</td>\n",
       "      <td>0.70944</td>\n",
       "      <td>0.715761</td>\n",
       "      <td>0.700379</td>\n",
       "      <td>0.645596</td>\n",
       "      <td>0.666035</td>\n",
       "      <td>0.73641</td>\n",
       "      <td>0.618837</td>\n",
       "      <td>0.725453</td>\n",
       "      <td>0.636115</td>\n",
       "      <td>0.70649</td>\n",
       "      <td>0.62263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.627054</td>\n",
       "      <td>0.621576</td>\n",
       "      <td>0.611252</td>\n",
       "      <td>0.528445</td>\n",
       "      <td>0.617362</td>\n",
       "      <td>0.621155</td>\n",
       "      <td>0.689423</td>\n",
       "      <td>0.633586</td>\n",
       "      <td>0.607038</td>\n",
       "      <td>0.584492</td>\n",
       "      <td>0.594185</td>\n",
       "      <td>0.658871</td>\n",
       "      <td>0.455542</td>\n",
       "      <td>0.505057</td>\n",
       "      <td>0.609987</td>\n",
       "      <td>0.666035</td>\n",
       "      <td>0.695322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_roc_auc</th>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.750737</td>\n",
       "      <td>0.768015</td>\n",
       "      <td>0.767594</td>\n",
       "      <td>0.698062</td>\n",
       "      <td>0.785504</td>\n",
       "      <td>0.758744</td>\n",
       "      <td>0.769069</td>\n",
       "      <td>0.693637</td>\n",
       "      <td>0.799831</td>\n",
       "      <td>0.823852</td>\n",
       "      <td>0.783607</td>\n",
       "      <td>0.742731</td>\n",
       "      <td>0.800885</td>\n",
       "      <td>0.850611</td>\n",
       "      <td>0.773915</td>\n",
       "      <td>0.788453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.626212</td>\n",
       "      <td>0.675516</td>\n",
       "      <td>0.536452</td>\n",
       "      <td>0.616308</td>\n",
       "      <td>0.626212</td>\n",
       "      <td>0.730931</td>\n",
       "      <td>0.769701</td>\n",
       "      <td>0.564897</td>\n",
       "      <td>0.59271</td>\n",
       "      <td>0.660135</td>\n",
       "      <td>0.652339</td>\n",
       "      <td>0.572903</td>\n",
       "      <td>0.62284</td>\n",
       "      <td>0.582807</td>\n",
       "      <td>0.617151</td>\n",
       "      <td>0.635272</td>\n",
       "      <td>0.665824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_roc_auc</th>\n",
       "      <td>0.703042</td>\n",
       "      <td>0.688684</td>\n",
       "      <td>0.705491</td>\n",
       "      <td>0.728377</td>\n",
       "      <td>0.708573</td>\n",
       "      <td>0.681383</td>\n",
       "      <td>0.724263</td>\n",
       "      <td>0.705323</td>\n",
       "      <td>0.699467</td>\n",
       "      <td>0.700011</td>\n",
       "      <td>0.728719</td>\n",
       "      <td>0.718076</td>\n",
       "      <td>0.638432</td>\n",
       "      <td>0.746352</td>\n",
       "      <td>0.698645</td>\n",
       "      <td>0.707211</td>\n",
       "      <td>0.70099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.577039</td>\n",
       "      <td>0.636896</td>\n",
       "      <td>0.565536</td>\n",
       "      <td>0.58407</td>\n",
       "      <td>0.63353</td>\n",
       "      <td>0.593385</td>\n",
       "      <td>0.694854</td>\n",
       "      <td>0.607974</td>\n",
       "      <td>0.593775</td>\n",
       "      <td>0.589803</td>\n",
       "      <td>0.62099</td>\n",
       "      <td>0.62141</td>\n",
       "      <td>0.512311</td>\n",
       "      <td>0.573221</td>\n",
       "      <td>0.609631</td>\n",
       "      <td>0.645465</td>\n",
       "      <td>0.662195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_roc_auc</th>\n",
       "      <td>0.043187</td>\n",
       "      <td>0.064749</td>\n",
       "      <td>0.070891</td>\n",
       "      <td>0.038853</td>\n",
       "      <td>0.008873</td>\n",
       "      <td>0.094436</td>\n",
       "      <td>0.024462</td>\n",
       "      <td>0.056791</td>\n",
       "      <td>0.004435</td>\n",
       "      <td>0.070679</td>\n",
       "      <td>0.068392</td>\n",
       "      <td>0.062354</td>\n",
       "      <td>0.078394</td>\n",
       "      <td>0.038909</td>\n",
       "      <td>0.108016</td>\n",
       "      <td>0.054172</td>\n",
       "      <td>0.068003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070137</td>\n",
       "      <td>0.027502</td>\n",
       "      <td>0.032723</td>\n",
       "      <td>0.039498</td>\n",
       "      <td>0.016996</td>\n",
       "      <td>0.125192</td>\n",
       "      <td>0.05902</td>\n",
       "      <td>0.030641</td>\n",
       "      <td>0.010421</td>\n",
       "      <td>0.055386</td>\n",
       "      <td>0.023958</td>\n",
       "      <td>0.035955</td>\n",
       "      <td>0.078165</td>\n",
       "      <td>0.052184</td>\n",
       "      <td>0.006291</td>\n",
       "      <td>0.014545</td>\n",
       "      <td>0.028645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_roc_auc</th>\n",
       "      <td>38</td>\n",
       "      <td>84</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>111</td>\n",
       "      <td>11</td>\n",
       "      <td>33</td>\n",
       "      <td>45</td>\n",
       "      <td>44</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>334</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>27</td>\n",
       "      <td>42</td>\n",
       "      <td>...</td>\n",
       "      <td>475</td>\n",
       "      <td>341</td>\n",
       "      <td>480</td>\n",
       "      <td>471</td>\n",
       "      <td>349</td>\n",
       "      <td>458</td>\n",
       "      <td>66</td>\n",
       "      <td>429</td>\n",
       "      <td>457</td>\n",
       "      <td>463</td>\n",
       "      <td>395</td>\n",
       "      <td>391</td>\n",
       "      <td>486</td>\n",
       "      <td>477</td>\n",
       "      <td>426</td>\n",
       "      <td>291</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_recall_weighted</th>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.798507</td>\n",
       "      <td>0.761194</td>\n",
       "      <td>0.768657</td>\n",
       "      <td>0.858209</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.761194</td>\n",
       "      <td>0.776119</td>\n",
       "      <td>0.813433</td>\n",
       "      <td>0.80597</td>\n",
       "      <td>0.783582</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.753731</td>\n",
       "      <td>0.597015</td>\n",
       "      <td>0.761194</td>\n",
       "      <td>0.813433</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559701</td>\n",
       "      <td>0.649254</td>\n",
       "      <td>0.80597</td>\n",
       "      <td>0.626866</td>\n",
       "      <td>0.783582</td>\n",
       "      <td>0.686567</td>\n",
       "      <td>0.828358</td>\n",
       "      <td>0.701493</td>\n",
       "      <td>0.679104</td>\n",
       "      <td>0.664179</td>\n",
       "      <td>0.791045</td>\n",
       "      <td>0.619403</td>\n",
       "      <td>0.634328</td>\n",
       "      <td>0.716418</td>\n",
       "      <td>0.791045</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.865672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_recall_weighted</th>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.813433</td>\n",
       "      <td>0.858209</td>\n",
       "      <td>0.791045</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.731343</td>\n",
       "      <td>0.701493</td>\n",
       "      <td>0.776119</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.80597</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.858209</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.604478</td>\n",
       "      <td>0.567164</td>\n",
       "      <td>0.656716</td>\n",
       "      <td>0.828358</td>\n",
       "      <td>0.61194</td>\n",
       "      <td>0.649254</td>\n",
       "      <td>0.828358</td>\n",
       "      <td>0.798507</td>\n",
       "      <td>0.820896</td>\n",
       "      <td>0.61194</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.813433</td>\n",
       "      <td>0.738806</td>\n",
       "      <td>0.701493</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.791045</td>\n",
       "      <td>0.850746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_recall_weighted</th>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.873134</td>\n",
       "      <td>0.873134</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.80597</td>\n",
       "      <td>0.798507</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.828358</td>\n",
       "      <td>0.820896</td>\n",
       "      <td>0.88806</td>\n",
       "      <td>0.873134</td>\n",
       "      <td>0.80597</td>\n",
       "      <td>0.798507</td>\n",
       "      <td>0.902985</td>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.798507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507463</td>\n",
       "      <td>0.522388</td>\n",
       "      <td>0.552239</td>\n",
       "      <td>0.544776</td>\n",
       "      <td>0.574627</td>\n",
       "      <td>0.597015</td>\n",
       "      <td>0.731343</td>\n",
       "      <td>0.791045</td>\n",
       "      <td>0.492537</td>\n",
       "      <td>0.597015</td>\n",
       "      <td>0.574627</td>\n",
       "      <td>0.544776</td>\n",
       "      <td>0.768657</td>\n",
       "      <td>0.656716</td>\n",
       "      <td>0.552239</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.843284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_recall_weighted</th>\n",
       "      <td>0.863184</td>\n",
       "      <td>0.855721</td>\n",
       "      <td>0.828358</td>\n",
       "      <td>0.830846</td>\n",
       "      <td>0.808458</td>\n",
       "      <td>0.838308</td>\n",
       "      <td>0.798507</td>\n",
       "      <td>0.771144</td>\n",
       "      <td>0.793532</td>\n",
       "      <td>0.828358</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.840796</td>\n",
       "      <td>0.848259</td>\n",
       "      <td>0.803483</td>\n",
       "      <td>0.778607</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.818408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557214</td>\n",
       "      <td>0.579602</td>\n",
       "      <td>0.671642</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.656716</td>\n",
       "      <td>0.644279</td>\n",
       "      <td>0.79602</td>\n",
       "      <td>0.763682</td>\n",
       "      <td>0.664179</td>\n",
       "      <td>0.624378</td>\n",
       "      <td>0.733831</td>\n",
       "      <td>0.659204</td>\n",
       "      <td>0.71393</td>\n",
       "      <td>0.691542</td>\n",
       "      <td>0.726368</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.853234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_recall_weighted</th>\n",
       "      <td>0.015334</td>\n",
       "      <td>0.014072</td>\n",
       "      <td>0.032242</td>\n",
       "      <td>0.049627</td>\n",
       "      <td>0.041476</td>\n",
       "      <td>0.023069</td>\n",
       "      <td>0.054839</td>\n",
       "      <td>0.061337</td>\n",
       "      <td>0.024626</td>\n",
       "      <td>0.016121</td>\n",
       "      <td>0.038697</td>\n",
       "      <td>0.040571</td>\n",
       "      <td>0.036728</td>\n",
       "      <td>0.042798</td>\n",
       "      <td>0.1313</td>\n",
       "      <td>0.05312</td>\n",
       "      <td>0.018615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039645</td>\n",
       "      <td>0.052534</td>\n",
       "      <td>0.104122</td>\n",
       "      <td>0.119144</td>\n",
       "      <td>0.090992</td>\n",
       "      <td>0.036728</td>\n",
       "      <td>0.045733</td>\n",
       "      <td>0.04408</td>\n",
       "      <td>0.134466</td>\n",
       "      <td>0.028796</td>\n",
       "      <td>0.114049</td>\n",
       "      <td>0.113232</td>\n",
       "      <td>0.057591</td>\n",
       "      <td>0.025368</td>\n",
       "      <td>0.124478</td>\n",
       "      <td>0.031268</td>\n",
       "      <td>0.009308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_recall_weighted</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>38</td>\n",
       "      <td>94</td>\n",
       "      <td>22</td>\n",
       "      <td>119</td>\n",
       "      <td>197</td>\n",
       "      <td>135</td>\n",
       "      <td>46</td>\n",
       "      <td>29</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>105</td>\n",
       "      <td>178</td>\n",
       "      <td>26</td>\n",
       "      <td>71</td>\n",
       "      <td>...</td>\n",
       "      <td>485</td>\n",
       "      <td>479</td>\n",
       "      <td>403</td>\n",
       "      <td>411</td>\n",
       "      <td>423</td>\n",
       "      <td>434</td>\n",
       "      <td>130</td>\n",
       "      <td>221</td>\n",
       "      <td>416</td>\n",
       "      <td>458</td>\n",
       "      <td>287</td>\n",
       "      <td>420</td>\n",
       "      <td>322</td>\n",
       "      <td>366</td>\n",
       "      <td>305</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41 rows × 486 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              252  \\\n",
       "mean_fit_time                                                            0.018433   \n",
       "std_fit_time                                                             0.000439   \n",
       "mean_score_time                                                           0.01111   \n",
       "std_score_time                                                           0.000112   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__max_depth                                                            2   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.35   \n",
       "split1_test_recall                                                       0.380952   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.418254   \n",
       "std_test_recall                                                          0.075701   \n",
       "rank_test_recall                                                              298   \n",
       "split0_test_precision                                                    0.583333   \n",
       "split1_test_precision                                                         0.5   \n",
       "split2_test_precision                                                    0.647059   \n",
       "mean_test_precision                                                      0.576797   \n",
       "std_test_precision                                                       0.060214   \n",
       "rank_test_precision                                                             9   \n",
       "split0_test_f1                                                             0.4375   \n",
       "split1_test_f1                                                           0.432432   \n",
       "split2_test_f1                                                           0.578947   \n",
       "mean_test_f1                                                              0.48296   \n",
       "std_test_f1                                                              0.067905   \n",
       "rank_test_f1                                                                    1   \n",
       "split0_test_roc_auc                                                      0.687719   \n",
       "split1_test_roc_auc                                                      0.659503   \n",
       "split2_test_roc_auc                                                      0.761905   \n",
       "mean_test_roc_auc                                                        0.703042   \n",
       "std_test_roc_auc                                                         0.043187   \n",
       "rank_test_roc_auc                                                              38   \n",
       "split0_test_recall_weighted                                              0.865672   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.880597   \n",
       "mean_test_recall_weighted                                                0.863184   \n",
       "std_test_recall_weighted                                                 0.015334   \n",
       "rank_test_recall_weighted                                                       1   \n",
       "\n",
       "                                                                              128  \\\n",
       "mean_fit_time                                                            0.018903   \n",
       "std_fit_time                                                             0.000141   \n",
       "mean_score_time                                                          0.011219   \n",
       "std_score_time                                                           0.000055   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__max_depth                                                            3   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                       0.380952   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.435714   \n",
       "std_test_recall                                                          0.040172   \n",
       "rank_test_recall                                                              272   \n",
       "split0_test_precision                                                      0.5625   \n",
       "split1_test_precision                                                    0.615385   \n",
       "split2_test_precision                                                     0.47619   \n",
       "mean_test_precision                                                      0.551358   \n",
       "std_test_precision                                                       0.057369   \n",
       "rank_test_precision                                                            11   \n",
       "split0_test_f1                                                                0.5   \n",
       "split1_test_f1                                                           0.470588   \n",
       "split2_test_f1                                                            0.47619   \n",
       "mean_test_f1                                                              0.48226   \n",
       "std_test_f1                                                              0.012751   \n",
       "rank_test_f1                                                                    2   \n",
       "split0_test_roc_auc                                                      0.599342   \n",
       "split1_test_roc_auc                                                      0.715971   \n",
       "split2_test_roc_auc                                                      0.750737   \n",
       "mean_test_roc_auc                                                        0.688684   \n",
       "std_test_roc_auc                                                         0.064749   \n",
       "rank_test_roc_auc                                                              84   \n",
       "split0_test_recall_weighted                                              0.865672   \n",
       "split1_test_recall_weighted                                              0.865672   \n",
       "split2_test_recall_weighted                                              0.835821   \n",
       "mean_test_recall_weighted                                                0.855721   \n",
       "std_test_recall_weighted                                                 0.014072   \n",
       "rank_test_recall_weighted                                                       2   \n",
       "\n",
       "                                                                              319  \\\n",
       "mean_fit_time                                                            0.018499   \n",
       "std_fit_time                                                             0.000124   \n",
       "mean_score_time                                                          0.013217   \n",
       "std_score_time                                                            0.00305   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__max_depth                                                            4   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.4   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                       0.619048   \n",
       "mean_test_recall                                                         0.498413   \n",
       "std_test_recall                                                          0.090796   \n",
       "rank_test_recall                                                              153   \n",
       "split0_test_precision                                                    0.347826   \n",
       "split1_test_precision                                                    0.416667   \n",
       "split2_test_precision                                                    0.590909   \n",
       "mean_test_precision                                                      0.451801   \n",
       "std_test_precision                                                       0.102301   \n",
       "rank_test_precision                                                            58   \n",
       "split0_test_f1                                                           0.372093   \n",
       "split1_test_f1                                                           0.444444   \n",
       "split2_test_f1                                                           0.604651   \n",
       "mean_test_f1                                                              0.47373   \n",
       "std_test_f1                                                              0.097174   \n",
       "rank_test_f1                                                                    3   \n",
       "split0_test_roc_auc                                                       0.60636   \n",
       "split1_test_roc_auc                                                      0.742099   \n",
       "split2_test_roc_auc                                                      0.768015   \n",
       "mean_test_roc_auc                                                        0.705491   \n",
       "std_test_roc_auc                                                         0.070891   \n",
       "rank_test_roc_auc                                                              32   \n",
       "split0_test_recall_weighted                                              0.798507   \n",
       "split1_test_recall_weighted                                              0.813433   \n",
       "split2_test_recall_weighted                                              0.873134   \n",
       "mean_test_recall_weighted                                                0.828358   \n",
       "std_test_recall_weighted                                                 0.032242   \n",
       "rank_test_recall_weighted                                                      49   \n",
       "\n",
       "                                                                              342  \\\n",
       "mean_fit_time                                                            0.020446   \n",
       "std_fit_time                                                               0.0028   \n",
       "mean_score_time                                                          0.011635   \n",
       "std_score_time                                                           0.000405   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__max_depth                                                            2   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                       0.380952   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.468254   \n",
       "std_test_recall                                                          0.062492   \n",
       "rank_test_recall                                                              201   \n",
       "split0_test_precision                                                      0.3125   \n",
       "split1_test_precision                                                    0.571429   \n",
       "split2_test_precision                                                    0.611111   \n",
       "mean_test_precision                                                      0.498347   \n",
       "std_test_precision                                                       0.132408   \n",
       "rank_test_precision                                                            26   \n",
       "split0_test_f1                                                           0.384615   \n",
       "split1_test_f1                                                           0.457143   \n",
       "split2_test_f1                                                           0.564103   \n",
       "mean_test_f1                                                              0.46862   \n",
       "std_test_f1                                                              0.073723   \n",
       "rank_test_f1                                                                    4   \n",
       "split0_test_roc_auc                                                      0.675439   \n",
       "split1_test_roc_auc                                                      0.742099   \n",
       "split2_test_roc_auc                                                      0.767594   \n",
       "mean_test_roc_auc                                                        0.728377   \n",
       "std_test_roc_auc                                                         0.038853   \n",
       "rank_test_roc_auc                                                               7   \n",
       "split0_test_recall_weighted                                              0.761194   \n",
       "split1_test_recall_weighted                                              0.858209   \n",
       "split2_test_recall_weighted                                              0.873134   \n",
       "mean_test_recall_weighted                                                0.830846   \n",
       "std_test_recall_weighted                                                 0.049627   \n",
       "rank_test_recall_weighted                                                      38   \n",
       "\n",
       "                                                                              433  \\\n",
       "mean_fit_time                                                            0.018579   \n",
       "std_fit_time                                                             0.000329   \n",
       "mean_score_time                                                          0.012802   \n",
       "std_score_time                                                           0.002244   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__max_depth                                                            3   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                       0.571429   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.515079   \n",
       "std_test_recall                                                          0.049956   \n",
       "rank_test_recall                                                              129   \n",
       "split0_test_precision                                                    0.310345   \n",
       "split1_test_precision                                                    0.387097   \n",
       "split2_test_precision                                                    0.578947   \n",
       "mean_test_precision                                                      0.425463   \n",
       "std_test_precision                                                       0.112963   \n",
       "rank_test_precision                                                            84   \n",
       "split0_test_f1                                                           0.367347   \n",
       "split1_test_f1                                                           0.461538   \n",
       "split2_test_f1                                                               0.55   \n",
       "mean_test_f1                                                             0.459628   \n",
       "std_test_f1                                                               0.07458   \n",
       "rank_test_f1                                                                    5   \n",
       "split0_test_roc_auc                                                      0.707895   \n",
       "split1_test_roc_auc                                                      0.719764   \n",
       "split2_test_roc_auc                                                      0.698062   \n",
       "mean_test_roc_auc                                                        0.708573   \n",
       "std_test_roc_auc                                                         0.008873   \n",
       "rank_test_roc_auc                                                              25   \n",
       "split0_test_recall_weighted                                              0.768657   \n",
       "split1_test_recall_weighted                                              0.791045   \n",
       "split2_test_recall_weighted                                              0.865672   \n",
       "mean_test_recall_weighted                                                0.808458   \n",
       "std_test_recall_weighted                                                 0.041476   \n",
       "rank_test_recall_weighted                                                      94   \n",
       "\n",
       "                                                                              276  \\\n",
       "mean_fit_time                                                            0.020675   \n",
       "std_fit_time                                                             0.002768   \n",
       "mean_score_time                                                          0.012218   \n",
       "std_score_time                                                           0.001561   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__max_depth                                                            3   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                       0.238095   \n",
       "split2_test_recall                                                       0.761905   \n",
       "mean_test_recall                                                         0.483333   \n",
       "std_test_recall                                                          0.215139   \n",
       "rank_test_recall                                                              169   \n",
       "split0_test_precision                                                    0.529412   \n",
       "split1_test_precision                                                    0.555556   \n",
       "split2_test_precision                                                    0.432432   \n",
       "mean_test_precision                                                        0.5058   \n",
       "std_test_precision                                                       0.052965   \n",
       "rank_test_precision                                                            23   \n",
       "split0_test_f1                                                           0.486486   \n",
       "split1_test_f1                                                           0.333333   \n",
       "split2_test_f1                                                           0.551724   \n",
       "mean_test_f1                                                             0.457181   \n",
       "std_test_f1                                                              0.091534   \n",
       "rank_test_f1                                                                    6   \n",
       "split0_test_roc_auc                                                      0.701754   \n",
       "split1_test_roc_auc                                                       0.55689   \n",
       "split2_test_roc_auc                                                      0.785504   \n",
       "mean_test_roc_auc                                                        0.681383   \n",
       "std_test_roc_auc                                                         0.094436   \n",
       "rank_test_roc_auc                                                             111   \n",
       "split0_test_recall_weighted                                              0.858209   \n",
       "split1_test_recall_weighted                                              0.850746   \n",
       "split2_test_recall_weighted                                               0.80597   \n",
       "mean_test_recall_weighted                                                0.838308   \n",
       "std_test_recall_weighted                                                 0.023069   \n",
       "rank_test_recall_weighted                                                      22   \n",
       "\n",
       "                                                                              254  \\\n",
       "mean_fit_time                                                            0.017953   \n",
       "std_fit_time                                                             0.000143   \n",
       "mean_score_time                                                          0.011123   \n",
       "std_score_time                                                           0.000106   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__max_depth                                                            2   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.35   \n",
       "split1_test_recall                                                       0.619048   \n",
       "split2_test_recall                                                       0.666667   \n",
       "mean_test_recall                                                         0.545238   \n",
       "std_test_recall                                                          0.139416   \n",
       "rank_test_recall                                                               94   \n",
       "split0_test_precision                                                    0.583333   \n",
       "split1_test_precision                                                    0.317073   \n",
       "split2_test_precision                                                    0.411765   \n",
       "mean_test_precision                                                       0.43739   \n",
       "std_test_precision                                                         0.1102   \n",
       "rank_test_precision                                                            73   \n",
       "split0_test_f1                                                             0.4375   \n",
       "split1_test_f1                                                           0.419355   \n",
       "split2_test_f1                                                           0.509091   \n",
       "mean_test_f1                                                             0.455315   \n",
       "std_test_f1                                                               0.03874   \n",
       "rank_test_f1                                                                    7   \n",
       "split0_test_roc_auc                                                      0.704605   \n",
       "split1_test_roc_auc                                                       0.70944   \n",
       "split2_test_roc_auc                                                      0.758744   \n",
       "mean_test_roc_auc                                                        0.724263   \n",
       "std_test_roc_auc                                                         0.024462   \n",
       "rank_test_roc_auc                                                              11   \n",
       "split0_test_recall_weighted                                              0.865672   \n",
       "split1_test_recall_weighted                                              0.731343   \n",
       "split2_test_recall_weighted                                              0.798507   \n",
       "mean_test_recall_weighted                                                0.798507   \n",
       "std_test_recall_weighted                                                 0.054839   \n",
       "rank_test_recall_weighted                                                     119   \n",
       "\n",
       "                                                                              68   \\\n",
       "mean_fit_time                                                            0.018744   \n",
       "std_fit_time                                                             0.000042   \n",
       "mean_score_time                                                          0.011004   \n",
       "std_score_time                                                           0.000021   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__max_depth                                                            4   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                       0.571429   \n",
       "split2_test_recall                                                       0.714286   \n",
       "mean_test_recall                                                         0.595238   \n",
       "std_test_recall                                                          0.089087   \n",
       "rank_test_recall                                                               42   \n",
       "split0_test_precision                                                      0.3125   \n",
       "split1_test_precision                                                     0.27907   \n",
       "split2_test_precision                                                    0.517241   \n",
       "mean_test_precision                                                      0.369604   \n",
       "std_test_precision                                                       0.105284   \n",
       "rank_test_precision                                                           192   \n",
       "split0_test_f1                                                           0.384615   \n",
       "split1_test_f1                                                              0.375   \n",
       "split2_test_f1                                                                0.6   \n",
       "mean_test_f1                                                             0.453205   \n",
       "std_test_f1                                                              0.103874   \n",
       "rank_test_f1                                                                    8   \n",
       "split0_test_roc_auc                                                       0.63114   \n",
       "split1_test_roc_auc                                                      0.715761   \n",
       "split2_test_roc_auc                                                      0.769069   \n",
       "mean_test_roc_auc                                                        0.705323   \n",
       "std_test_roc_auc                                                         0.056791   \n",
       "rank_test_roc_auc                                                              33   \n",
       "split0_test_recall_weighted                                              0.761194   \n",
       "split1_test_recall_weighted                                              0.701493   \n",
       "split2_test_recall_weighted                                              0.850746   \n",
       "mean_test_recall_weighted                                                0.771144   \n",
       "std_test_recall_weighted                                                 0.061337   \n",
       "rank_test_recall_weighted                                                     197   \n",
       "\n",
       "                                                                              232  \\\n",
       "mean_fit_time                                                            0.019812   \n",
       "std_fit_time                                                             0.000209   \n",
       "mean_score_time                                                          0.011532   \n",
       "std_score_time                                                           0.000696   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__max_depth                                                            4   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                       0.571429   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.548413   \n",
       "std_test_recall                                                          0.019473   \n",
       "rank_test_recall                                                               83   \n",
       "split0_test_precision                                                     0.34375   \n",
       "split1_test_precision                                                    0.363636   \n",
       "split2_test_precision                                                    0.458333   \n",
       "mean_test_precision                                                      0.388573   \n",
       "std_test_precision                                                       0.049991   \n",
       "rank_test_precision                                                           152   \n",
       "split0_test_f1                                                           0.423077   \n",
       "split1_test_f1                                                           0.444444   \n",
       "split2_test_f1                                                           0.488889   \n",
       "mean_test_f1                                                             0.452137   \n",
       "std_test_f1                                                              0.027413   \n",
       "rank_test_f1                                                                    9   \n",
       "split0_test_roc_auc                                                      0.704386   \n",
       "split1_test_roc_auc                                                      0.700379   \n",
       "split2_test_roc_auc                                                      0.693637   \n",
       "mean_test_roc_auc                                                        0.699467   \n",
       "std_test_roc_auc                                                         0.004435   \n",
       "rank_test_roc_auc                                                              45   \n",
       "split0_test_recall_weighted                                              0.776119   \n",
       "split1_test_recall_weighted                                              0.776119   \n",
       "split2_test_recall_weighted                                              0.828358   \n",
       "mean_test_recall_weighted                                                0.793532   \n",
       "std_test_recall_weighted                                                 0.024626   \n",
       "rank_test_recall_weighted                                                     135   \n",
       "\n",
       "                                                                              277  \\\n",
       "mean_fit_time                                                            0.018673   \n",
       "std_fit_time                                                             0.000208   \n",
       "mean_score_time                                                          0.011215   \n",
       "std_score_time                                                           0.000109   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__max_depth                                                            3   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.35   \n",
       "split1_test_recall                                                       0.333333   \n",
       "split2_test_recall                                                       0.761905   \n",
       "mean_test_recall                                                         0.481746   \n",
       "std_test_recall                                                          0.198219   \n",
       "rank_test_recall                                                              179   \n",
       "split0_test_precision                                                    0.368421   \n",
       "split1_test_precision                                                    0.538462   \n",
       "split2_test_precision                                                    0.457143   \n",
       "mean_test_precision                                                      0.454675   \n",
       "std_test_precision                                                       0.069441   \n",
       "rank_test_precision                                                            54   \n",
       "split0_test_f1                                                           0.358974   \n",
       "split1_test_f1                                                           0.411765   \n",
       "split2_test_f1                                                           0.571429   \n",
       "mean_test_f1                                                             0.447389   \n",
       "std_test_f1                                                              0.090318   \n",
       "rank_test_f1                                                                   10   \n",
       "split0_test_roc_auc                                                      0.654605   \n",
       "split1_test_roc_auc                                                      0.645596   \n",
       "split2_test_roc_auc                                                      0.799831   \n",
       "mean_test_roc_auc                                                        0.700011   \n",
       "std_test_roc_auc                                                         0.070679   \n",
       "rank_test_roc_auc                                                              44   \n",
       "split0_test_recall_weighted                                              0.813433   \n",
       "split1_test_recall_weighted                                              0.850746   \n",
       "split2_test_recall_weighted                                              0.820896   \n",
       "mean_test_recall_weighted                                                0.828358   \n",
       "std_test_recall_weighted                                                 0.016121   \n",
       "rank_test_recall_weighted                                                      46   \n",
       "\n",
       "                                                                              397  \\\n",
       "mean_fit_time                                                            0.018539   \n",
       "std_fit_time                                                              0.00015   \n",
       "mean_score_time                                                          0.011169   \n",
       "std_score_time                                                           0.000091   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__max_depth                                                            4   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                       0.238095   \n",
       "split2_test_recall                                                       0.666667   \n",
       "mean_test_recall                                                         0.451587   \n",
       "std_test_recall                                                          0.174967   \n",
       "rank_test_recall                                                              231   \n",
       "split0_test_precision                                                       0.375   \n",
       "split1_test_precision                                                    0.333333   \n",
       "split2_test_precision                                                    0.636364   \n",
       "mean_test_precision                                                      0.448232   \n",
       "std_test_precision                                                       0.134112   \n",
       "rank_test_precision                                                            61   \n",
       "split0_test_f1                                                           0.409091   \n",
       "split1_test_f1                                                           0.277778   \n",
       "split2_test_f1                                                           0.651163   \n",
       "mean_test_f1                                                              0.44601   \n",
       "std_test_f1                                                              0.154653   \n",
       "rank_test_f1                                                                   11   \n",
       "split0_test_roc_auc                                                      0.696272   \n",
       "split1_test_roc_auc                                                      0.666035   \n",
       "split2_test_roc_auc                                                      0.823852   \n",
       "mean_test_roc_auc                                                        0.728719   \n",
       "std_test_roc_auc                                                         0.068392   \n",
       "rank_test_roc_auc                                                               6   \n",
       "split0_test_recall_weighted                                               0.80597   \n",
       "split1_test_recall_weighted                                               0.80597   \n",
       "split2_test_recall_weighted                                               0.88806   \n",
       "mean_test_recall_weighted                                                0.833333   \n",
       "std_test_recall_weighted                                                 0.038697   \n",
       "rank_test_recall_weighted                                                      29   \n",
       "\n",
       "                                                                              381  \\\n",
       "mean_fit_time                                                            0.020448   \n",
       "std_fit_time                                                             0.001811   \n",
       "mean_score_time                                                          0.012775   \n",
       "std_score_time                                                           0.001368   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__max_depth                                                            4   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                       0.333333   \n",
       "split2_test_recall                                                       0.380952   \n",
       "mean_test_recall                                                         0.404762   \n",
       "std_test_recall                                                          0.070093   \n",
       "rank_test_recall                                                              310   \n",
       "split0_test_precision                                                    0.344828   \n",
       "split1_test_precision                                                    0.636364   \n",
       "split2_test_precision                                                    0.666667   \n",
       "mean_test_precision                                                      0.549286   \n",
       "std_test_precision                                                       0.145102   \n",
       "rank_test_precision                                                            12   \n",
       "split0_test_f1                                                           0.408163   \n",
       "split1_test_f1                                                             0.4375   \n",
       "split2_test_f1                                                           0.484848   \n",
       "mean_test_f1                                                             0.443504   \n",
       "std_test_f1                                                              0.031593   \n",
       "rank_test_f1                                                                   12   \n",
       "split0_test_roc_auc                                                      0.634211   \n",
       "split1_test_roc_auc                                                       0.73641   \n",
       "split2_test_roc_auc                                                      0.783607   \n",
       "mean_test_roc_auc                                                        0.718076   \n",
       "std_test_roc_auc                                                         0.062354   \n",
       "rank_test_roc_auc                                                              15   \n",
       "split0_test_recall_weighted                                              0.783582   \n",
       "split1_test_recall_weighted                                              0.865672   \n",
       "split2_test_recall_weighted                                              0.873134   \n",
       "mean_test_recall_weighted                                                0.840796   \n",
       "std_test_recall_weighted                                                 0.040571   \n",
       "rank_test_recall_weighted                                                      18   \n",
       "\n",
       "                                                                              244  \\\n",
       "mean_fit_time                                                            0.019144   \n",
       "std_fit_time                                                             0.001176   \n",
       "mean_score_time                                                          0.011219   \n",
       "std_score_time                                                           0.000298   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__max_depth                                                            2   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.35   \n",
       "split1_test_recall                                                       0.285714   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.386508   \n",
       "std_test_recall                                                          0.100572   \n",
       "rank_test_recall                                                              345   \n",
       "split0_test_precision                                                       0.875   \n",
       "split1_test_precision                                                         0.5   \n",
       "split2_test_precision                                                    0.407407   \n",
       "mean_test_precision                                                      0.594136   \n",
       "std_test_precision                                                       0.202166   \n",
       "rank_test_precision                                                             8   \n",
       "split0_test_f1                                                                0.5   \n",
       "split1_test_f1                                                           0.363636   \n",
       "split2_test_f1                                                           0.458333   \n",
       "mean_test_f1                                                             0.440657   \n",
       "std_test_f1                                                              0.057056   \n",
       "rank_test_f1                                                                   13   \n",
       "split0_test_roc_auc                                                      0.553728   \n",
       "split1_test_roc_auc                                                      0.618837   \n",
       "split2_test_roc_auc                                                      0.742731   \n",
       "mean_test_roc_auc                                                        0.638432   \n",
       "std_test_roc_auc                                                         0.078394   \n",
       "rank_test_roc_auc                                                             334   \n",
       "split0_test_recall_weighted                                              0.895522   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                               0.80597   \n",
       "mean_test_recall_weighted                                                0.848259   \n",
       "std_test_recall_weighted                                                 0.036728   \n",
       "rank_test_recall_weighted                                                       9   \n",
       "\n",
       "                                                                              36   \\\n",
       "mean_fit_time                                                            0.017377   \n",
       "std_fit_time                                                             0.002706   \n",
       "mean_score_time                                                          0.012432   \n",
       "std_score_time                                                           0.001376   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__max_depth                                                            3   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                       0.285714   \n",
       "split2_test_recall                                                       0.809524   \n",
       "mean_test_recall                                                         0.531746   \n",
       "std_test_recall                                                          0.215019   \n",
       "rank_test_recall                                                              102   \n",
       "split0_test_precision                                                     0.30303   \n",
       "split1_test_precision                                                         0.6   \n",
       "split2_test_precision                                                       0.425   \n",
       "mean_test_precision                                                      0.442677   \n",
       "std_test_precision                                                        0.12188   \n",
       "rank_test_precision                                                            68   \n",
       "split0_test_f1                                                           0.377358   \n",
       "split1_test_f1                                                           0.387097   \n",
       "split2_test_f1                                                           0.557377   \n",
       "mean_test_f1                                                             0.440611   \n",
       "std_test_f1                                                              0.082662   \n",
       "rank_test_f1                                                                   14   \n",
       "split0_test_roc_auc                                                      0.712719   \n",
       "split1_test_roc_auc                                                      0.725453   \n",
       "split2_test_roc_auc                                                      0.800885   \n",
       "mean_test_roc_auc                                                        0.746352   \n",
       "std_test_roc_auc                                                         0.038909   \n",
       "rank_test_roc_auc                                                               1   \n",
       "split0_test_recall_weighted                                              0.753731   \n",
       "split1_test_recall_weighted                                              0.858209   \n",
       "split2_test_recall_weighted                                              0.798507   \n",
       "mean_test_recall_weighted                                                0.803483   \n",
       "std_test_recall_weighted                                                 0.042798   \n",
       "rank_test_recall_weighted                                                     105   \n",
       "\n",
       "                                                                              442  \\\n",
       "mean_fit_time                                                            0.018533   \n",
       "std_fit_time                                                             0.000273   \n",
       "mean_score_time                                                           0.01098   \n",
       "std_score_time                                                           0.000085   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__max_depth                                                            3   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.6   \n",
       "split1_test_recall                                                       0.238095   \n",
       "split2_test_recall                                                       0.714286   \n",
       "mean_test_recall                                                          0.51746   \n",
       "std_test_recall                                                          0.202976   \n",
       "rank_test_recall                                                              118   \n",
       "split0_test_precision                                                    0.206897   \n",
       "split1_test_precision                                                    0.454545   \n",
       "split2_test_precision                                                    0.681818   \n",
       "mean_test_precision                                                      0.447753   \n",
       "std_test_precision                                                       0.193945   \n",
       "rank_test_precision                                                            63   \n",
       "split0_test_f1                                                           0.307692   \n",
       "split1_test_f1                                                             0.3125   \n",
       "split2_test_f1                                                           0.697674   \n",
       "mean_test_f1                                                             0.439289   \n",
       "std_test_f1                                                              0.182717   \n",
       "rank_test_f1                                                                   15   \n",
       "split0_test_roc_auc                                                      0.609211   \n",
       "split1_test_roc_auc                                                      0.636115   \n",
       "split2_test_roc_auc                                                      0.850611   \n",
       "mean_test_roc_auc                                                        0.698645   \n",
       "std_test_roc_auc                                                         0.108016   \n",
       "rank_test_roc_auc                                                              49   \n",
       "split0_test_recall_weighted                                              0.597015   \n",
       "split1_test_recall_weighted                                              0.835821   \n",
       "split2_test_recall_weighted                                              0.902985   \n",
       "mean_test_recall_weighted                                                0.778607   \n",
       "std_test_recall_weighted                                                   0.1313   \n",
       "rank_test_recall_weighted                                                     178   \n",
       "\n",
       "                                                                              408  \\\n",
       "mean_fit_time                                                            0.020638   \n",
       "std_fit_time                                                              0.00138   \n",
       "mean_score_time                                                          0.011961   \n",
       "std_score_time                                                           0.001057   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__max_depth                                                            2   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.4   \n",
       "split1_test_recall                                                       0.285714   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.403175   \n",
       "std_test_recall                                                          0.097228   \n",
       "rank_test_recall                                                              314   \n",
       "split0_test_precision                                                    0.285714   \n",
       "split1_test_precision                                                    0.666667   \n",
       "split2_test_precision                                                    0.647059   \n",
       "mean_test_precision                                                      0.533147   \n",
       "std_test_precision                                                       0.175144   \n",
       "rank_test_precision                                                            15   \n",
       "split0_test_f1                                                           0.333333   \n",
       "split1_test_f1                                                                0.4   \n",
       "split2_test_f1                                                           0.578947   \n",
       "mean_test_f1                                                             0.437427   \n",
       "std_test_f1                                                              0.103705   \n",
       "rank_test_f1                                                                   16   \n",
       "split0_test_roc_auc                                                      0.641228   \n",
       "split1_test_roc_auc                                                       0.70649   \n",
       "split2_test_roc_auc                                                      0.773915   \n",
       "mean_test_roc_auc                                                        0.707211   \n",
       "std_test_roc_auc                                                         0.054172   \n",
       "rank_test_roc_auc                                                              27   \n",
       "split0_test_recall_weighted                                              0.761194   \n",
       "split1_test_recall_weighted                                              0.865672   \n",
       "split2_test_recall_weighted                                              0.880597   \n",
       "mean_test_recall_weighted                                                0.835821   \n",
       "std_test_recall_weighted                                                  0.05312   \n",
       "rank_test_recall_weighted                                                      26   \n",
       "\n",
       "                                                                              279  \\\n",
       "mean_fit_time                                                            0.018568   \n",
       "std_fit_time                                                             0.000846   \n",
       "mean_score_time                                                          0.011601   \n",
       "std_score_time                                                           0.000076   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__max_depth                                                            3   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                       0.238095   \n",
       "split2_test_recall                                                       0.761905   \n",
       "mean_test_recall                                                              0.5   \n",
       "std_test_recall                                                          0.213844   \n",
       "rank_test_recall                                                              145   \n",
       "split0_test_precision                                                         0.4   \n",
       "split1_test_precision                                                         0.5   \n",
       "split2_test_precision                                                    0.421053   \n",
       "mean_test_precision                                                      0.440351   \n",
       "std_test_precision                                                       0.043045   \n",
       "rank_test_precision                                                            69   \n",
       "split0_test_f1                                                           0.444444   \n",
       "split1_test_f1                                                           0.322581   \n",
       "split2_test_f1                                                           0.542373   \n",
       "mean_test_f1                                                             0.436466   \n",
       "std_test_f1                                                              0.089907   \n",
       "rank_test_f1                                                                   17   \n",
       "split0_test_roc_auc                                                      0.691886   \n",
       "split1_test_roc_auc                                                       0.62263   \n",
       "split2_test_roc_auc                                                      0.788453   \n",
       "mean_test_roc_auc                                                         0.70099   \n",
       "std_test_roc_auc                                                         0.068003   \n",
       "rank_test_roc_auc                                                              42   \n",
       "split0_test_recall_weighted                                              0.813433   \n",
       "split1_test_recall_weighted                                              0.843284   \n",
       "split2_test_recall_weighted                                              0.798507   \n",
       "mean_test_recall_weighted                                                0.818408   \n",
       "std_test_recall_weighted                                                 0.018615   \n",
       "rank_test_recall_weighted                                                      71   \n",
       "\n",
       "                                ...  \\\n",
       "mean_fit_time                   ...   \n",
       "std_fit_time                    ...   \n",
       "mean_score_time                 ...   \n",
       "std_score_time                  ...   \n",
       "param_clf__class_weight         ...   \n",
       "param_clf__criterion            ...   \n",
       "param_clf__max_depth            ...   \n",
       "param_smt__k_neighbors          ...   \n",
       "param_smt__sampling_strategy    ...   \n",
       "param_under__sampling_strategy  ...   \n",
       "params                          ...   \n",
       "split0_test_recall              ...   \n",
       "split1_test_recall              ...   \n",
       "split2_test_recall              ...   \n",
       "mean_test_recall                ...   \n",
       "std_test_recall                 ...   \n",
       "rank_test_recall                ...   \n",
       "split0_test_precision           ...   \n",
       "split1_test_precision           ...   \n",
       "split2_test_precision           ...   \n",
       "mean_test_precision             ...   \n",
       "std_test_precision              ...   \n",
       "rank_test_precision             ...   \n",
       "split0_test_f1                  ...   \n",
       "split1_test_f1                  ...   \n",
       "split2_test_f1                  ...   \n",
       "mean_test_f1                    ...   \n",
       "std_test_f1                     ...   \n",
       "rank_test_f1                    ...   \n",
       "split0_test_roc_auc             ...   \n",
       "split1_test_roc_auc             ...   \n",
       "split2_test_roc_auc             ...   \n",
       "mean_test_roc_auc               ...   \n",
       "std_test_roc_auc                ...   \n",
       "rank_test_roc_auc               ...   \n",
       "split0_test_recall_weighted     ...   \n",
       "split1_test_recall_weighted     ...   \n",
       "split2_test_recall_weighted     ...   \n",
       "mean_test_recall_weighted       ...   \n",
       "std_test_recall_weighted        ...   \n",
       "rank_test_recall_weighted       ...   \n",
       "\n",
       "                                                                              65   \\\n",
       "mean_fit_time                                                            0.019574   \n",
       "std_fit_time                                                             0.001007   \n",
       "mean_score_time                                                          0.011581   \n",
       "std_score_time                                                            0.00023   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__max_depth                                                            4   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                            0.4   \n",
       "split1_test_recall                                                        0.52381   \n",
       "split2_test_recall                                                       0.761905   \n",
       "mean_test_recall                                                         0.561905   \n",
       "std_test_recall                                                          0.150183   \n",
       "rank_test_recall                                                               72   \n",
       "split0_test_precision                                                    0.145455   \n",
       "split1_test_precision                                                    0.203704   \n",
       "split2_test_precision                                                    0.207792   \n",
       "mean_test_precision                                                       0.18565   \n",
       "std_test_precision                                                       0.028472   \n",
       "rank_test_precision                                                           486   \n",
       "split0_test_f1                                                           0.213333   \n",
       "split1_test_f1                                                           0.293333   \n",
       "split2_test_f1                                                           0.326531   \n",
       "mean_test_f1                                                             0.277732   \n",
       "std_test_f1                                                              0.047511   \n",
       "rank_test_f1                                                                  470   \n",
       "split0_test_roc_auc                                                      0.477851   \n",
       "split1_test_roc_auc                                                      0.627054   \n",
       "split2_test_roc_auc                                                      0.626212   \n",
       "mean_test_roc_auc                                                        0.577039   \n",
       "std_test_roc_auc                                                         0.070137   \n",
       "rank_test_roc_auc                                                             475   \n",
       "split0_test_recall_weighted                                              0.559701   \n",
       "split1_test_recall_weighted                                              0.604478   \n",
       "split2_test_recall_weighted                                              0.507463   \n",
       "mean_test_recall_weighted                                                0.557214   \n",
       "std_test_recall_weighted                                                 0.039645   \n",
       "rank_test_recall_weighted                                                     485   \n",
       "\n",
       "                                                                              137  \\\n",
       "mean_fit_time                                                            0.020686   \n",
       "std_fit_time                                                             0.001385   \n",
       "mean_score_time                                                          0.012751   \n",
       "std_score_time                                                           0.001897   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__max_depth                                                            4   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                            0.4   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                       0.714286   \n",
       "mean_test_recall                                                         0.530159   \n",
       "std_test_recall                                                          0.133861   \n",
       "rank_test_recall                                                              105   \n",
       "split0_test_precision                                                    0.186047   \n",
       "split1_test_precision                                                    0.175439   \n",
       "split2_test_precision                                                    0.205479   \n",
       "mean_test_precision                                                      0.188988   \n",
       "std_test_precision                                                       0.012439   \n",
       "rank_test_precision                                                           484   \n",
       "split0_test_f1                                                           0.253968   \n",
       "split1_test_f1                                                            0.25641   \n",
       "split2_test_f1                                                           0.319149   \n",
       "mean_test_f1                                                             0.276509   \n",
       "std_test_f1                                                              0.030167   \n",
       "rank_test_f1                                                                  471   \n",
       "split0_test_roc_auc                                                      0.613596   \n",
       "split1_test_roc_auc                                                      0.621576   \n",
       "split2_test_roc_auc                                                      0.675516   \n",
       "mean_test_roc_auc                                                        0.636896   \n",
       "std_test_roc_auc                                                         0.027502   \n",
       "rank_test_roc_auc                                                             341   \n",
       "split0_test_recall_weighted                                              0.649254   \n",
       "split1_test_recall_weighted                                              0.567164   \n",
       "split2_test_recall_weighted                                              0.522388   \n",
       "mean_test_recall_weighted                                                0.579602   \n",
       "std_test_recall_weighted                                                 0.052534   \n",
       "rank_test_recall_weighted                                                     479   \n",
       "\n",
       "                                                                              485  \\\n",
       "mean_fit_time                                                            0.016055   \n",
       "std_fit_time                                                             0.002566   \n",
       "mean_score_time                                                          0.006884   \n",
       "std_score_time                                                           0.000362   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__max_depth                                                            4   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.3   \n",
       "split1_test_recall                                                       0.380952   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.385714   \n",
       "std_test_recall                                                          0.072008   \n",
       "rank_test_recall                                                              350   \n",
       "split0_test_precision                                                    0.333333   \n",
       "split1_test_precision                                                    0.195122   \n",
       "split2_test_precision                                                    0.169492   \n",
       "mean_test_precision                                                      0.232649   \n",
       "std_test_precision                                                       0.071959   \n",
       "rank_test_precision                                                           450   \n",
       "split0_test_f1                                                           0.315789   \n",
       "split1_test_f1                                                           0.258065   \n",
       "split2_test_f1                                                               0.25   \n",
       "mean_test_f1                                                             0.274618   \n",
       "std_test_f1                                                              0.029298   \n",
       "rank_test_f1                                                                  472   \n",
       "split0_test_roc_auc                                                      0.548904   \n",
       "split1_test_roc_auc                                                      0.611252   \n",
       "split2_test_roc_auc                                                      0.536452   \n",
       "mean_test_roc_auc                                                        0.565536   \n",
       "std_test_roc_auc                                                         0.032723   \n",
       "rank_test_roc_auc                                                             480   \n",
       "split0_test_recall_weighted                                               0.80597   \n",
       "split1_test_recall_weighted                                              0.656716   \n",
       "split2_test_recall_weighted                                              0.552239   \n",
       "mean_test_recall_weighted                                                0.671642   \n",
       "std_test_recall_weighted                                                 0.104122   \n",
       "rank_test_recall_weighted                                                     403   \n",
       "\n",
       "                                                                              407  \\\n",
       "mean_fit_time                                                            0.018332   \n",
       "std_fit_time                                                             0.000085   \n",
       "mean_score_time                                                          0.011289   \n",
       "std_score_time                                                           0.000151   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__max_depth                                                            2   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                       0.142857   \n",
       "split2_test_recall                                                       0.619048   \n",
       "mean_test_recall                                                         0.437302   \n",
       "std_test_recall                                                          0.210103   \n",
       "rank_test_recall                                                              262   \n",
       "split0_test_precision                                                    0.211538   \n",
       "split1_test_precision                                                       0.375   \n",
       "split2_test_precision                                                     0.19697   \n",
       "mean_test_precision                                                      0.261169   \n",
       "std_test_precision                                                        0.08071   \n",
       "rank_test_precision                                                           403   \n",
       "split0_test_f1                                                           0.305556   \n",
       "split1_test_f1                                                           0.206897   \n",
       "split2_test_f1                                                           0.298851   \n",
       "mean_test_f1                                                             0.270434   \n",
       "std_test_f1                                                              0.045011   \n",
       "rank_test_f1                                                                  473   \n",
       "split0_test_roc_auc                                                      0.607456   \n",
       "split1_test_roc_auc                                                      0.528445   \n",
       "split2_test_roc_auc                                                      0.616308   \n",
       "mean_test_roc_auc                                                         0.58407   \n",
       "std_test_roc_auc                                                         0.039498   \n",
       "rank_test_roc_auc                                                             471   \n",
       "split0_test_recall_weighted                                              0.626866   \n",
       "split1_test_recall_weighted                                              0.828358   \n",
       "split2_test_recall_weighted                                              0.544776   \n",
       "mean_test_recall_weighted                                                0.666667   \n",
       "std_test_recall_weighted                                                 0.119144   \n",
       "rank_test_recall_weighted                                                     411   \n",
       "\n",
       "                                                                              228  \\\n",
       "mean_fit_time                                                            0.022093   \n",
       "std_fit_time                                                             0.002313   \n",
       "mean_score_time                                                           0.01169   \n",
       "std_score_time                                                           0.000779   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__max_depth                                                            4   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                           0.25   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                       0.571429   \n",
       "mean_test_recall                                                         0.416667   \n",
       "std_test_recall                                                          0.131492   \n",
       "rank_test_recall                                                              304   \n",
       "split0_test_precision                                                    0.263158   \n",
       "split1_test_precision                                                    0.183673   \n",
       "split2_test_precision                                                         0.2   \n",
       "mean_test_precision                                                       0.21561   \n",
       "std_test_precision                                                       0.034275   \n",
       "rank_test_precision                                                           466   \n",
       "split0_test_f1                                                            0.25641   \n",
       "split1_test_f1                                                           0.257143   \n",
       "split2_test_f1                                                           0.296296   \n",
       "mean_test_f1                                                              0.26995   \n",
       "std_test_f1                                                              0.018632   \n",
       "rank_test_f1                                                                  474   \n",
       "split0_test_roc_auc                                                      0.657018   \n",
       "split1_test_roc_auc                                                      0.617362   \n",
       "split2_test_roc_auc                                                      0.626212   \n",
       "mean_test_roc_auc                                                         0.63353   \n",
       "std_test_roc_auc                                                         0.016996   \n",
       "rank_test_roc_auc                                                             349   \n",
       "split0_test_recall_weighted                                              0.783582   \n",
       "split1_test_recall_weighted                                               0.61194   \n",
       "split2_test_recall_weighted                                              0.574627   \n",
       "mean_test_recall_weighted                                                0.656716   \n",
       "std_test_recall_weighted                                                 0.090992   \n",
       "rank_test_recall_weighted                                                     423   \n",
       "\n",
       "                                                                              226  \\\n",
       "mean_fit_time                                                            0.019656   \n",
       "std_fit_time                                                             0.001528   \n",
       "mean_score_time                                                          0.011816   \n",
       "std_score_time                                                           0.000986   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__max_depth                                                            4   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                            0.2   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                       0.761905   \n",
       "mean_test_recall                                                         0.463492   \n",
       "std_test_recall                                                          0.230722   \n",
       "rank_test_recall                                                              218   \n",
       "split0_test_precision                                                    0.133333   \n",
       "split1_test_precision                                                    0.204545   \n",
       "split2_test_precision                                                    0.246154   \n",
       "mean_test_precision                                                      0.194678   \n",
       "std_test_precision                                                       0.046584   \n",
       "rank_test_precision                                                           482   \n",
       "split0_test_f1                                                               0.16   \n",
       "split1_test_f1                                                           0.276923   \n",
       "split2_test_f1                                                           0.372093   \n",
       "mean_test_f1                                                             0.269672   \n",
       "std_test_f1                                                              0.086738   \n",
       "rank_test_f1                                                                  475   \n",
       "split0_test_roc_auc                                                       0.42807   \n",
       "split1_test_roc_auc                                                      0.621155   \n",
       "split2_test_roc_auc                                                      0.730931   \n",
       "mean_test_roc_auc                                                        0.593385   \n",
       "std_test_roc_auc                                                         0.125192   \n",
       "rank_test_roc_auc                                                             458   \n",
       "split0_test_recall_weighted                                              0.686567   \n",
       "split1_test_recall_weighted                                              0.649254   \n",
       "split2_test_recall_weighted                                              0.597015   \n",
       "mean_test_recall_weighted                                                0.644279   \n",
       "std_test_recall_weighted                                                 0.036728   \n",
       "rank_test_recall_weighted                                                     434   \n",
       "\n",
       "                                                                              430  \\\n",
       "mean_fit_time                                                            0.019472   \n",
       "std_fit_time                                                             0.001056   \n",
       "mean_score_time                                                          0.011474   \n",
       "std_score_time                                                           0.000512   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__max_depth                                                            2   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.2   \n",
       "split1_test_recall                                                       0.047619   \n",
       "split2_test_recall                                                       0.761905   \n",
       "mean_test_recall                                                         0.336508   \n",
       "std_test_recall                                                          0.307166   \n",
       "rank_test_recall                                                              428   \n",
       "split0_test_precision                                                    0.363636   \n",
       "split1_test_precision                                                        0.25   \n",
       "split2_test_precision                                                    0.340426   \n",
       "mean_test_precision                                                      0.318021   \n",
       "std_test_precision                                                       0.049022   \n",
       "rank_test_precision                                                           285   \n",
       "split0_test_f1                                                           0.258065   \n",
       "split1_test_f1                                                               0.08   \n",
       "split2_test_f1                                                           0.470588   \n",
       "mean_test_f1                                                             0.269551   \n",
       "std_test_f1                                                              0.159664   \n",
       "rank_test_f1                                                                  476   \n",
       "split0_test_roc_auc                                                      0.625439   \n",
       "split1_test_roc_auc                                                      0.689423   \n",
       "split2_test_roc_auc                                                      0.769701   \n",
       "mean_test_roc_auc                                                        0.694854   \n",
       "std_test_roc_auc                                                          0.05902   \n",
       "rank_test_roc_auc                                                              66   \n",
       "split0_test_recall_weighted                                              0.828358   \n",
       "split1_test_recall_weighted                                              0.828358   \n",
       "split2_test_recall_weighted                                              0.731343   \n",
       "mean_test_recall_weighted                                                 0.79602   \n",
       "std_test_recall_weighted                                                 0.045733   \n",
       "rank_test_recall_weighted                                                     130   \n",
       "\n",
       "                                                                              412  \\\n",
       "mean_fit_time                                                            0.019176   \n",
       "std_fit_time                                                             0.000658   \n",
       "mean_score_time                                                          0.010919   \n",
       "std_score_time                                                           0.000119   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__max_depth                                                            2   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.4   \n",
       "split1_test_recall                                                       0.142857   \n",
       "split2_test_recall                                                       0.333333   \n",
       "mean_test_recall                                                         0.292063   \n",
       "std_test_recall                                                          0.108959   \n",
       "rank_test_recall                                                              465   \n",
       "split0_test_precision                                                    0.222222   \n",
       "split1_test_precision                                                        0.25   \n",
       "split2_test_precision                                                    0.333333   \n",
       "mean_test_precision                                                      0.268519   \n",
       "std_test_precision                                                       0.047213   \n",
       "rank_test_precision                                                           384   \n",
       "split0_test_f1                                                           0.285714   \n",
       "split1_test_f1                                                           0.181818   \n",
       "split2_test_f1                                                           0.333333   \n",
       "mean_test_f1                                                             0.266955   \n",
       "std_test_f1                                                              0.063262   \n",
       "rank_test_f1                                                                  477   \n",
       "split0_test_roc_auc                                                      0.625439   \n",
       "split1_test_roc_auc                                                      0.633586   \n",
       "split2_test_roc_auc                                                      0.564897   \n",
       "mean_test_roc_auc                                                        0.607974   \n",
       "std_test_roc_auc                                                         0.030641   \n",
       "rank_test_roc_auc                                                             429   \n",
       "split0_test_recall_weighted                                              0.701493   \n",
       "split1_test_recall_weighted                                              0.798507   \n",
       "split2_test_recall_weighted                                              0.791045   \n",
       "mean_test_recall_weighted                                                0.763682   \n",
       "std_test_recall_weighted                                                  0.04408   \n",
       "rank_test_recall_weighted                                                     221   \n",
       "\n",
       "                                                                              191  \\\n",
       "mean_fit_time                                                            0.019162   \n",
       "std_fit_time                                                             0.000555   \n",
       "mean_score_time                                                          0.010783   \n",
       "std_score_time                                                           0.000283   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__max_depth                                                            3   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                            0.2   \n",
       "split1_test_recall                                                       0.285714   \n",
       "split2_test_recall                                                       0.714286   \n",
       "mean_test_recall                                                              0.4   \n",
       "std_test_recall                                                          0.224972   \n",
       "rank_test_recall                                                              331   \n",
       "split0_test_precision                                                    0.129032   \n",
       "split1_test_precision                                                         0.4   \n",
       "split2_test_precision                                                    0.194805   \n",
       "mean_test_precision                                                      0.241279   \n",
       "std_test_precision                                                         0.1154   \n",
       "rank_test_precision                                                           440   \n",
       "split0_test_f1                                                           0.156863   \n",
       "split1_test_f1                                                           0.333333   \n",
       "split2_test_f1                                                           0.306122   \n",
       "mean_test_f1                                                              0.26544   \n",
       "std_test_f1                                                              0.077575   \n",
       "rank_test_f1                                                                  478   \n",
       "split0_test_roc_auc                                                      0.581579   \n",
       "split1_test_roc_auc                                                      0.607038   \n",
       "split2_test_roc_auc                                                       0.59271   \n",
       "mean_test_roc_auc                                                        0.593775   \n",
       "std_test_roc_auc                                                         0.010421   \n",
       "rank_test_roc_auc                                                             457   \n",
       "split0_test_recall_weighted                                              0.679104   \n",
       "split1_test_recall_weighted                                              0.820896   \n",
       "split2_test_recall_weighted                                              0.492537   \n",
       "mean_test_recall_weighted                                                0.664179   \n",
       "std_test_recall_weighted                                                 0.134466   \n",
       "rank_test_recall_weighted                                                     416   \n",
       "\n",
       "                                                                              29   \\\n",
       "mean_fit_time                                                            0.018908   \n",
       "std_fit_time                                                             0.000547   \n",
       "mean_score_time                                                          0.011479   \n",
       "std_score_time                                                           0.000611   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__max_depth                                                            3   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                            0.4   \n",
       "split1_test_recall                                                       0.285714   \n",
       "split2_test_recall                                                       0.666667   \n",
       "mean_test_recall                                                         0.450794   \n",
       "std_test_recall                                                          0.159617   \n",
       "rank_test_recall                                                              235   \n",
       "split0_test_precision                                                    0.195122   \n",
       "split1_test_precision                                                    0.139535   \n",
       "split2_test_precision                                                    0.229508   \n",
       "mean_test_precision                                                      0.188055   \n",
       "std_test_precision                                                        0.03707   \n",
       "rank_test_precision                                                           485   \n",
       "split0_test_f1                                                           0.262295   \n",
       "split1_test_f1                                                             0.1875   \n",
       "split2_test_f1                                                           0.341463   \n",
       "mean_test_f1                                                             0.263753   \n",
       "std_test_f1                                                              0.062864   \n",
       "rank_test_f1                                                                  479   \n",
       "split0_test_roc_auc                                                      0.524781   \n",
       "split1_test_roc_auc                                                      0.584492   \n",
       "split2_test_roc_auc                                                      0.660135   \n",
       "mean_test_roc_auc                                                        0.589803   \n",
       "std_test_roc_auc                                                         0.055386   \n",
       "rank_test_roc_auc                                                             463   \n",
       "split0_test_recall_weighted                                              0.664179   \n",
       "split1_test_recall_weighted                                               0.61194   \n",
       "split2_test_recall_weighted                                              0.597015   \n",
       "mean_test_recall_weighted                                                0.624378   \n",
       "std_test_recall_weighted                                                 0.028796   \n",
       "rank_test_recall_weighted                                                     458   \n",
       "\n",
       "                                                                              195  \\\n",
       "mean_fit_time                                                            0.019968   \n",
       "std_fit_time                                                             0.000435   \n",
       "mean_score_time                                                          0.011562   \n",
       "std_score_time                                                           0.000612   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__max_depth                                                            3   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                           0.25   \n",
       "split1_test_recall                                                       0.095238   \n",
       "split2_test_recall                                                       0.809524   \n",
       "mean_test_recall                                                         0.384921   \n",
       "std_test_recall                                                          0.306816   \n",
       "rank_test_recall                                                              353   \n",
       "split0_test_precision                                                    0.277778   \n",
       "split1_test_precision                                                         0.4   \n",
       "split2_test_precision                                                    0.242857   \n",
       "mean_test_precision                                                      0.306878   \n",
       "std_test_precision                                                       0.067373   \n",
       "rank_test_precision                                                           310   \n",
       "split0_test_f1                                                           0.263158   \n",
       "split1_test_f1                                                           0.153846   \n",
       "split2_test_f1                                                           0.373626   \n",
       "mean_test_f1                                                             0.263543   \n",
       "std_test_f1                                                              0.089725   \n",
       "rank_test_f1                                                                  480   \n",
       "split0_test_roc_auc                                                      0.616447   \n",
       "split1_test_roc_auc                                                      0.594185   \n",
       "split2_test_roc_auc                                                      0.652339   \n",
       "mean_test_roc_auc                                                         0.62099   \n",
       "std_test_roc_auc                                                         0.023958   \n",
       "rank_test_roc_auc                                                             395   \n",
       "split0_test_recall_weighted                                              0.791045   \n",
       "split1_test_recall_weighted                                              0.835821   \n",
       "split2_test_recall_weighted                                              0.574627   \n",
       "mean_test_recall_weighted                                                0.733831   \n",
       "std_test_recall_weighted                                                 0.114049   \n",
       "rank_test_recall_weighted                                                     287   \n",
       "\n",
       "                                                                              170  \\\n",
       "mean_fit_time                                                            0.019992   \n",
       "std_fit_time                                                             0.000968   \n",
       "mean_score_time                                                          0.011447   \n",
       "std_score_time                                                            0.00024   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__max_depth                                                            2   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                            0.6   \n",
       "split1_test_recall                                                       0.190476   \n",
       "split2_test_recall                                                       0.428571   \n",
       "mean_test_recall                                                         0.406349   \n",
       "std_test_recall                                                          0.167924   \n",
       "rank_test_recall                                                              309   \n",
       "split0_test_precision                                                    0.218182   \n",
       "split1_test_precision                                                    0.333333   \n",
       "split2_test_precision                                                    0.155172   \n",
       "mean_test_precision                                                      0.235563   \n",
       "std_test_precision                                                       0.073765   \n",
       "rank_test_precision                                                           446   \n",
       "split0_test_f1                                                               0.32   \n",
       "split1_test_f1                                                           0.242424   \n",
       "split2_test_f1                                                           0.227848   \n",
       "mean_test_f1                                                             0.263424   \n",
       "std_test_f1                                                              0.040445   \n",
       "rank_test_f1                                                                  481   \n",
       "split0_test_roc_auc                                                      0.632456   \n",
       "split1_test_roc_auc                                                      0.658871   \n",
       "split2_test_roc_auc                                                      0.572903   \n",
       "mean_test_roc_auc                                                         0.62141   \n",
       "std_test_roc_auc                                                         0.035955   \n",
       "rank_test_roc_auc                                                             391   \n",
       "split0_test_recall_weighted                                              0.619403   \n",
       "split1_test_recall_weighted                                              0.813433   \n",
       "split2_test_recall_weighted                                              0.544776   \n",
       "mean_test_recall_weighted                                                0.659204   \n",
       "std_test_recall_weighted                                                 0.113232   \n",
       "rank_test_recall_weighted                                                     420   \n",
       "\n",
       "                                                                              64   \\\n",
       "mean_fit_time                                                            0.018089   \n",
       "std_fit_time                                                             0.001894   \n",
       "mean_score_time                                                          0.014375   \n",
       "std_score_time                                                           0.003797   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__max_depth                                                            4   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                            0.3   \n",
       "split1_test_recall                                                       0.238095   \n",
       "split2_test_recall                                                       0.428571   \n",
       "mean_test_recall                                                         0.322222   \n",
       "std_test_recall                                                          0.079333   \n",
       "rank_test_recall                                                              442   \n",
       "split0_test_precision                                                    0.146341   \n",
       "split1_test_precision                                                    0.208333   \n",
       "split2_test_precision                                                    0.321429   \n",
       "mean_test_precision                                                      0.225368   \n",
       "std_test_precision                                                       0.072487   \n",
       "rank_test_precision                                                           456   \n",
       "split0_test_f1                                                           0.196721   \n",
       "split1_test_f1                                                           0.222222   \n",
       "split2_test_f1                                                           0.367347   \n",
       "mean_test_f1                                                             0.262097   \n",
       "std_test_f1                                                              0.075148   \n",
       "rank_test_f1                                                                  482   \n",
       "split0_test_roc_auc                                                      0.458553   \n",
       "split1_test_roc_auc                                                      0.455542   \n",
       "split2_test_roc_auc                                                       0.62284   \n",
       "mean_test_roc_auc                                                        0.512311   \n",
       "std_test_roc_auc                                                         0.078165   \n",
       "rank_test_roc_auc                                                             486   \n",
       "split0_test_recall_weighted                                              0.634328   \n",
       "split1_test_recall_weighted                                              0.738806   \n",
       "split2_test_recall_weighted                                              0.768657   \n",
       "mean_test_recall_weighted                                                 0.71393   \n",
       "std_test_recall_weighted                                                 0.057591   \n",
       "rank_test_recall_weighted                                                     322   \n",
       "\n",
       "                                                                              274  \\\n",
       "mean_fit_time                                                            0.018518   \n",
       "std_fit_time                                                             0.000289   \n",
       "mean_score_time                                                           0.01124   \n",
       "std_score_time                                                           0.000062   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__max_depth                                                            3   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.35   \n",
       "split1_test_recall                                                       0.238095   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.354762   \n",
       "std_test_recall                                                           0.09726   \n",
       "rank_test_recall                                                              390   \n",
       "split0_test_precision                                                     0.21875   \n",
       "split1_test_precision                                                    0.172414   \n",
       "split2_test_precision                                                    0.222222   \n",
       "mean_test_precision                                                      0.204462   \n",
       "std_test_precision                                                       0.022706   \n",
       "rank_test_precision                                                           475   \n",
       "split0_test_f1                                                           0.269231   \n",
       "split1_test_f1                                                                0.2   \n",
       "split2_test_f1                                                            0.30303   \n",
       "mean_test_f1                                                              0.25742   \n",
       "std_test_f1                                                              0.042883   \n",
       "rank_test_f1                                                                  483   \n",
       "split0_test_roc_auc                                                      0.631798   \n",
       "split1_test_roc_auc                                                      0.505057   \n",
       "split2_test_roc_auc                                                      0.582807   \n",
       "mean_test_roc_auc                                                        0.573221   \n",
       "std_test_roc_auc                                                         0.052184   \n",
       "rank_test_roc_auc                                                             477   \n",
       "split0_test_recall_weighted                                              0.716418   \n",
       "split1_test_recall_weighted                                              0.701493   \n",
       "split2_test_recall_weighted                                              0.656716   \n",
       "mean_test_recall_weighted                                                0.691542   \n",
       "std_test_recall_weighted                                                 0.025368   \n",
       "rank_test_recall_weighted                                                     366   \n",
       "\n",
       "                                                                              463  \\\n",
       "mean_fit_time                                                            0.019009   \n",
       "std_fit_time                                                             0.000163   \n",
       "mean_score_time                                                          0.011236   \n",
       "std_score_time                                                           0.000267   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__max_depth                                                            4   \n",
       "param_smt__k_neighbors                                                          1   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.2   \n",
       "split1_test_recall                                                       0.142857   \n",
       "split2_test_recall                                                       0.571429   \n",
       "mean_test_recall                                                         0.304762   \n",
       "std_test_recall                                                          0.189999   \n",
       "rank_test_recall                                                              461   \n",
       "split0_test_precision                                                        0.25   \n",
       "split1_test_precision                                                    0.428571   \n",
       "split2_test_precision                                                    0.190476   \n",
       "mean_test_precision                                                      0.289683   \n",
       "std_test_precision                                                       0.101171   \n",
       "rank_test_precision                                                           342   \n",
       "split0_test_f1                                                           0.222222   \n",
       "split1_test_f1                                                           0.214286   \n",
       "split2_test_f1                                                           0.285714   \n",
       "mean_test_f1                                                             0.240741   \n",
       "std_test_f1                                                              0.031966   \n",
       "rank_test_f1                                                                  484   \n",
       "split0_test_roc_auc                                                      0.601754   \n",
       "split1_test_roc_auc                                                      0.609987   \n",
       "split2_test_roc_auc                                                      0.617151   \n",
       "mean_test_roc_auc                                                        0.609631   \n",
       "std_test_roc_auc                                                         0.006291   \n",
       "rank_test_roc_auc                                                             426   \n",
       "split0_test_recall_weighted                                              0.791045   \n",
       "split1_test_recall_weighted                                              0.835821   \n",
       "split2_test_recall_weighted                                              0.552239   \n",
       "mean_test_recall_weighted                                                0.726368   \n",
       "std_test_recall_weighted                                                 0.124478   \n",
       "rank_test_recall_weighted                                                     305   \n",
       "\n",
       "                                                                              336  \\\n",
       "mean_fit_time                                                            0.019011   \n",
       "std_fit_time                                                             0.000952   \n",
       "mean_score_time                                                          0.011118   \n",
       "std_score_time                                                           0.000101   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__max_depth                                                            2   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.1   \n",
       "split1_test_recall                                                       0.238095   \n",
       "split2_test_recall                                                       0.142857   \n",
       "mean_test_recall                                                         0.160317   \n",
       "std_test_recall                                                          0.057713   \n",
       "rank_test_recall                                                              485   \n",
       "split0_test_precision                                                         1.0   \n",
       "split1_test_precision                                                    0.294118   \n",
       "split2_test_precision                                                         0.5   \n",
       "mean_test_precision                                                      0.598039   \n",
       "std_test_precision                                                       0.296396   \n",
       "rank_test_precision                                                             7   \n",
       "split0_test_f1                                                           0.181818   \n",
       "split1_test_f1                                                           0.263158   \n",
       "split2_test_f1                                                           0.222222   \n",
       "mean_test_f1                                                             0.222399   \n",
       "std_test_f1                                                              0.033207   \n",
       "rank_test_f1                                                                  485   \n",
       "split0_test_roc_auc                                                      0.635088   \n",
       "split1_test_roc_auc                                                      0.666035   \n",
       "split2_test_roc_auc                                                      0.635272   \n",
       "mean_test_roc_auc                                                        0.645465   \n",
       "std_test_roc_auc                                                         0.014545   \n",
       "rank_test_roc_auc                                                             291   \n",
       "split0_test_recall_weighted                                              0.865672   \n",
       "split1_test_recall_weighted                                              0.791045   \n",
       "split2_test_recall_weighted                                              0.843284   \n",
       "mean_test_recall_weighted                                                0.833333   \n",
       "std_test_recall_weighted                                                 0.031268   \n",
       "rank_test_recall_weighted                                                      29   \n",
       "\n",
       "                                                                              330  \n",
       "mean_fit_time                                                             0.01879  \n",
       "std_fit_time                                                              0.00035  \n",
       "mean_score_time                                                          0.011292  \n",
       "std_score_time                                                           0.000163  \n",
       "param_clf__class_weight                                                      None  \n",
       "param_clf__criterion                                                      entropy  \n",
       "param_clf__max_depth                                                            2  \n",
       "param_smt__k_neighbors                                                          1  \n",
       "param_smt__sampling_strategy                                                  0.4  \n",
       "param_under__sampling_strategy                                                0.5  \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...  \n",
       "split0_test_recall                                                            0.1  \n",
       "split1_test_recall                                                       0.285714  \n",
       "split2_test_recall                                                            0.0  \n",
       "mean_test_recall                                                         0.128571  \n",
       "std_test_recall                                                          0.118379  \n",
       "rank_test_recall                                                              486  \n",
       "split0_test_precision                                                         1.0  \n",
       "split1_test_precision                                                    0.545455  \n",
       "split2_test_precision                                                         0.0  \n",
       "mean_test_precision                                                      0.515152  \n",
       "std_test_precision                                                        0.40881  \n",
       "rank_test_precision                                                            18  \n",
       "split0_test_f1                                                           0.181818  \n",
       "split1_test_f1                                                              0.375  \n",
       "split2_test_f1                                                                0.0  \n",
       "mean_test_f1                                                             0.185606  \n",
       "std_test_f1                                                              0.153117  \n",
       "rank_test_f1                                                                  486  \n",
       "split0_test_roc_auc                                                      0.625439  \n",
       "split1_test_roc_auc                                                      0.695322  \n",
       "split2_test_roc_auc                                                      0.665824  \n",
       "mean_test_roc_auc                                                        0.662195  \n",
       "std_test_roc_auc                                                         0.028645  \n",
       "rank_test_roc_auc                                                             203  \n",
       "split0_test_recall_weighted                                              0.865672  \n",
       "split1_test_recall_weighted                                              0.850746  \n",
       "split2_test_recall_weighted                                              0.843284  \n",
       "mean_test_recall_weighted                                                0.853234  \n",
       "std_test_recall_weighted                                                 0.009308  \n",
       "rank_test_recall_weighted                                                       3  \n",
       "\n",
       "[41 rows x 486 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "pipeline = IMBLpipeline([\n",
    "    ('smt', SMOTE()), \n",
    "    ('under', RandomUnderSampler()), \n",
    "    ('clf', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "            'clf__max_depth': [2,3,4],\n",
    "            'clf__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "               'clf__class_weight': ['balanced', None],\n",
    "               'smt__sampling_strategy': [ 0.2, 0.3, 0.4],\n",
    "               'under__sampling_strategy': [0.5, 0.6, 0.7],\n",
    "               'smt__k_neighbors': [1, 3, 5]\n",
    "             }\n",
    "nfolds = 3\n",
    "scores = ['recall', 'precision', 'f1', 'roc_auc', 'recall_weighted']\n",
    "gscv_dt = GridSearchCV(pipeline, \n",
    "                    parameters, \n",
    "                    scoring=scores,\n",
    "                    cv=StratifiedKFold(n_splits=nfolds, shuffle=True),\n",
    "                    return_train_score=False, \n",
    "                    verbose=1, \n",
    "                    refit=\"f1\",\n",
    "                    n_jobs=-1)\n",
    "resultsGSCV = gscv_dt.fit(X, y)\n",
    "\n",
    "# Get the classifier name from the pipeline\n",
    "classifier_name = resultsGSCV.best_estimator_.named_steps['clf'].__class__.__name__\n",
    "    \n",
    "# save the best estimator\n",
    "save_best_estimator(gscv_dt, classifier_name, notebook_name)\n",
    "\n",
    "results = pd.DataFrame(resultsGSCV.cv_results_)\n",
    "display(results.sort_values(by=[f'rank_test_f1']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "395678b1-57e4-480f-980d-4106ab700c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.88      0.91      0.90       147\n",
      "         poz       0.38      0.31      0.34        26\n",
      "\n",
      "    accuracy                           0.82       173\n",
      "   macro avg       0.63      0.61      0.62       173\n",
      "weighted avg       0.81      0.82      0.81       173\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on test data\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = gscv_dt.best_estimator_.steps[2][1].predict(X_eval)\n",
    "DT_classification_report = classification_report(y_eval, y_pred)\n",
    "\n",
    "# Create classification report as dictionary\n",
    "DT_report_dict = classification_report(y_eval, y_pred, output_dict=True)\n",
    "\n",
    "print(classification_report(y_eval, y_pred, target_names=[\"neg\", \"poz\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "158fb422-dd77-4258-b2f8-0f6be3d407db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the model, where \"loaded_estimator\" contains the best estimator saved\n",
    "# with open('best_estimator.pkl', 'rb') as f:\n",
    "#     loaded_estimator = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5024ff2c-4a0c-41da-91cb-8c87eea28b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:ns2=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"420.0\" height=\"306.0\" viewBox=\"0.0 0.0 419.25 306.0\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1.5 1.5) rotate(0) translate(4 200)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-200 275.5,-200 275.5,4 -4,4\" />\n",
       "<g id=\"clust4\" class=\"cluster\">\n",
       "<title>cluster_legend</title>\n",
       "</g>\n",
       "\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>node1</title>\n",
       "<text text-anchor=\"start\" x=\"73\" y=\"-103.4\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#444443\">Chl-a@0.44</text>\n",
       "</g>\n",
       "\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>node4</title>\n",
       "<text text-anchor=\"start\" x=\"151.5\" y=\"-103.4\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#444443\">month@11.50</text>\n",
       "</g>\n",
       "\n",
       "\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>leaf2</title>\n",
       "<polygon fill=\"none\" stroke=\"#444443\" stroke-width=\"0\" points=\"67,-83 0,-83 0,0 67,0 67,-83\" />\n",
       "<svg width=\"59px\" height=\"75px\" viewBox=\"0 0 78.519999 99.516228\" version=\"1.1\" preserveAspectRatio=\"xMinYMin meet\" x=\"4.5\" y=\"-78.5\">\n",
       " <metadata>\n",
       "  <rdf:RDF>\n",
       "   <ns2:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\" />\n",
       "    <dc:date>2023-04-10T10:07:52.341045</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <ns2:Agent>\n",
       "      <dc:title>Matplotlib v3.5.2, https://matplotlib.org/</dc:title>\n",
       "     </ns2:Agent>\n",
       "    </dc:creator>\n",
       "   </ns2:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 99.516228  L 78.519999 99.516228  L 78.519999 -0  L 0 -0  L 0 99.516228  z \" style=\"fill: none\" />\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 78.519999 39.259991  C 78.519999 34.606071 77.692478 29.98901 76.076148 25.624787  C 74.459817 21.260564 72.080278 17.218307 69.048843 13.687108  C 66.017408 10.155909 62.382094 7.1917 58.312955 4.933143  C 54.243816 2.674587 49.805307 1.157458 45.205055 0.452725  C 40.604803 -0.252008 35.915675 -0.133182 31.357021 0.803645  C 26.798367 1.740472 22.442396 3.48046 18.492874 5.942216  C 14.543352 8.403972 11.062838 11.548501 8.214145 15.228708  C 5.365453 18.908914 3.193705 23.066504 1.800493 27.506992  C 0.40728 31.947479 -0.185328 36.600527 0.050389 41.248474  C 0.286105 45.89642 1.346414 50.465642 3.181714 54.742398  C 5.017014 59.019154 7.598235 62.935701 10.804631 66.308829  C 14.011027 69.681957 17.79181 72.458235 21.97012 74.507795  C 26.14843 76.557355 30.658083 77.84773 35.288125 78.31856  C 39.918167 78.78939 44.595258 78.433216 49.100612 77.2667  C 53.605965 76.100184 57.868216 74.141803 61.687984 71.483167  C 65.507751 68.824531 68.824531 65.507753 71.483168 61.687987  L 39.259999 39.259991  L 78.519999 39.259991  z \" style=\"fill: #fefebb; stroke: #444443; stroke-width: 0.5; stroke-linejoin: miter\" />\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 71.483168 61.687987  C 73.760427 58.416161 75.526146 54.816499 76.719504 51.012994  C 77.912862 47.209489 78.519998 43.246319 78.519999 39.259998  L 39.259999 39.259991  L 71.483168 61.687987  z \" style=\"fill: #a1dab4; stroke: #444443; stroke-width: 0.5; stroke-linejoin: miter\" />\n",
       "   </g>\n",
       "   <g id=\"text_1\">\n",
       "    \n",
       "    <g style=\"fill: #444443\" transform=\"translate(26.62203 88.102822)scale(0.09 -0.09)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-6e\" d=\"M 422 0  L 422 3319  L 928 3319  L 928 2847  Q 1294 3394 1984 3394  Q 2284 3394 2536 3286  Q 2788 3178 2913 3003  Q 3038 2828 3088 2588  Q 3119 2431 3119 2041  L 3119 0  L 2556 0  L 2556 2019  Q 2556 2363 2490 2533  Q 2425 2703 2258 2804  Q 2091 2906 1866 2906  Q 1506 2906 1245 2678  Q 984 2450 984 1813  L 984 0  L 422 0  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-3d\" d=\"M 3381 2694  L 356 2694  L 356 3219  L 3381 3219  L 3381 2694  z M 3381 1303  L 356 1303  L 356 1828  L 3381 1828  L 3381 1303  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-31\" d=\"M 2384 0  L 1822 0  L 1822 3584  Q 1619 3391 1289 3197  Q 959 3003 697 2906  L 697 3450  Q 1169 3672 1522 3987  Q 1875 4303 2022 4600  L 2384 4600  L 2384 0  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-34\" d=\"M 2069 0  L 2069 1097  L 81 1097  L 81 1613  L 2172 4581  L 2631 4581  L 2631 1613  L 3250 1613  L 3250 1097  L 2631 1097  L 2631 0  L 2069 0  z M 2069 1613  L 2069 3678  L 634 1613  L 2069 1613  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-35\" d=\"M 266 1200  L 856 1250  Q 922 819 1161 601  Q 1400 384 1738 384  Q 2144 384 2425 690  Q 2706 997 2706 1503  Q 2706 1984 2436 2262  Q 2166 2541 1728 2541  Q 1456 2541 1237 2417  Q 1019 2294 894 2097  L 366 2166  L 809 4519  L 3088 4519  L 3088 3981  L 1259 3981  L 1013 2750  Q 1425 3038 1878 3038  Q 2478 3038 2890 2622  Q 3303 2206 3303 1553  Q 3303 931 2941 478  Q 2500 -78 1738 -78  Q 1113 -78 717 272  Q 322 622 266 1200  z \" transform=\"scale(0.015625)\" />\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-6e\" />\n",
       "     <use xlink:href=\"#ArialMT-3d\" x=\"55.615234\" />\n",
       "     <use xlink:href=\"#ArialMT-31\" x=\"114.013672\" />\n",
       "     <use xlink:href=\"#ArialMT-34\" x=\"169.628906\" />\n",
       "     <use xlink:href=\"#ArialMT-35\" x=\"225.244141\" />\n",
       "    </g>\n",
       "    \n",
       "    <g style=\"fill: #444443\" transform=\"translate(31.752733 97.62201)scale(0.09 -0.09)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-65\" d=\"M 2694 1069  L 3275 997  Q 3138 488 2766 206  Q 2394 -75 1816 -75  Q 1088 -75 661 373  Q 234 822 234 1631  Q 234 2469 665 2931  Q 1097 3394 1784 3394  Q 2450 3394 2872 2941  Q 3294 2488 3294 1666  Q 3294 1616 3291 1516  L 816 1516  Q 847 969 1125 678  Q 1403 388 1819 388  Q 2128 388 2347 550  Q 2566 713 2694 1069  z M 847 1978  L 2700 1978  Q 2663 2397 2488 2606  Q 2219 2931 1791 2931  Q 1403 2931 1139 2672  Q 875 2413 847 1978  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-67\" d=\"M 319 -275  L 866 -356  Q 900 -609 1056 -725  Q 1266 -881 1628 -881  Q 2019 -881 2231 -725  Q 2444 -569 2519 -288  Q 2563 -116 2559 434  Q 2191 0 1641 0  Q 956 0 581 494  Q 206 988 206 1678  Q 206 2153 378 2554  Q 550 2956 876 3175  Q 1203 3394 1644 3394  Q 2231 3394 2613 2919  L 2613 3319  L 3131 3319  L 3131 450  Q 3131 -325 2973 -648  Q 2816 -972 2473 -1159  Q 2131 -1347 1631 -1347  Q 1038 -1347 672 -1080  Q 306 -813 319 -275  z M 784 1719  Q 784 1066 1043 766  Q 1303 466 1694 466  Q 2081 466 2343 764  Q 2606 1063 2606 1700  Q 2606 2309 2336 2618  Q 2066 2928 1684 2928  Q 1309 2928 1046 2623  Q 784 2319 784 1719  z \" transform=\"scale(0.015625)\" />\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-6e\" />\n",
       "     <use xlink:href=\"#ArialMT-65\" x=\"55.615234\" />\n",
       "     <use xlink:href=\"#ArialMT-67\" x=\"111.230469\" />\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       "</svg></g>\n",
       "\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>node1-&gt;leaf2</title>\n",
       "<path fill=\"none\" stroke=\"#444443\" stroke-width=\"0.3\" d=\"M98.86,-99.69C92.17,-93.84 81.32,-84.34 70.35,-74.75\" />\n",
       "<polygon fill=\"#444443\" stroke=\"#444443\" stroke-width=\"0.3\" points=\"71.19,-73.62 67.26,-72.04 69.35,-75.73 71.19,-73.62\" />\n",
       "</g>\n",
       "\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>leaf3</title>\n",
       "<polygon fill=\"none\" stroke=\"#444443\" stroke-width=\"0\" points=\"148,-83 81,-83 81,0 148,0 148,-83\" />\n",
       "<svg width=\"59px\" height=\"75px\" viewBox=\"0 0 78.519993 99.516236\" version=\"1.1\" preserveAspectRatio=\"xMinYMin meet\" x=\"85.5\" y=\"-78.5\">\n",
       " <metadata>\n",
       "  <rdf:RDF>\n",
       "   <ns2:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\" />\n",
       "    <dc:date>2023-04-10T10:07:52.377887</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <ns2:Agent>\n",
       "      <dc:title>Matplotlib v3.5.2, https://matplotlib.org/</dc:title>\n",
       "     </ns2:Agent>\n",
       "    </dc:creator>\n",
       "   </ns2:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 99.516236  L 78.519993 99.516236  L 78.519993 0  L 0 0  L 0 99.516236  z \" style=\"fill: none\" />\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 78.519993 39.259999  C 78.519993 32.237423 76.635764 25.341811 73.064565 19.295067  C 69.493367 13.248323 64.36433 8.26909 58.214415 4.878645  C 52.0645 1.4882 45.116079 -0.190862 38.096588 0.017241  C 31.077096 0.225343 24.240348 2.313085 18.302086 6.061901  C 12.363824 9.810716 7.538768 15.085052 4.332055 21.332737  C 1.125342 27.580421 -0.347078 34.575546 0.068944 41.585788  C 0.484967 48.59603 2.774388 55.367909 6.697527 61.192473  C 10.620667 67.017037 16.035669 71.683678 22.375636 74.703843  L 39.259993 39.259999  L 78.519993 39.259999  z \" style=\"fill: #fefebb; stroke: #444443; stroke-width: 0.5; stroke-linejoin: miter\" />\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 22.375636 74.703843  C 28.359616 77.554427 34.969162 78.84371 41.585782 78.451048  C 48.202403 78.058385 54.613069 75.996419 60.2179 72.458097  C 65.822731 68.919776 70.441274 64.019021 73.641347 58.21442  C 76.84142 52.40982 78.519993 45.88826 78.519993 39.259999  L 39.259993 39.259999  L 22.375636 74.703843  z \" style=\"fill: #a1dab4; stroke: #444443; stroke-width: 0.5; stroke-linejoin: miter\" />\n",
       "   </g>\n",
       "   <g id=\"text_1\">\n",
       "    \n",
       "    <g style=\"fill: #444443\" transform=\"translate(26.622024 88.10283)scale(0.09 -0.09)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-6e\" d=\"M 422 0  L 422 3319  L 928 3319  L 928 2847  Q 1294 3394 1984 3394  Q 2284 3394 2536 3286  Q 2788 3178 2913 3003  Q 3038 2828 3088 2588  Q 3119 2431 3119 2041  L 3119 0  L 2556 0  L 2556 2019  Q 2556 2363 2490 2533  Q 2425 2703 2258 2804  Q 2091 2906 1866 2906  Q 1506 2906 1245 2678  Q 984 2450 984 1813  L 984 0  L 422 0  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-3d\" d=\"M 3381 2694  L 356 2694  L 356 3219  L 3381 3219  L 3381 2694  z M 3381 1303  L 356 1303  L 356 1828  L 3381 1828  L 3381 1303  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-32\" d=\"M 3222 541  L 3222 0  L 194 0  Q 188 203 259 391  Q 375 700 629 1000  Q 884 1300 1366 1694  Q 2113 2306 2375 2664  Q 2638 3022 2638 3341  Q 2638 3675 2398 3904  Q 2159 4134 1775 4134  Q 1369 4134 1125 3890  Q 881 3647 878 3216  L 300 3275  Q 359 3922 746 4261  Q 1134 4600 1788 4600  Q 2447 4600 2831 4234  Q 3216 3869 3216 3328  Q 3216 3053 3103 2787  Q 2991 2522 2730 2228  Q 2469 1934 1863 1422  Q 1356 997 1212 845  Q 1069 694 975 541  L 3222 541  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-31\" d=\"M 2384 0  L 1822 0  L 1822 3584  Q 1619 3391 1289 3197  Q 959 3003 697 2906  L 697 3450  Q 1169 3672 1522 3987  Q 1875 4303 2022 4600  L 2384 4600  L 2384 0  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-35\" d=\"M 266 1200  L 856 1250  Q 922 819 1161 601  Q 1400 384 1738 384  Q 2144 384 2425 690  Q 2706 997 2706 1503  Q 2706 1984 2436 2262  Q 2166 2541 1728 2541  Q 1456 2541 1237 2417  Q 1019 2294 894 2097  L 366 2166  L 809 4519  L 3088 4519  L 3088 3981  L 1259 3981  L 1013 2750  Q 1425 3038 1878 3038  Q 2478 3038 2890 2622  Q 3303 2206 3303 1553  Q 3303 931 2941 478  Q 2500 -78 1738 -78  Q 1113 -78 717 272  Q 322 622 266 1200  z \" transform=\"scale(0.015625)\" />\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-6e\" />\n",
       "     <use xlink:href=\"#ArialMT-3d\" x=\"55.615234\" />\n",
       "     <use xlink:href=\"#ArialMT-32\" x=\"114.013672\" />\n",
       "     <use xlink:href=\"#ArialMT-31\" x=\"169.628906\" />\n",
       "     <use xlink:href=\"#ArialMT-35\" x=\"225.244141\" />\n",
       "    </g>\n",
       "    \n",
       "    <g style=\"fill: #444443\" transform=\"translate(31.752728 97.622018)scale(0.09 -0.09)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-65\" d=\"M 2694 1069  L 3275 997  Q 3138 488 2766 206  Q 2394 -75 1816 -75  Q 1088 -75 661 373  Q 234 822 234 1631  Q 234 2469 665 2931  Q 1097 3394 1784 3394  Q 2450 3394 2872 2941  Q 3294 2488 3294 1666  Q 3294 1616 3291 1516  L 816 1516  Q 847 969 1125 678  Q 1403 388 1819 388  Q 2128 388 2347 550  Q 2566 713 2694 1069  z M 847 1978  L 2700 1978  Q 2663 2397 2488 2606  Q 2219 2931 1791 2931  Q 1403 2931 1139 2672  Q 875 2413 847 1978  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-67\" d=\"M 319 -275  L 866 -356  Q 900 -609 1056 -725  Q 1266 -881 1628 -881  Q 2019 -881 2231 -725  Q 2444 -569 2519 -288  Q 2563 -116 2559 434  Q 2191 0 1641 0  Q 956 0 581 494  Q 206 988 206 1678  Q 206 2153 378 2554  Q 550 2956 876 3175  Q 1203 3394 1644 3394  Q 2231 3394 2613 2919  L 2613 3319  L 3131 3319  L 3131 450  Q 3131 -325 2973 -648  Q 2816 -972 2473 -1159  Q 2131 -1347 1631 -1347  Q 1038 -1347 672 -1080  Q 306 -813 319 -275  z M 784 1719  Q 784 1066 1043 766  Q 1303 466 1694 466  Q 2081 466 2343 764  Q 2606 1063 2606 1700  Q 2606 2309 2336 2618  Q 2066 2928 1684 2928  Q 1309 2928 1046 2623  Q 784 2319 784 1719  z \" transform=\"scale(0.015625)\" />\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-6e\" />\n",
       "     <use xlink:href=\"#ArialMT-65\" x=\"55.615234\" />\n",
       "     <use xlink:href=\"#ArialMT-67\" x=\"111.230469\" />\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       "</svg></g>\n",
       "\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>node1-&gt;leaf3</title>\n",
       "<path fill=\"none\" stroke=\"#444443\" stroke-width=\"0.3\" d=\"M106.33,-99.69C106.79,-96.45 107.42,-92.09 108.11,-87.22\" />\n",
       "<polygon fill=\"#444443\" stroke=\"#444443\" stroke-width=\"0.3\" points=\"109.53,-87.18 108.71,-83.02 106.76,-86.78 109.53,-87.18\" />\n",
       "</g>\n",
       "\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>leaf5</title>\n",
       "<polygon fill=\"none\" stroke=\"#444443\" stroke-width=\"0\" points=\"206.5,-74 156.5,-74 156.5,-9 206.5,-9 206.5,-74\" />\n",
       "<svg width=\"42px\" height=\"57px\" viewBox=\"0 0 56.171996 76.168848\" version=\"1.1\" preserveAspectRatio=\"xMinYMin meet\" x=\"160.5\" y=\"-69.5\">\n",
       " <metadata>\n",
       "  <rdf:RDF>\n",
       "   <ns2:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\" />\n",
       "    <dc:date>2023-04-10T10:07:52.417260</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <ns2:Agent>\n",
       "      <dc:title>Matplotlib v3.5.2, https://matplotlib.org/</dc:title>\n",
       "     </ns2:Agent>\n",
       "    </dc:creator>\n",
       "   </ns2:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 76.168848  L 56.171996 76.168848  L 56.171996 0  L 0 0  L 0 76.168848  z \" style=\"fill: none\" />\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 56.171996 28.085999  C 56.171996 23.60877 55.101413 19.195769 53.049828 15.216249  C 50.998242 11.236729 48.024517 7.804867 44.377473 5.207819  L 28.085996 28.085999  L 56.171996 28.085999  z \" style=\"fill: #fefebb; stroke: #444443; stroke-width: 0.5; stroke-linejoin: miter\" />\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 44.377473 5.207819  C 41.830613 3.394208 38.998694 2.018547 35.998735 1.137679  C 32.998776 0.256812 29.872676 -0.116959 26.749609 0.031811  C 23.626541 0.180582 20.550125 0.849815 17.647482 2.011858  C 14.744839 3.173901 12.056508 4.812525 9.693576 6.860018  C 7.330643 8.907511 5.326109 11.335279 3.762805 14.043001  C 2.199501 16.750724 1.09926 19.700585 0.507546 22.770691  C -0.084169 25.840798 -0.159092 28.988273 0.285871 32.083057  C 0.730834 35.177841 1.689469 38.176712 3.122165 40.955752  C 4.55486 43.734793 6.441606 46.255189 8.704439 48.412796  C 10.967272 50.570403 13.57459 52.335085 16.418653 53.633925  C 19.262716 54.932765 22.303804 55.747622 25.416255 56.044824  C 28.528706 56.342027 31.669051 56.117425 34.707525 55.380298  C 37.745999 54.643172 40.640167 53.403816 43.270437 51.713444  C 45.900707 50.023071 48.230345 47.90529 50.163086 45.447609  C 52.095826 42.989928 53.604677 40.226673 54.62729 37.272026  C 55.649903 34.317378 56.171996 31.212603 56.171996 28.085994  L 28.085996 28.085999  L 44.377473 5.207819  z \" style=\"fill: #a1dab4; stroke: #444443; stroke-width: 0.5; stroke-linejoin: miter\" />\n",
       "   </g>\n",
       "   <g id=\"text_1\">\n",
       "    \n",
       "    <g style=\"fill: #444443\" transform=\"translate(17.950449 64.860911)scale(0.09 -0.09)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-6e\" d=\"M 422 0  L 422 3319  L 928 3319  L 928 2847  Q 1294 3394 1984 3394  Q 2284 3394 2536 3286  Q 2788 3178 2913 3003  Q 3038 2828 3088 2588  Q 3119 2431 3119 2041  L 3119 0  L 2556 0  L 2556 2019  Q 2556 2363 2490 2533  Q 2425 2703 2258 2804  Q 2091 2906 1866 2906  Q 1506 2906 1245 2678  Q 984 2450 984 1813  L 984 0  L 422 0  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-3d\" d=\"M 3381 2694  L 356 2694  L 356 3219  L 3381 3219  L 3381 2694  z M 3381 1303  L 356 1303  L 356 1828  L 3381 1828  L 3381 1303  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-33\" d=\"M 269 1209  L 831 1284  Q 928 806 1161 595  Q 1394 384 1728 384  Q 2125 384 2398 659  Q 2672 934 2672 1341  Q 2672 1728 2419 1979  Q 2166 2231 1775 2231  Q 1616 2231 1378 2169  L 1441 2663  Q 1497 2656 1531 2656  Q 1891 2656 2178 2843  Q 2466 3031 2466 3422  Q 2466 3731 2256 3934  Q 2047 4138 1716 4138  Q 1388 4138 1169 3931  Q 950 3725 888 3313  L 325 3413  Q 428 3978 793 4289  Q 1159 4600 1703 4600  Q 2078 4600 2393 4439  Q 2709 4278 2876 4000  Q 3044 3722 3044 3409  Q 3044 3113 2884 2869  Q 2725 2625 2413 2481  Q 2819 2388 3044 2092  Q 3269 1797 3269 1353  Q 3269 753 2831 336  Q 2394 -81 1725 -81  Q 1122 -81 723 278  Q 325 638 269 1209  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-39\" d=\"M 350 1059  L 891 1109  Q 959 728 1153 556  Q 1347 384 1650 384  Q 1909 384 2104 503  Q 2300 622 2425 820  Q 2550 1019 2634 1356  Q 2719 1694 2719 2044  Q 2719 2081 2716 2156  Q 2547 1888 2255 1720  Q 1963 1553 1622 1553  Q 1053 1553 659 1965  Q 266 2378 266 3053  Q 266 3750 677 4175  Q 1088 4600 1706 4600  Q 2153 4600 2523 4359  Q 2894 4119 3086 3673  Q 3278 3228 3278 2384  Q 3278 1506 3087 986  Q 2897 466 2520 194  Q 2144 -78 1638 -78  Q 1100 -78 759 220  Q 419 519 350 1059  z M 2653 3081  Q 2653 3566 2395 3850  Q 2138 4134 1775 4134  Q 1400 4134 1122 3828  Q 844 3522 844 3034  Q 844 2597 1108 2323  Q 1372 2050 1759 2050  Q 2150 2050 2401 2323  Q 2653 2597 2653 3081  z \" transform=\"scale(0.015625)\" />\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-6e\" />\n",
       "     <use xlink:href=\"#ArialMT-3d\" x=\"55.615234\" />\n",
       "     <use xlink:href=\"#ArialMT-33\" x=\"114.013672\" />\n",
       "     <use xlink:href=\"#ArialMT-39\" x=\"169.628906\" />\n",
       "    </g>\n",
       "    \n",
       "    <g style=\"fill: #444443\" transform=\"translate(20.831152 74.380098)scale(0.09 -0.09)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-70\" d=\"M 422 -1272  L 422 3319  L 934 3319  L 934 2888  Q 1116 3141 1344 3267  Q 1572 3394 1897 3394  Q 2322 3394 2647 3175  Q 2972 2956 3137 2557  Q 3303 2159 3303 1684  Q 3303 1175 3120 767  Q 2938 359 2589 142  Q 2241 -75 1856 -75  Q 1575 -75 1351 44  Q 1128 163 984 344  L 984 -1272  L 422 -1272  z M 931 1641  Q 931 1000 1190 694  Q 1450 388 1819 388  Q 2194 388 2461 705  Q 2728 1022 2728 1688  Q 2728 2322 2467 2637  Q 2206 2953 1844 2953  Q 1484 2953 1207 2617  Q 931 2281 931 1641  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-6f\" d=\"M 213 1659  Q 213 2581 725 3025  Q 1153 3394 1769 3394  Q 2453 3394 2887 2945  Q 3322 2497 3322 1706  Q 3322 1066 3130 698  Q 2938 331 2570 128  Q 2203 -75 1769 -75  Q 1072 -75 642 372  Q 213 819 213 1659  z M 791 1659  Q 791 1022 1069 705  Q 1347 388 1769 388  Q 2188 388 2466 706  Q 2744 1025 2744 1678  Q 2744 2294 2464 2611  Q 2184 2928 1769 2928  Q 1347 2928 1069 2612  Q 791 2297 791 1659  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-7a\" d=\"M 125 0  L 125 456  L 2238 2881  Q 1878 2863 1603 2863  L 250 2863  L 250 3319  L 2963 3319  L 2963 2947  L 1166 841  L 819 456  Q 1197 484 1528 484  L 3063 484  L 3063 0  L 125 0  z \" transform=\"scale(0.015625)\" />\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-70\" />\n",
       "     <use xlink:href=\"#ArialMT-6f\" x=\"55.615234\" />\n",
       "     <use xlink:href=\"#ArialMT-7a\" x=\"111.230469\" />\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       "</svg></g>\n",
       "\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>node4-&gt;leaf5</title>\n",
       "<path fill=\"none\" stroke=\"#444443\" stroke-width=\"0.3\" d=\"M187.85,-99.69C187.28,-94.56 186.4,-86.61 185.47,-78.26\" />\n",
       "<polygon fill=\"#444443\" stroke=\"#444443\" stroke-width=\"0.3\" points=\"186.85,-77.96 185.02,-74.14 184.07,-78.27 186.85,-77.96\" />\n",
       "</g>\n",
       "\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>leaf6</title>\n",
       "<polygon fill=\"none\" stroke=\"#444443\" stroke-width=\"0\" points=\"239,-57 220,-57 220,-26 239,-26 239,-57\" />\n",
       "<svg width=\"11px\" height=\"23px\" viewBox=\"0 0 15.26625 31.046797\" version=\"1.1\" preserveAspectRatio=\"xMinYMin meet\" x=\"224.5\" y=\"-52.5\">\n",
       " <metadata>\n",
       "  <rdf:RDF>\n",
       "   <ns2:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\" />\n",
       "    <dc:date>2023-04-10T10:07:52.452227</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <ns2:Agent>\n",
       "      <dc:title>Matplotlib v3.5.2, https://matplotlib.org/</dc:title>\n",
       "     </ns2:Agent>\n",
       "    </dc:creator>\n",
       "   </ns2:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 31.046798  L 15.26625 31.046798  L 15.26625 -0  L 0 -0  L 0 31.046798  z \" style=\"fill: none\" />\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 13.975125 6.342  C 13.975125 5.509176 13.811077 4.684451 13.492369 3.915022  C 13.173661 3.145593 12.706492 2.446424 12.117596 1.857529  C 11.528701 1.268633 10.829532 0.801464 10.060103 0.482756  C 9.290674 0.164048 8.465949 0 7.633125 0  C 6.800301 0 5.975576 0.164048 5.206147 0.482756  C 4.436718 0.801464 3.737549 1.268633 3.148654 1.857529  C 2.559758 2.446424 2.092589 3.145593 1.773881 3.915022  C 1.455173 4.684451 1.291125 5.509176 1.291125 6.342  C 1.291125 7.174824 1.455173 7.999549 1.773881 8.768978  C 2.092589 9.538407 2.559758 10.237576 3.148654 10.826471  C 3.737549 11.415367 4.436718 11.882536 5.206147 12.201244  C 5.975576 12.519952 6.800301 12.684 7.633125 12.684  C 8.465949 12.684 9.290674 12.519952 10.060103 12.201244  C 10.829532 11.882536 11.528701 11.415367 12.117596 10.826471  C 12.706492 10.237576 13.173661 9.538407 13.492369 8.768978  C 13.811077 7.999549 13.975125 7.174824 13.975125 6.342  M 7.633125 6.342  M 13.975125 6.342  z \" style=\"fill: #fefebb; stroke: #444443; stroke-width: 0.5; stroke-linejoin: miter\" />\n",
       "   </g>\n",
       "   <g id=\"text_1\">\n",
       "    \n",
       "    <g style=\"fill: #444443\" transform=\"translate(0 19.633391)scale(0.09 -0.09)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-6e\" d=\"M 422 0  L 422 3319  L 928 3319  L 928 2847  Q 1294 3394 1984 3394  Q 2284 3394 2536 3286  Q 2788 3178 2913 3003  Q 3038 2828 3088 2588  Q 3119 2431 3119 2041  L 3119 0  L 2556 0  L 2556 2019  Q 2556 2363 2490 2533  Q 2425 2703 2258 2804  Q 2091 2906 1866 2906  Q 1506 2906 1245 2678  Q 984 2450 984 1813  L 984 0  L 422 0  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-3d\" d=\"M 3381 2694  L 356 2694  L 356 3219  L 3381 3219  L 3381 2694  z M 3381 1303  L 356 1303  L 356 1828  L 3381 1828  L 3381 1303  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-33\" d=\"M 269 1209  L 831 1284  Q 928 806 1161 595  Q 1394 384 1728 384  Q 2125 384 2398 659  Q 2672 934 2672 1341  Q 2672 1728 2419 1979  Q 2166 2231 1775 2231  Q 1616 2231 1378 2169  L 1441 2663  Q 1497 2656 1531 2656  Q 1891 2656 2178 2843  Q 2466 3031 2466 3422  Q 2466 3731 2256 3934  Q 2047 4138 1716 4138  Q 1388 4138 1169 3931  Q 950 3725 888 3313  L 325 3413  Q 428 3978 793 4289  Q 1159 4600 1703 4600  Q 2078 4600 2393 4439  Q 2709 4278 2876 4000  Q 3044 3722 3044 3409  Q 3044 3113 2884 2869  Q 2725 2625 2413 2481  Q 2819 2388 3044 2092  Q 3269 1797 3269 1353  Q 3269 753 2831 336  Q 2394 -81 1725 -81  Q 1122 -81 723 278  Q 325 638 269 1209  z \" transform=\"scale(0.015625)\" />\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-6e\" />\n",
       "     <use xlink:href=\"#ArialMT-3d\" x=\"55.615234\" />\n",
       "     <use xlink:href=\"#ArialMT-33\" x=\"114.013672\" />\n",
       "    </g>\n",
       "    \n",
       "    <g style=\"fill: #444443\" transform=\"translate(0.125859 29.152579)scale(0.09 -0.09)\">\n",
       "     <defs>\n",
       "      <path id=\"ArialMT-65\" d=\"M 2694 1069  L 3275 997  Q 3138 488 2766 206  Q 2394 -75 1816 -75  Q 1088 -75 661 373  Q 234 822 234 1631  Q 234 2469 665 2931  Q 1097 3394 1784 3394  Q 2450 3394 2872 2941  Q 3294 2488 3294 1666  Q 3294 1616 3291 1516  L 816 1516  Q 847 969 1125 678  Q 1403 388 1819 388  Q 2128 388 2347 550  Q 2566 713 2694 1069  z M 847 1978  L 2700 1978  Q 2663 2397 2488 2606  Q 2219 2931 1791 2931  Q 1403 2931 1139 2672  Q 875 2413 847 1978  z \" transform=\"scale(0.015625)\" />\n",
       "      <path id=\"ArialMT-67\" d=\"M 319 -275  L 866 -356  Q 900 -609 1056 -725  Q 1266 -881 1628 -881  Q 2019 -881 2231 -725  Q 2444 -569 2519 -288  Q 2563 -116 2559 434  Q 2191 0 1641 0  Q 956 0 581 494  Q 206 988 206 1678  Q 206 2153 378 2554  Q 550 2956 876 3175  Q 1203 3394 1644 3394  Q 2231 3394 2613 2919  L 2613 3319  L 3131 3319  L 3131 450  Q 3131 -325 2973 -648  Q 2816 -972 2473 -1159  Q 2131 -1347 1631 -1347  Q 1038 -1347 672 -1080  Q 306 -813 319 -275  z M 784 1719  Q 784 1066 1043 766  Q 1303 466 1694 466  Q 2081 466 2343 764  Q 2606 1063 2606 1700  Q 2606 2309 2336 2618  Q 2066 2928 1684 2928  Q 1309 2928 1046 2623  Q 784 2319 784 1719  z \" transform=\"scale(0.015625)\" />\n",
       "     </defs>\n",
       "     <use xlink:href=\"#ArialMT-6e\" />\n",
       "     <use xlink:href=\"#ArialMT-65\" x=\"55.615234\" />\n",
       "     <use xlink:href=\"#ArialMT-67\" x=\"111.230469\" />\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       "</svg></g>\n",
       "\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>node4-&gt;leaf6</title>\n",
       "<path fill=\"none\" stroke=\"#444443\" stroke-width=\"0.3\" d=\"M192.28,-99.69C197.97,-90.94 208.98,-74.03 217.55,-60.86\" />\n",
       "<polygon fill=\"#444443\" stroke=\"#444443\" stroke-width=\"0.3\" points=\"218.89,-61.37 219.9,-57.25 216.54,-59.84 218.89,-61.37\" />\n",
       "</g>\n",
       "\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>node0</title>\n",
       "<text text-anchor=\"start\" x=\"82.5\" y=\"-162.9\" font-family=\"Helvetica,sans-Serif\" font-size=\"12.00\" fill=\"#444443\">Dinophysis fortii@37.50</text>\n",
       "</g>\n",
       "\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>node0-&gt;node1</title>\n",
       "<path fill=\"none\" stroke=\"#444443\" stroke-width=\"0.3\" d=\"M142.42,-159.28C135.3,-149.29 120.54,-128.6 112,-116.62\" />\n",
       "<polygon fill=\"#444443\" stroke=\"#444443\" stroke-width=\"0.3\" points=\"113.03,-115.64 109.56,-113.2 110.75,-117.27 113.03,-115.64\" />\n",
       "<text text-anchor=\"start\" x=\"125.5\" y=\"-124.8\" font-family=\"Times,serif\" font-size=\"14.00\">&#8804;</text>\n",
       "</g>\n",
       "\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>node0-&gt;node4</title>\n",
       "<path fill=\"none\" stroke=\"#444443\" stroke-width=\"0.3\" d=\"M150.68,-159.28C157.98,-149.29 173.09,-128.6 181.84,-116.62\" />\n",
       "<polygon fill=\"#444443\" stroke=\"#444443\" stroke-width=\"0.3\" points=\"183.11,-117.26 184.34,-113.2 180.85,-115.6 183.11,-117.26\" />\n",
       "<text text-anchor=\"start\" x=\"178.5\" y=\"-124.8\" font-family=\"Times,serif\" font-size=\"14.00\">&gt;</text>\n",
       "</g>\n",
       "\n",
       "\n",
       "\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>legend</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-width=\"0\" points=\"263.5,-188 217.5,-188 217.5,-144 263.5,-144 263.5,-188\" />\n",
       "<svg width=\"42px\" height=\"40px\" viewBox=\"0 0 56.182812 54.36\" version=\"1.1\" preserveAspectRatio=\"xMinYMin meet\" x=\"219.5\" y=\"-186\">\n",
       " <metadata>\n",
       "  <rdf:RDF>\n",
       "   <ns2:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\" />\n",
       "    <dc:date>2023-04-10T10:07:52.243631</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <ns2:Agent>\n",
       "      <dc:title>Matplotlib v3.5.2, https://matplotlib.org/</dc:title>\n",
       "     </ns2:Agent>\n",
       "    </dc:creator>\n",
       "   </ns2:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 54.36  L 56.182812 54.36  L 56.182812 0  L 0 0  L 0 54.36  z \" style=\"fill: none\" />\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_2\">\n",
       "     <path d=\"M 2 54.262031  L 54.182812 54.262031  Q 56.182812 54.262031 56.182812 52.262031  L 56.182812 2.097969  Q 56.182812 0.097969 54.182812 0.097969  L 2 0.097969  Q 0 0.097969 0 2.097969  L 0 52.262031  Q 0 54.262031 2 54.262031  z \" style=\"fill: #ffffff; opacity: 0.8; stroke: #444443; stroke-width: 0.5; stroke-linejoin: miter\" />\n",
       "    </g>\n",
       "    <g id=\"text_1\">\n",
       "     \n",
       "     <g style=\"fill: #444443\" transform=\"translate(10.501562 15.696406)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-Bold-74\" d=\"M 1759 4494  L 1759 3500  L 2913 3500  L 2913 2700  L 1759 2700  L 1759 1216  Q 1759 972 1856 886  Q 1953 800 2241 800  L 2816 800  L 2816 0  L 1856 0  Q 1194 0 917 276  Q 641 553 641 1216  L 641 2700  L 84 2700  L 84 3500  L 641 3500  L 641 4494  L 1759 4494  z \" transform=\"scale(0.015625)\" />\n",
       "       <path id=\"DejaVuSans-Bold-61\" d=\"M 2106 1575  Q 1756 1575 1579 1456  Q 1403 1338 1403 1106  Q 1403 894 1545 773  Q 1688 653 1941 653  Q 2256 653 2472 879  Q 2688 1106 2688 1447  L 2688 1575  L 2106 1575  z M 3816 1997  L 3816 0  L 2688 0  L 2688 519  Q 2463 200 2181 54  Q 1900 -91 1497 -91  Q 953 -91 614 226  Q 275 544 275 1050  Q 275 1666 698 1953  Q 1122 2241 2028 2241  L 2688 2241  L 2688 2328  Q 2688 2594 2478 2717  Q 2269 2841 1825 2841  Q 1466 2841 1156 2769  Q 847 2697 581 2553  L 581 3406  Q 941 3494 1303 3539  Q 1666 3584 2028 3584  Q 2975 3584 3395 3211  Q 3816 2838 3816 1997  z \" transform=\"scale(0.015625)\" />\n",
       "       <path id=\"DejaVuSans-Bold-72\" d=\"M 3138 2547  Q 2991 2616 2845 2648  Q 2700 2681 2553 2681  Q 2122 2681 1889 2404  Q 1656 2128 1656 1613  L 1656 0  L 538 0  L 538 3500  L 1656 3500  L 1656 2925  Q 1872 3269 2151 3426  Q 2431 3584 2822 3584  Q 2878 3584 2943 3579  Q 3009 3575 3134 3559  L 3138 2547  z \" transform=\"scale(0.015625)\" />\n",
       "       <path id=\"DejaVuSans-Bold-67\" d=\"M 2919 594  Q 2688 288 2409 144  Q 2131 0 1766 0  Q 1125 0 706 504  Q 288 1009 288 1791  Q 288 2575 706 3076  Q 1125 3578 1766 3578  Q 2131 3578 2409 3434  Q 2688 3291 2919 2981  L 2919 3500  L 4044 3500  L 4044 353  Q 4044 -491 3511 -936  Q 2978 -1381 1966 -1381  Q 1638 -1381 1331 -1331  Q 1025 -1281 716 -1178  L 716 -306  Q 1009 -475 1290 -558  Q 1572 -641 1856 -641  Q 2406 -641 2662 -400  Q 2919 -159 2919 353  L 2919 594  z M 2181 2772  Q 1834 2772 1640 2515  Q 1447 2259 1447 1791  Q 1447 1309 1634 1061  Q 1822 813 2181 813  Q 2531 813 2725 1069  Q 2919 1325 2919 1791  Q 2919 2259 2725 2515  Q 2531 2772 2181 2772  z \" transform=\"scale(0.015625)\" />\n",
       "       <path id=\"DejaVuSans-Bold-65\" d=\"M 4031 1759  L 4031 1441  L 1416 1441  Q 1456 1047 1700 850  Q 1944 653 2381 653  Q 2734 653 3104 758  Q 3475 863 3866 1075  L 3866 213  Q 3469 63 3072 -14  Q 2675 -91 2278 -91  Q 1328 -91 801 392  Q 275 875 275 1747  Q 275 2603 792 3093  Q 1309 3584 2216 3584  Q 3041 3584 3536 3087  Q 4031 2591 4031 1759  z M 2881 2131  Q 2881 2450 2695 2645  Q 2509 2841 2209 2841  Q 1884 2841 1681 2658  Q 1478 2475 1428 2131  L 2881 2131  z \" transform=\"scale(0.015625)\" />\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-Bold-74\" />\n",
       "      <use xlink:href=\"#DejaVuSans-Bold-61\" x=\"47.802734\" />\n",
       "      <use xlink:href=\"#DejaVuSans-Bold-72\" x=\"115.283203\" />\n",
       "      <use xlink:href=\"#DejaVuSans-Bold-67\" x=\"164.599609\" />\n",
       "      <use xlink:href=\"#DejaVuSans-Bold-65\" x=\"236.181641\" />\n",
       "      <use xlink:href=\"#DejaVuSans-Bold-74\" x=\"304.003906\" />\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"patch_3\">\n",
       "     <path d=\"M 8 30.012031  L 28 30.012031  L 28 23.012031  L 8 23.012031  z \" style=\"fill: #fefebb; stroke: #444443; stroke-width: 0.4; stroke-linejoin: miter\" />\n",
       "    </g>\n",
       "    <g id=\"text_2\">\n",
       "     \n",
       "     <g style=\"fill: #444443\" transform=\"translate(31.5 30.012031)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"ArialMT-6e\" d=\"M 422 0  L 422 3319  L 928 3319  L 928 2847  Q 1294 3394 1984 3394  Q 2284 3394 2536 3286  Q 2788 3178 2913 3003  Q 3038 2828 3088 2588  Q 3119 2431 3119 2041  L 3119 0  L 2556 0  L 2556 2019  Q 2556 2363 2490 2533  Q 2425 2703 2258 2804  Q 2091 2906 1866 2906  Q 1506 2906 1245 2678  Q 984 2450 984 1813  L 984 0  L 422 0  z \" transform=\"scale(0.015625)\" />\n",
       "       <path id=\"ArialMT-65\" d=\"M 2694 1069  L 3275 997  Q 3138 488 2766 206  Q 2394 -75 1816 -75  Q 1088 -75 661 373  Q 234 822 234 1631  Q 234 2469 665 2931  Q 1097 3394 1784 3394  Q 2450 3394 2872 2941  Q 3294 2488 3294 1666  Q 3294 1616 3291 1516  L 816 1516  Q 847 969 1125 678  Q 1403 388 1819 388  Q 2128 388 2347 550  Q 2566 713 2694 1069  z M 847 1978  L 2700 1978  Q 2663 2397 2488 2606  Q 2219 2931 1791 2931  Q 1403 2931 1139 2672  Q 875 2413 847 1978  z \" transform=\"scale(0.015625)\" />\n",
       "       <path id=\"ArialMT-67\" d=\"M 319 -275  L 866 -356  Q 900 -609 1056 -725  Q 1266 -881 1628 -881  Q 2019 -881 2231 -725  Q 2444 -569 2519 -288  Q 2563 -116 2559 434  Q 2191 0 1641 0  Q 956 0 581 494  Q 206 988 206 1678  Q 206 2153 378 2554  Q 550 2956 876 3175  Q 1203 3394 1644 3394  Q 2231 3394 2613 2919  L 2613 3319  L 3131 3319  L 3131 450  Q 3131 -325 2973 -648  Q 2816 -972 2473 -1159  Q 2131 -1347 1631 -1347  Q 1038 -1347 672 -1080  Q 306 -813 319 -275  z M 784 1719  Q 784 1066 1043 766  Q 1303 466 1694 466  Q 2081 466 2343 764  Q 2606 1063 2606 1700  Q 2606 2309 2336 2618  Q 2066 2928 1684 2928  Q 1309 2928 1046 2623  Q 784 2319 784 1719  z \" transform=\"scale(0.015625)\" />\n",
       "      </defs>\n",
       "      <use xlink:href=\"#ArialMT-6e\" />\n",
       "      <use xlink:href=\"#ArialMT-65\" x=\"55.615234\" />\n",
       "      <use xlink:href=\"#ArialMT-67\" x=\"111.230469\" />\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"patch_4\">\n",
       "     <path d=\"M 8 44.274531  L 28 44.274531  L 28 37.274531  L 8 37.274531  z \" style=\"fill: #a1dab4; stroke: #444443; stroke-width: 0.4; stroke-linejoin: miter\" />\n",
       "    </g>\n",
       "    <g id=\"text_3\">\n",
       "     \n",
       "     <g style=\"fill: #444443\" transform=\"translate(31.5 44.274531)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"ArialMT-70\" d=\"M 422 -1272  L 422 3319  L 934 3319  L 934 2888  Q 1116 3141 1344 3267  Q 1572 3394 1897 3394  Q 2322 3394 2647 3175  Q 2972 2956 3137 2557  Q 3303 2159 3303 1684  Q 3303 1175 3120 767  Q 2938 359 2589 142  Q 2241 -75 1856 -75  Q 1575 -75 1351 44  Q 1128 163 984 344  L 984 -1272  L 422 -1272  z M 931 1641  Q 931 1000 1190 694  Q 1450 388 1819 388  Q 2194 388 2461 705  Q 2728 1022 2728 1688  Q 2728 2322 2467 2637  Q 2206 2953 1844 2953  Q 1484 2953 1207 2617  Q 931 2281 931 1641  z \" transform=\"scale(0.015625)\" />\n",
       "       <path id=\"ArialMT-6f\" d=\"M 213 1659  Q 213 2581 725 3025  Q 1153 3394 1769 3394  Q 2453 3394 2887 2945  Q 3322 2497 3322 1706  Q 3322 1066 3130 698  Q 2938 331 2570 128  Q 2203 -75 1769 -75  Q 1072 -75 642 372  Q 213 819 213 1659  z M 791 1659  Q 791 1022 1069 705  Q 1347 388 1769 388  Q 2188 388 2466 706  Q 2744 1025 2744 1678  Q 2744 2294 2464 2611  Q 2184 2928 1769 2928  Q 1347 2928 1069 2612  Q 791 2297 791 1659  z \" transform=\"scale(0.015625)\" />\n",
       "       <path id=\"ArialMT-7a\" d=\"M 125 0  L 125 456  L 2238 2881  Q 1878 2863 1603 2863  L 250 2863  L 250 3319  L 2963 3319  L 2963 2947  L 1166 841  L 819 456  Q 1197 484 1528 484  L 3063 484  L 3063 0  L 125 0  z \" transform=\"scale(0.015625)\" />\n",
       "      </defs>\n",
       "      <use xlink:href=\"#ArialMT-70\" />\n",
       "      <use xlink:href=\"#ArialMT-6f\" x=\"55.615234\" />\n",
       "      <use xlink:href=\"#ArialMT-7a\" x=\"111.230469\" />\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       "</svg></g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<dtreeviz.trees.DTreeViz at 0x13d2fd130>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = gscv_dt.best_estimator_.steps[2][1]\n",
    "viz = dtreeviz(clf, X, y,\n",
    "                target_name=\"target\",\n",
    "                feature_names=X.columns,\n",
    "                class_names=[\"neg\", \"poz\"],\n",
    "             fancy=False,\n",
    "               scale=1.5\n",
    "              )\n",
    "\n",
    "viz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0db8e4c-76d7-4b28-a874-e70f5256bf91",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be1409e-0948-4107-bc34-558b1bb08a35",
   "metadata": {},
   "source": [
    "#### Model evaluation (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d8c5f08-1792-4ae9-b2ca-e72b2c24fe00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 486 candidates, totalling 1458 fits\n",
      "Best estimator saved as: objects/estimators/RandomForestClassifier-HAB_modelling_5_7-10042023_1009.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>126</th>\n",
       "      <th>411</th>\n",
       "      <th>463</th>\n",
       "      <th>82</th>\n",
       "      <th>224</th>\n",
       "      <th>376</th>\n",
       "      <th>105</th>\n",
       "      <th>92</th>\n",
       "      <th>300</th>\n",
       "      <th>216</th>\n",
       "      <th>104</th>\n",
       "      <th>175</th>\n",
       "      <th>468</th>\n",
       "      <th>385</th>\n",
       "      <th>12</th>\n",
       "      <th>396</th>\n",
       "      <th>169</th>\n",
       "      <th>...</th>\n",
       "      <th>295</th>\n",
       "      <th>344</th>\n",
       "      <th>237</th>\n",
       "      <th>263</th>\n",
       "      <th>136</th>\n",
       "      <th>134</th>\n",
       "      <th>191</th>\n",
       "      <th>311</th>\n",
       "      <th>74</th>\n",
       "      <th>146</th>\n",
       "      <th>384</th>\n",
       "      <th>3</th>\n",
       "      <th>165</th>\n",
       "      <th>272</th>\n",
       "      <th>75</th>\n",
       "      <th>290</th>\n",
       "      <th>219</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>0.689731</td>\n",
       "      <td>0.818113</td>\n",
       "      <td>0.773781</td>\n",
       "      <td>0.684903</td>\n",
       "      <td>0.317713</td>\n",
       "      <td>1.234373</td>\n",
       "      <td>1.251706</td>\n",
       "      <td>1.073898</td>\n",
       "      <td>0.927014</td>\n",
       "      <td>0.299425</td>\n",
       "      <td>1.153692</td>\n",
       "      <td>0.310319</td>\n",
       "      <td>1.17535</td>\n",
       "      <td>0.269808</td>\n",
       "      <td>0.221702</td>\n",
       "      <td>0.730845</td>\n",
       "      <td>0.303432</td>\n",
       "      <td>...</td>\n",
       "      <td>0.949684</td>\n",
       "      <td>0.702515</td>\n",
       "      <td>0.898744</td>\n",
       "      <td>1.416587</td>\n",
       "      <td>0.683938</td>\n",
       "      <td>0.742342</td>\n",
       "      <td>0.825548</td>\n",
       "      <td>1.498386</td>\n",
       "      <td>0.680617</td>\n",
       "      <td>1.11247</td>\n",
       "      <td>0.279356</td>\n",
       "      <td>0.208524</td>\n",
       "      <td>0.312586</td>\n",
       "      <td>0.289324</td>\n",
       "      <td>0.734914</td>\n",
       "      <td>0.825505</td>\n",
       "      <td>0.31888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.00152</td>\n",
       "      <td>0.006781</td>\n",
       "      <td>0.007133</td>\n",
       "      <td>0.011361</td>\n",
       "      <td>0.002402</td>\n",
       "      <td>0.009563</td>\n",
       "      <td>0.005609</td>\n",
       "      <td>0.019837</td>\n",
       "      <td>0.005718</td>\n",
       "      <td>0.003086</td>\n",
       "      <td>0.006945</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.008009</td>\n",
       "      <td>0.005075</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>0.00162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008231</td>\n",
       "      <td>0.002932</td>\n",
       "      <td>0.015098</td>\n",
       "      <td>0.004891</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.003674</td>\n",
       "      <td>0.01603</td>\n",
       "      <td>0.009003</td>\n",
       "      <td>0.012754</td>\n",
       "      <td>0.006316</td>\n",
       "      <td>0.002627</td>\n",
       "      <td>0.014267</td>\n",
       "      <td>0.004533</td>\n",
       "      <td>0.00434</td>\n",
       "      <td>0.006375</td>\n",
       "      <td>0.006258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.120347</td>\n",
       "      <td>0.129402</td>\n",
       "      <td>0.130636</td>\n",
       "      <td>0.124255</td>\n",
       "      <td>0.050639</td>\n",
       "      <td>0.205502</td>\n",
       "      <td>0.190514</td>\n",
       "      <td>0.196611</td>\n",
       "      <td>0.130656</td>\n",
       "      <td>0.048622</td>\n",
       "      <td>0.187298</td>\n",
       "      <td>0.049214</td>\n",
       "      <td>0.198735</td>\n",
       "      <td>0.050032</td>\n",
       "      <td>0.04086</td>\n",
       "      <td>0.131854</td>\n",
       "      <td>0.048707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132821</td>\n",
       "      <td>0.132466</td>\n",
       "      <td>0.124929</td>\n",
       "      <td>0.210636</td>\n",
       "      <td>0.120998</td>\n",
       "      <td>0.121703</td>\n",
       "      <td>0.126653</td>\n",
       "      <td>0.206228</td>\n",
       "      <td>0.12314</td>\n",
       "      <td>0.19264</td>\n",
       "      <td>0.049579</td>\n",
       "      <td>0.037107</td>\n",
       "      <td>0.047235</td>\n",
       "      <td>0.048287</td>\n",
       "      <td>0.120059</td>\n",
       "      <td>0.126528</td>\n",
       "      <td>0.050307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.005557</td>\n",
       "      <td>0.002955</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.002603</td>\n",
       "      <td>0.006185</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0.004903</td>\n",
       "      <td>0.003433</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.00333</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.00201</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.00514</td>\n",
       "      <td>0.000659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003062</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.00376</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.002828</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.002818</td>\n",
       "      <td>0.00125</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.002434</td>\n",
       "      <td>0.002071</td>\n",
       "      <td>0.000442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__class_weight</th>\n",
       "      <td>balanced</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>None</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>balanced</td>\n",
       "      <td>None</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>...</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>None</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>None</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>balanced_subsample</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__criterion</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>entropy</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>entropy</td>\n",
       "      <td>entropy</td>\n",
       "      <td>gini</td>\n",
       "      <td>entropy</td>\n",
       "      <td>entropy</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>entropy</td>\n",
       "      <td>entropy</td>\n",
       "      <td>gini</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>entropy</td>\n",
       "      <td>gini</td>\n",
       "      <td>entropy</td>\n",
       "      <td>gini</td>\n",
       "      <td>...</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>gini</td>\n",
       "      <td>entropy</td>\n",
       "      <td>entropy</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>gini</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>entropy</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>entropy</td>\n",
       "      <td>gini</td>\n",
       "      <td>gini</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>entropy</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>entropy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__n_estimators</th>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>500</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>500</td>\n",
       "      <td>300</td>\n",
       "      <td>500</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_smt__k_neighbors</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_smt__sampling_strategy</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_under__sampling_strategy</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': 'balanced_subsample', 'c...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': 'balanced_subsample', 'c...</td>\n",
       "      <td>{'clf__class_weight': 'balanced_subsample', 'c...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': 'balanced_subsample', 'c...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': 'balanced_subsample', 'c...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'clf__class_weight': 'balanced_subsample', 'c...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': 'balanced_subsample', 'c...</td>\n",
       "      <td>{'clf__class_weight': 'balanced_subsample', 'c...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': 'balanced_subsample', 'c...</td>\n",
       "      <td>{'clf__class_weight': 'balanced_subsample', 'c...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__criterion': ...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': 'balanced_subsample', 'c...</td>\n",
       "      <td>{'clf__class_weight': 'balanced_subsample', 'c...</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__criter...</td>\n",
       "      <td>{'clf__class_weight': 'balanced_subsample', 'c...</td>\n",
       "      <td>{'clf__class_weight': 'balanced_subsample', 'c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_recall</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.55</td>\n",
       "      <td>...</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_recall</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.238095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_recall</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_recall</th>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.469048</td>\n",
       "      <td>0.548413</td>\n",
       "      <td>0.549206</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.469048</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.581746</td>\n",
       "      <td>0.419841</td>\n",
       "      <td>0.468254</td>\n",
       "      <td>0.531746</td>\n",
       "      <td>0.484921</td>\n",
       "      <td>0.499206</td>\n",
       "      <td>0.468254</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.46746</td>\n",
       "      <td>0.469048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370635</td>\n",
       "      <td>0.501587</td>\n",
       "      <td>0.355556</td>\n",
       "      <td>0.515873</td>\n",
       "      <td>0.435714</td>\n",
       "      <td>0.419048</td>\n",
       "      <td>0.484921</td>\n",
       "      <td>0.453175</td>\n",
       "      <td>0.469048</td>\n",
       "      <td>0.46746</td>\n",
       "      <td>0.340476</td>\n",
       "      <td>0.32381</td>\n",
       "      <td>0.337302</td>\n",
       "      <td>0.451587</td>\n",
       "      <td>0.355556</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.32381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_recall</th>\n",
       "      <td>0.062904</td>\n",
       "      <td>0.096558</td>\n",
       "      <td>0.058332</td>\n",
       "      <td>0.122047</td>\n",
       "      <td>0.097202</td>\n",
       "      <td>0.096558</td>\n",
       "      <td>0.084739</td>\n",
       "      <td>0.108524</td>\n",
       "      <td>0.062098</td>\n",
       "      <td>0.137922</td>\n",
       "      <td>0.062492</td>\n",
       "      <td>0.107545</td>\n",
       "      <td>0.085191</td>\n",
       "      <td>0.09976</td>\n",
       "      <td>0.084739</td>\n",
       "      <td>0.078736</td>\n",
       "      <td>0.096558</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041528</td>\n",
       "      <td>0.119544</td>\n",
       "      <td>0.120802</td>\n",
       "      <td>0.155928</td>\n",
       "      <td>0.117079</td>\n",
       "      <td>0.078919</td>\n",
       "      <td>0.07429</td>\n",
       "      <td>0.118894</td>\n",
       "      <td>0.129931</td>\n",
       "      <td>0.078736</td>\n",
       "      <td>0.077445</td>\n",
       "      <td>0.053875</td>\n",
       "      <td>0.099286</td>\n",
       "      <td>0.097208</td>\n",
       "      <td>0.083872</td>\n",
       "      <td>0.100019</td>\n",
       "      <td>0.06644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_recall</th>\n",
       "      <td>52</td>\n",
       "      <td>168</td>\n",
       "      <td>26</td>\n",
       "      <td>17</td>\n",
       "      <td>92</td>\n",
       "      <td>168</td>\n",
       "      <td>244</td>\n",
       "      <td>5</td>\n",
       "      <td>352</td>\n",
       "      <td>183</td>\n",
       "      <td>40</td>\n",
       "      <td>125</td>\n",
       "      <td>108</td>\n",
       "      <td>183</td>\n",
       "      <td>244</td>\n",
       "      <td>209</td>\n",
       "      <td>168</td>\n",
       "      <td>...</td>\n",
       "      <td>464</td>\n",
       "      <td>76</td>\n",
       "      <td>470</td>\n",
       "      <td>66</td>\n",
       "      <td>330</td>\n",
       "      <td>375</td>\n",
       "      <td>116</td>\n",
       "      <td>224</td>\n",
       "      <td>177</td>\n",
       "      <td>209</td>\n",
       "      <td>479</td>\n",
       "      <td>483</td>\n",
       "      <td>482</td>\n",
       "      <td>267</td>\n",
       "      <td>470</td>\n",
       "      <td>159</td>\n",
       "      <td>484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_precision</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.34375</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.28125</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_precision</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.37931</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_precision</th>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.411765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_precision</th>\n",
       "      <td>0.621212</td>\n",
       "      <td>0.612358</td>\n",
       "      <td>0.511272</td>\n",
       "      <td>0.51152</td>\n",
       "      <td>0.56711</td>\n",
       "      <td>0.598214</td>\n",
       "      <td>0.608466</td>\n",
       "      <td>0.464684</td>\n",
       "      <td>0.680927</td>\n",
       "      <td>0.593168</td>\n",
       "      <td>0.502599</td>\n",
       "      <td>0.549735</td>\n",
       "      <td>0.537304</td>\n",
       "      <td>0.587783</td>\n",
       "      <td>0.59768</td>\n",
       "      <td>0.574446</td>\n",
       "      <td>0.582492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.466742</td>\n",
       "      <td>0.352165</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.337181</td>\n",
       "      <td>0.393838</td>\n",
       "      <td>0.394269</td>\n",
       "      <td>0.358454</td>\n",
       "      <td>0.378869</td>\n",
       "      <td>0.359711</td>\n",
       "      <td>0.352549</td>\n",
       "      <td>0.494318</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.508772</td>\n",
       "      <td>0.359394</td>\n",
       "      <td>0.492845</td>\n",
       "      <td>0.341019</td>\n",
       "      <td>0.455936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_precision</th>\n",
       "      <td>0.140509</td>\n",
       "      <td>0.060851</td>\n",
       "      <td>0.032547</td>\n",
       "      <td>0.030962</td>\n",
       "      <td>0.019255</td>\n",
       "      <td>0.06765</td>\n",
       "      <td>0.087502</td>\n",
       "      <td>0.060077</td>\n",
       "      <td>0.099023</td>\n",
       "      <td>0.089687</td>\n",
       "      <td>0.018396</td>\n",
       "      <td>0.060932</td>\n",
       "      <td>0.018226</td>\n",
       "      <td>0.034958</td>\n",
       "      <td>0.121693</td>\n",
       "      <td>0.029772</td>\n",
       "      <td>0.059234</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053061</td>\n",
       "      <td>0.00862</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.034357</td>\n",
       "      <td>0.031182</td>\n",
       "      <td>0.021734</td>\n",
       "      <td>0.061422</td>\n",
       "      <td>0.025004</td>\n",
       "      <td>0.024767</td>\n",
       "      <td>0.060236</td>\n",
       "      <td>0.084657</td>\n",
       "      <td>0.033672</td>\n",
       "      <td>0.039132</td>\n",
       "      <td>0.033405</td>\n",
       "      <td>0.094556</td>\n",
       "      <td>0.033058</td>\n",
       "      <td>0.082414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_precision</th>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>177</td>\n",
       "      <td>176</td>\n",
       "      <td>69</td>\n",
       "      <td>32</td>\n",
       "      <td>22</td>\n",
       "      <td>307</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>205</td>\n",
       "      <td>99</td>\n",
       "      <td>117</td>\n",
       "      <td>44</td>\n",
       "      <td>33</td>\n",
       "      <td>58</td>\n",
       "      <td>49</td>\n",
       "      <td>...</td>\n",
       "      <td>301</td>\n",
       "      <td>483</td>\n",
       "      <td>179</td>\n",
       "      <td>486</td>\n",
       "      <td>443</td>\n",
       "      <td>441</td>\n",
       "      <td>480</td>\n",
       "      <td>463</td>\n",
       "      <td>477</td>\n",
       "      <td>482</td>\n",
       "      <td>227</td>\n",
       "      <td>146</td>\n",
       "      <td>187</td>\n",
       "      <td>478</td>\n",
       "      <td>233</td>\n",
       "      <td>485</td>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_f1</th>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.378378</td>\n",
       "      <td>0.436364</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.392157</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.470588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_f1</th>\n",
       "      <td>0.55814</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.55814</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.436364</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.44898</td>\n",
       "      <td>0.436364</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.324324</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.440678</td>\n",
       "      <td>0.294118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_f1</th>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.341463</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.368421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_f1</th>\n",
       "      <td>0.54815</td>\n",
       "      <td>0.528993</td>\n",
       "      <td>0.52588</td>\n",
       "      <td>0.525272</td>\n",
       "      <td>0.525154</td>\n",
       "      <td>0.519721</td>\n",
       "      <td>0.517446</td>\n",
       "      <td>0.515501</td>\n",
       "      <td>0.51482</td>\n",
       "      <td>0.51426</td>\n",
       "      <td>0.514097</td>\n",
       "      <td>0.513088</td>\n",
       "      <td>0.512967</td>\n",
       "      <td>0.511748</td>\n",
       "      <td>0.509957</td>\n",
       "      <td>0.509576</td>\n",
       "      <td>0.509358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.409722</td>\n",
       "      <td>0.407424</td>\n",
       "      <td>0.406937</td>\n",
       "      <td>0.403446</td>\n",
       "      <td>0.402907</td>\n",
       "      <td>0.402569</td>\n",
       "      <td>0.402222</td>\n",
       "      <td>0.401797</td>\n",
       "      <td>0.400796</td>\n",
       "      <td>0.399862</td>\n",
       "      <td>0.399775</td>\n",
       "      <td>0.399287</td>\n",
       "      <td>0.399194</td>\n",
       "      <td>0.398723</td>\n",
       "      <td>0.394828</td>\n",
       "      <td>0.393877</td>\n",
       "      <td>0.377709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_f1</th>\n",
       "      <td>0.017303</td>\n",
       "      <td>0.085085</td>\n",
       "      <td>0.019318</td>\n",
       "      <td>0.073854</td>\n",
       "      <td>0.050437</td>\n",
       "      <td>0.076344</td>\n",
       "      <td>0.083629</td>\n",
       "      <td>0.078415</td>\n",
       "      <td>0.059001</td>\n",
       "      <td>0.106513</td>\n",
       "      <td>0.022008</td>\n",
       "      <td>0.087876</td>\n",
       "      <td>0.038477</td>\n",
       "      <td>0.055638</td>\n",
       "      <td>0.084692</td>\n",
       "      <td>0.036378</td>\n",
       "      <td>0.057768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024801</td>\n",
       "      <td>0.046758</td>\n",
       "      <td>0.093525</td>\n",
       "      <td>0.074655</td>\n",
       "      <td>0.049208</td>\n",
       "      <td>0.039762</td>\n",
       "      <td>0.029979</td>\n",
       "      <td>0.049552</td>\n",
       "      <td>0.066443</td>\n",
       "      <td>0.063992</td>\n",
       "      <td>0.073828</td>\n",
       "      <td>0.050418</td>\n",
       "      <td>0.074424</td>\n",
       "      <td>0.059206</td>\n",
       "      <td>0.03537</td>\n",
       "      <td>0.034181</td>\n",
       "      <td>0.072343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_f1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>470</td>\n",
       "      <td>471</td>\n",
       "      <td>472</td>\n",
       "      <td>473</td>\n",
       "      <td>474</td>\n",
       "      <td>475</td>\n",
       "      <td>476</td>\n",
       "      <td>477</td>\n",
       "      <td>478</td>\n",
       "      <td>479</td>\n",
       "      <td>480</td>\n",
       "      <td>481</td>\n",
       "      <td>482</td>\n",
       "      <td>483</td>\n",
       "      <td>484</td>\n",
       "      <td>485</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_roc_auc</th>\n",
       "      <td>0.826096</td>\n",
       "      <td>0.816886</td>\n",
       "      <td>0.812061</td>\n",
       "      <td>0.809211</td>\n",
       "      <td>0.819298</td>\n",
       "      <td>0.819518</td>\n",
       "      <td>0.783114</td>\n",
       "      <td>0.849123</td>\n",
       "      <td>0.823465</td>\n",
       "      <td>0.859211</td>\n",
       "      <td>0.850219</td>\n",
       "      <td>0.783772</td>\n",
       "      <td>0.780921</td>\n",
       "      <td>0.808114</td>\n",
       "      <td>0.773684</td>\n",
       "      <td>0.777193</td>\n",
       "      <td>0.761623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.743202</td>\n",
       "      <td>0.777193</td>\n",
       "      <td>0.800877</td>\n",
       "      <td>0.79386</td>\n",
       "      <td>0.792763</td>\n",
       "      <td>0.77193</td>\n",
       "      <td>0.730044</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.790789</td>\n",
       "      <td>0.745395</td>\n",
       "      <td>0.739912</td>\n",
       "      <td>0.773465</td>\n",
       "      <td>0.79057</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.773904</td>\n",
       "      <td>0.775658</td>\n",
       "      <td>0.819079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_roc_auc</th>\n",
       "      <td>0.856511</td>\n",
       "      <td>0.860514</td>\n",
       "      <td>0.824062</td>\n",
       "      <td>0.855457</td>\n",
       "      <td>0.891488</td>\n",
       "      <td>0.860303</td>\n",
       "      <td>0.875263</td>\n",
       "      <td>0.848083</td>\n",
       "      <td>0.846186</td>\n",
       "      <td>0.860514</td>\n",
       "      <td>0.868732</td>\n",
       "      <td>0.845133</td>\n",
       "      <td>0.817952</td>\n",
       "      <td>0.854825</td>\n",
       "      <td>0.864307</td>\n",
       "      <td>0.887063</td>\n",
       "      <td>0.86515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.86831</td>\n",
       "      <td>0.827855</td>\n",
       "      <td>0.844711</td>\n",
       "      <td>0.841551</td>\n",
       "      <td>0.796249</td>\n",
       "      <td>0.851665</td>\n",
       "      <td>0.825116</td>\n",
       "      <td>0.861568</td>\n",
       "      <td>0.82322</td>\n",
       "      <td>0.873788</td>\n",
       "      <td>0.836072</td>\n",
       "      <td>0.831226</td>\n",
       "      <td>0.8504</td>\n",
       "      <td>0.769701</td>\n",
       "      <td>0.861778</td>\n",
       "      <td>0.803413</td>\n",
       "      <td>0.849136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_roc_auc</th>\n",
       "      <td>0.800253</td>\n",
       "      <td>0.837337</td>\n",
       "      <td>0.804467</td>\n",
       "      <td>0.804046</td>\n",
       "      <td>0.790771</td>\n",
       "      <td>0.835651</td>\n",
       "      <td>0.817741</td>\n",
       "      <td>0.786346</td>\n",
       "      <td>0.805731</td>\n",
       "      <td>0.771808</td>\n",
       "      <td>0.76338</td>\n",
       "      <td>0.766751</td>\n",
       "      <td>0.800464</td>\n",
       "      <td>0.843447</td>\n",
       "      <td>0.7933</td>\n",
       "      <td>0.809313</td>\n",
       "      <td>0.793089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.82638</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.822166</td>\n",
       "      <td>0.788453</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.806153</td>\n",
       "      <td>0.762115</td>\n",
       "      <td>0.803624</td>\n",
       "      <td>0.767804</td>\n",
       "      <td>0.799831</td>\n",
       "      <td>0.827434</td>\n",
       "      <td>0.812895</td>\n",
       "      <td>0.816266</td>\n",
       "      <td>0.76043</td>\n",
       "      <td>0.80236</td>\n",
       "      <td>0.780236</td>\n",
       "      <td>0.818584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_roc_auc</th>\n",
       "      <td>0.82762</td>\n",
       "      <td>0.838246</td>\n",
       "      <td>0.81353</td>\n",
       "      <td>0.822904</td>\n",
       "      <td>0.833852</td>\n",
       "      <td>0.838491</td>\n",
       "      <td>0.825373</td>\n",
       "      <td>0.827851</td>\n",
       "      <td>0.825127</td>\n",
       "      <td>0.830511</td>\n",
       "      <td>0.827444</td>\n",
       "      <td>0.798552</td>\n",
       "      <td>0.799779</td>\n",
       "      <td>0.835462</td>\n",
       "      <td>0.81043</td>\n",
       "      <td>0.824523</td>\n",
       "      <td>0.80662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.812631</td>\n",
       "      <td>0.788984</td>\n",
       "      <td>0.822585</td>\n",
       "      <td>0.807955</td>\n",
       "      <td>0.783639</td>\n",
       "      <td>0.809916</td>\n",
       "      <td>0.772425</td>\n",
       "      <td>0.810619</td>\n",
       "      <td>0.793938</td>\n",
       "      <td>0.806338</td>\n",
       "      <td>0.801139</td>\n",
       "      <td>0.805862</td>\n",
       "      <td>0.819079</td>\n",
       "      <td>0.755658</td>\n",
       "      <td>0.812681</td>\n",
       "      <td>0.786436</td>\n",
       "      <td>0.828933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_roc_auc</th>\n",
       "      <td>0.022992</td>\n",
       "      <td>0.017823</td>\n",
       "      <td>0.008067</td>\n",
       "      <td>0.023115</td>\n",
       "      <td>0.042386</td>\n",
       "      <td>0.016771</td>\n",
       "      <td>0.038005</td>\n",
       "      <td>0.029351</td>\n",
       "      <td>0.016558</td>\n",
       "      <td>0.041513</td>\n",
       "      <td>0.045926</td>\n",
       "      <td>0.033663</td>\n",
       "      <td>0.015126</td>\n",
       "      <td>0.019888</td>\n",
       "      <td>0.038929</td>\n",
       "      <td>0.046126</td>\n",
       "      <td>0.043334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051992</td>\n",
       "      <td>0.028186</td>\n",
       "      <td>0.017898</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.015434</td>\n",
       "      <td>0.03266</td>\n",
       "      <td>0.039492</td>\n",
       "      <td>0.039058</td>\n",
       "      <td>0.022732</td>\n",
       "      <td>0.052618</td>\n",
       "      <td>0.043438</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>0.024506</td>\n",
       "      <td>0.013832</td>\n",
       "      <td>0.03661</td>\n",
       "      <td>0.01215</td>\n",
       "      <td>0.014287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_roc_auc</th>\n",
       "      <td>104</td>\n",
       "      <td>23</td>\n",
       "      <td>287</td>\n",
       "      <td>166</td>\n",
       "      <td>47</td>\n",
       "      <td>21</td>\n",
       "      <td>133</td>\n",
       "      <td>102</td>\n",
       "      <td>138</td>\n",
       "      <td>72</td>\n",
       "      <td>106</td>\n",
       "      <td>420</td>\n",
       "      <td>412</td>\n",
       "      <td>38</td>\n",
       "      <td>319</td>\n",
       "      <td>147</td>\n",
       "      <td>360</td>\n",
       "      <td>...</td>\n",
       "      <td>298</td>\n",
       "      <td>460</td>\n",
       "      <td>170</td>\n",
       "      <td>337</td>\n",
       "      <td>471</td>\n",
       "      <td>321</td>\n",
       "      <td>484</td>\n",
       "      <td>315</td>\n",
       "      <td>442</td>\n",
       "      <td>362</td>\n",
       "      <td>404</td>\n",
       "      <td>367</td>\n",
       "      <td>217</td>\n",
       "      <td>486</td>\n",
       "      <td>297</td>\n",
       "      <td>464</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35 rows × 486 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              126  \\\n",
       "mean_fit_time                                                            0.689731   \n",
       "std_fit_time                                                              0.00152   \n",
       "mean_score_time                                                          0.120347   \n",
       "std_score_time                                                           0.001152   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__n_estimators                                                       300   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                       0.571429   \n",
       "split2_test_recall                                                       0.428571   \n",
       "mean_test_recall                                                         0.516667   \n",
       "std_test_recall                                                          0.062904   \n",
       "rank_test_recall                                                               52   \n",
       "split0_test_precision                                                         0.5   \n",
       "split1_test_precision                                                    0.545455   \n",
       "split2_test_precision                                                    0.818182   \n",
       "mean_test_precision                                                      0.621212   \n",
       "std_test_precision                                                       0.140509   \n",
       "rank_test_precision                                                            14   \n",
       "split0_test_f1                                                            0.52381   \n",
       "split1_test_f1                                                            0.55814   \n",
       "split2_test_f1                                                             0.5625   \n",
       "mean_test_f1                                                              0.54815   \n",
       "std_test_f1                                                              0.017303   \n",
       "rank_test_f1                                                                    1   \n",
       "split0_test_roc_auc                                                      0.826096   \n",
       "split1_test_roc_auc                                                      0.856511   \n",
       "split2_test_roc_auc                                                      0.800253   \n",
       "mean_test_roc_auc                                                         0.82762   \n",
       "std_test_roc_auc                                                         0.022992   \n",
       "rank_test_roc_auc                                                             104   \n",
       "\n",
       "                                                                              411  \\\n",
       "mean_fit_time                                                            0.818113   \n",
       "std_fit_time                                                             0.006781   \n",
       "mean_score_time                                                          0.129402   \n",
       "std_score_time                                                           0.005557   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__n_estimators                                                       300   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                        0.52381   \n",
       "split2_test_recall                                                       0.333333   \n",
       "mean_test_recall                                                         0.469048   \n",
       "std_test_recall                                                          0.096558   \n",
       "rank_test_recall                                                              168   \n",
       "split0_test_precision                                                      0.6875   \n",
       "split1_test_precision                                                    0.611111   \n",
       "split2_test_precision                                                    0.538462   \n",
       "mean_test_precision                                                      0.612358   \n",
       "std_test_precision                                                       0.060851   \n",
       "rank_test_precision                                                            21   \n",
       "split0_test_f1                                                           0.611111   \n",
       "split1_test_f1                                                           0.564103   \n",
       "split2_test_f1                                                           0.411765   \n",
       "mean_test_f1                                                             0.528993   \n",
       "std_test_f1                                                              0.085085   \n",
       "rank_test_f1                                                                    2   \n",
       "split0_test_roc_auc                                                      0.816886   \n",
       "split1_test_roc_auc                                                      0.860514   \n",
       "split2_test_roc_auc                                                      0.837337   \n",
       "mean_test_roc_auc                                                        0.838246   \n",
       "std_test_roc_auc                                                         0.017823   \n",
       "rank_test_roc_auc                                                              23   \n",
       "\n",
       "                                                                              463  \\\n",
       "mean_fit_time                                                            0.773781   \n",
       "std_fit_time                                                             0.007133   \n",
       "mean_score_time                                                          0.130636   \n",
       "std_score_time                                                           0.002955   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__n_estimators                                                       300   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                       0.619048   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.548413   \n",
       "std_test_recall                                                          0.058332   \n",
       "rank_test_recall                                                               26   \n",
       "split0_test_precision                                                    0.478261   \n",
       "split1_test_precision                                                         0.5   \n",
       "split2_test_precision                                                    0.555556   \n",
       "mean_test_precision                                                      0.511272   \n",
       "std_test_precision                                                       0.032547   \n",
       "rank_test_precision                                                           177   \n",
       "split0_test_f1                                                           0.511628   \n",
       "split1_test_f1                                                           0.553191   \n",
       "split2_test_f1                                                           0.512821   \n",
       "mean_test_f1                                                              0.52588   \n",
       "std_test_f1                                                              0.019318   \n",
       "rank_test_f1                                                                    3   \n",
       "split0_test_roc_auc                                                      0.812061   \n",
       "split1_test_roc_auc                                                      0.824062   \n",
       "split2_test_roc_auc                                                      0.804467   \n",
       "mean_test_roc_auc                                                         0.81353   \n",
       "std_test_roc_auc                                                         0.008067   \n",
       "rank_test_roc_auc                                                             287   \n",
       "\n",
       "                                                                              82   \\\n",
       "mean_fit_time                                                            0.684903   \n",
       "std_fit_time                                                             0.011361   \n",
       "mean_score_time                                                          0.124255   \n",
       "std_score_time                                                           0.001502   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__n_estimators                                                       300   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                            0.6   \n",
       "split1_test_recall                                                       0.666667   \n",
       "split2_test_recall                                                       0.380952   \n",
       "mean_test_recall                                                         0.549206   \n",
       "std_test_recall                                                          0.122047   \n",
       "rank_test_recall                                                               17   \n",
       "split0_test_precision                                                    0.545455   \n",
       "split1_test_precision                                                    0.518519   \n",
       "split2_test_precision                                                    0.470588   \n",
       "mean_test_precision                                                       0.51152   \n",
       "std_test_precision                                                       0.030962   \n",
       "rank_test_precision                                                           176   \n",
       "split0_test_f1                                                           0.571429   \n",
       "split1_test_f1                                                           0.583333   \n",
       "split2_test_f1                                                           0.421053   \n",
       "mean_test_f1                                                             0.525272   \n",
       "std_test_f1                                                              0.073854   \n",
       "rank_test_f1                                                                    4   \n",
       "split0_test_roc_auc                                                      0.809211   \n",
       "split1_test_roc_auc                                                      0.855457   \n",
       "split2_test_roc_auc                                                      0.804046   \n",
       "mean_test_roc_auc                                                        0.822904   \n",
       "std_test_roc_auc                                                         0.023115   \n",
       "rank_test_roc_auc                                                             166   \n",
       "\n",
       "                                                                              224  \\\n",
       "mean_fit_time                                                            0.317713   \n",
       "std_fit_time                                                             0.002402   \n",
       "mean_score_time                                                          0.050639   \n",
       "std_score_time                                                           0.002603   \n",
       "param_clf__class_weight                                        balanced_subsample   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__n_estimators                                                       100   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': 'balanced_subsample', 'c...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                       0.619048   \n",
       "split2_test_recall                                                       0.380952   \n",
       "mean_test_recall                                                              0.5   \n",
       "std_test_recall                                                          0.097202   \n",
       "rank_test_recall                                                               92   \n",
       "split0_test_precision                                                    0.588235   \n",
       "split1_test_precision                                                    0.541667   \n",
       "split2_test_precision                                                    0.571429   \n",
       "mean_test_precision                                                       0.56711   \n",
       "std_test_precision                                                       0.019255   \n",
       "rank_test_precision                                                            69   \n",
       "split0_test_f1                                                           0.540541   \n",
       "split1_test_f1                                                           0.577778   \n",
       "split2_test_f1                                                           0.457143   \n",
       "mean_test_f1                                                             0.525154   \n",
       "std_test_f1                                                              0.050437   \n",
       "rank_test_f1                                                                    5   \n",
       "split0_test_roc_auc                                                      0.819298   \n",
       "split1_test_roc_auc                                                      0.891488   \n",
       "split2_test_roc_auc                                                      0.790771   \n",
       "mean_test_roc_auc                                                        0.833852   \n",
       "std_test_roc_auc                                                         0.042386   \n",
       "rank_test_roc_auc                                                              47   \n",
       "\n",
       "                                                                              376  \\\n",
       "mean_fit_time                                                            1.234373   \n",
       "std_fit_time                                                             0.009563   \n",
       "mean_score_time                                                          0.205502   \n",
       "std_score_time                                                           0.006185   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__n_estimators                                                       500   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                        0.52381   \n",
       "split2_test_recall                                                       0.333333   \n",
       "mean_test_recall                                                         0.469048   \n",
       "std_test_recall                                                          0.096558   \n",
       "rank_test_recall                                                              168   \n",
       "split0_test_precision                                                      0.6875   \n",
       "split1_test_precision                                                     0.52381   \n",
       "split2_test_precision                                                    0.583333   \n",
       "mean_test_precision                                                      0.598214   \n",
       "std_test_precision                                                        0.06765   \n",
       "rank_test_precision                                                            32   \n",
       "split0_test_f1                                                           0.611111   \n",
       "split1_test_f1                                                            0.52381   \n",
       "split2_test_f1                                                           0.424242   \n",
       "mean_test_f1                                                             0.519721   \n",
       "std_test_f1                                                              0.076344   \n",
       "rank_test_f1                                                                    6   \n",
       "split0_test_roc_auc                                                      0.819518   \n",
       "split1_test_roc_auc                                                      0.860303   \n",
       "split2_test_roc_auc                                                      0.835651   \n",
       "mean_test_roc_auc                                                        0.838491   \n",
       "std_test_roc_auc                                                         0.016771   \n",
       "rank_test_roc_auc                                                              21   \n",
       "\n",
       "                                                                              105  \\\n",
       "mean_fit_time                                                            1.251706   \n",
       "std_fit_time                                                             0.005609   \n",
       "mean_score_time                                                          0.190514   \n",
       "std_score_time                                                           0.002869   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__n_estimators                                                       500   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                        0.52381   \n",
       "split2_test_recall                                                       0.333333   \n",
       "mean_test_recall                                                         0.452381   \n",
       "std_test_recall                                                          0.084739   \n",
       "rank_test_recall                                                              244   \n",
       "split0_test_precision                                                    0.714286   \n",
       "split1_test_precision                                                    0.611111   \n",
       "split2_test_precision                                                         0.5   \n",
       "mean_test_precision                                                      0.608466   \n",
       "std_test_precision                                                       0.087502   \n",
       "rank_test_precision                                                            22   \n",
       "split0_test_f1                                                           0.588235   \n",
       "split1_test_f1                                                           0.564103   \n",
       "split2_test_f1                                                                0.4   \n",
       "mean_test_f1                                                             0.517446   \n",
       "std_test_f1                                                              0.083629   \n",
       "rank_test_f1                                                                    7   \n",
       "split0_test_roc_auc                                                      0.783114   \n",
       "split1_test_roc_auc                                                      0.875263   \n",
       "split2_test_roc_auc                                                      0.817741   \n",
       "mean_test_roc_auc                                                        0.825373   \n",
       "std_test_roc_auc                                                         0.038005   \n",
       "rank_test_roc_auc                                                             133   \n",
       "\n",
       "                                                                              92   \\\n",
       "mean_fit_time                                                            1.073898   \n",
       "std_fit_time                                                             0.019837   \n",
       "mean_score_time                                                          0.196611   \n",
       "std_score_time                                                           0.004903   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__n_estimators                                                       500   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                           0.65   \n",
       "split1_test_recall                                                       0.666667   \n",
       "split2_test_recall                                                       0.428571   \n",
       "mean_test_recall                                                         0.581746   \n",
       "std_test_recall                                                          0.108524   \n",
       "rank_test_recall                                                                5   \n",
       "split0_test_precision                                                    0.464286   \n",
       "split1_test_precision                                                    0.538462   \n",
       "split2_test_precision                                                    0.391304   \n",
       "mean_test_precision                                                      0.464684   \n",
       "std_test_precision                                                       0.060077   \n",
       "rank_test_precision                                                           307   \n",
       "split0_test_f1                                                           0.541667   \n",
       "split1_test_f1                                                           0.595745   \n",
       "split2_test_f1                                                           0.409091   \n",
       "mean_test_f1                                                             0.515501   \n",
       "std_test_f1                                                              0.078415   \n",
       "rank_test_f1                                                                    8   \n",
       "split0_test_roc_auc                                                      0.849123   \n",
       "split1_test_roc_auc                                                      0.848083   \n",
       "split2_test_roc_auc                                                      0.786346   \n",
       "mean_test_roc_auc                                                        0.827851   \n",
       "std_test_roc_auc                                                         0.029351   \n",
       "rank_test_roc_auc                                                             102   \n",
       "\n",
       "                                                                              300  \\\n",
       "mean_fit_time                                                            0.927014   \n",
       "std_fit_time                                                             0.005718   \n",
       "mean_score_time                                                          0.130656   \n",
       "std_score_time                                                           0.003433   \n",
       "param_clf__class_weight                                        balanced_subsample   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__n_estimators                                                       300   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': 'balanced_subsample', 'c...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                       0.333333   \n",
       "mean_test_recall                                                         0.419841   \n",
       "std_test_recall                                                          0.062098   \n",
       "rank_test_recall                                                              352   \n",
       "split0_test_precision                                                    0.818182   \n",
       "split1_test_precision                                                    0.588235   \n",
       "split2_test_precision                                                    0.636364   \n",
       "mean_test_precision                                                      0.680927   \n",
       "std_test_precision                                                       0.099023   \n",
       "rank_test_precision                                                             2   \n",
       "split0_test_f1                                                           0.580645   \n",
       "split1_test_f1                                                           0.526316   \n",
       "split2_test_f1                                                             0.4375   \n",
       "mean_test_f1                                                              0.51482   \n",
       "std_test_f1                                                              0.059001   \n",
       "rank_test_f1                                                                    9   \n",
       "split0_test_roc_auc                                                      0.823465   \n",
       "split1_test_roc_auc                                                      0.846186   \n",
       "split2_test_roc_auc                                                      0.805731   \n",
       "mean_test_roc_auc                                                        0.825127   \n",
       "std_test_roc_auc                                                         0.016558   \n",
       "rank_test_roc_auc                                                             138   \n",
       "\n",
       "                                                                              216  \\\n",
       "mean_fit_time                                                            0.299425   \n",
       "std_fit_time                                                             0.003086   \n",
       "mean_score_time                                                          0.048622   \n",
       "std_score_time                                                           0.000148   \n",
       "param_clf__class_weight                                        balanced_subsample   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__n_estimators                                                       100   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': 'balanced_subsample', 'c...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                       0.619048   \n",
       "split2_test_recall                                                       0.285714   \n",
       "mean_test_recall                                                         0.468254   \n",
       "std_test_recall                                                          0.137922   \n",
       "rank_test_recall                                                              183   \n",
       "split0_test_precision                                                    0.714286   \n",
       "split1_test_precision                                                    0.565217   \n",
       "split2_test_precision                                                         0.5   \n",
       "mean_test_precision                                                      0.593168   \n",
       "std_test_precision                                                       0.089687   \n",
       "rank_test_precision                                                            36   \n",
       "split0_test_f1                                                           0.588235   \n",
       "split1_test_f1                                                           0.590909   \n",
       "split2_test_f1                                                           0.363636   \n",
       "mean_test_f1                                                              0.51426   \n",
       "std_test_f1                                                              0.106513   \n",
       "rank_test_f1                                                                   10   \n",
       "split0_test_roc_auc                                                      0.859211   \n",
       "split1_test_roc_auc                                                      0.860514   \n",
       "split2_test_roc_auc                                                      0.771808   \n",
       "mean_test_roc_auc                                                        0.830511   \n",
       "std_test_roc_auc                                                         0.041513   \n",
       "rank_test_roc_auc                                                              72   \n",
       "\n",
       "                                                                              104  \\\n",
       "mean_fit_time                                                            1.153692   \n",
       "std_fit_time                                                             0.006945   \n",
       "mean_score_time                                                          0.187298   \n",
       "std_score_time                                                            0.00333   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__n_estimators                                                       500   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                       0.619048   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.531746   \n",
       "std_test_recall                                                          0.062492   \n",
       "rank_test_recall                                                               40   \n",
       "split0_test_precision                                                    0.526316   \n",
       "split1_test_precision                                                    0.481481   \n",
       "split2_test_precision                                                         0.5   \n",
       "mean_test_precision                                                      0.502599   \n",
       "std_test_precision                                                       0.018396   \n",
       "rank_test_precision                                                           205   \n",
       "split0_test_f1                                                           0.512821   \n",
       "split1_test_f1                                                           0.541667   \n",
       "split2_test_f1                                                           0.487805   \n",
       "mean_test_f1                                                             0.514097   \n",
       "std_test_f1                                                              0.022008   \n",
       "rank_test_f1                                                                   11   \n",
       "split0_test_roc_auc                                                      0.850219   \n",
       "split1_test_roc_auc                                                      0.868732   \n",
       "split2_test_roc_auc                                                       0.76338   \n",
       "mean_test_roc_auc                                                        0.827444   \n",
       "std_test_roc_auc                                                         0.045926   \n",
       "rank_test_roc_auc                                                             106   \n",
       "\n",
       "                                                                              175  \\\n",
       "mean_fit_time                                                            0.310319   \n",
       "std_fit_time                                                             0.000936   \n",
       "mean_score_time                                                          0.049214   \n",
       "std_score_time                                                           0.000454   \n",
       "param_clf__class_weight                                        balanced_subsample   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__n_estimators                                                       100   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': 'balanced_subsample', 'c...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                       0.571429   \n",
       "split2_test_recall                                                       0.333333   \n",
       "mean_test_recall                                                         0.484921   \n",
       "std_test_recall                                                          0.107545   \n",
       "rank_test_recall                                                              125   \n",
       "split0_test_precision                                                    0.611111   \n",
       "split1_test_precision                                                    0.571429   \n",
       "split2_test_precision                                                    0.466667   \n",
       "mean_test_precision                                                      0.549735   \n",
       "std_test_precision                                                       0.060932   \n",
       "rank_test_precision                                                            99   \n",
       "split0_test_f1                                                           0.578947   \n",
       "split1_test_f1                                                           0.571429   \n",
       "split2_test_f1                                                           0.388889   \n",
       "mean_test_f1                                                             0.513088   \n",
       "std_test_f1                                                              0.087876   \n",
       "rank_test_f1                                                                   12   \n",
       "split0_test_roc_auc                                                      0.783772   \n",
       "split1_test_roc_auc                                                      0.845133   \n",
       "split2_test_roc_auc                                                      0.766751   \n",
       "mean_test_roc_auc                                                        0.798552   \n",
       "std_test_roc_auc                                                         0.033663   \n",
       "rank_test_roc_auc                                                             420   \n",
       "\n",
       "                                                                              468  \\\n",
       "mean_fit_time                                                             1.17535   \n",
       "std_fit_time                                                             0.008009   \n",
       "mean_score_time                                                          0.198735   \n",
       "std_score_time                                                           0.000337   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__n_estimators                                                       500   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                       0.619048   \n",
       "split2_test_recall                                                       0.428571   \n",
       "mean_test_recall                                                         0.499206   \n",
       "std_test_recall                                                          0.085191   \n",
       "rank_test_recall                                                              108   \n",
       "split0_test_precision                                                      0.5625   \n",
       "split1_test_precision                                                        0.52   \n",
       "split2_test_precision                                                    0.529412   \n",
       "mean_test_precision                                                      0.537304   \n",
       "std_test_precision                                                       0.018226   \n",
       "rank_test_precision                                                           117   \n",
       "split0_test_f1                                                                0.5   \n",
       "split1_test_f1                                                           0.565217   \n",
       "split2_test_f1                                                           0.473684   \n",
       "mean_test_f1                                                             0.512967   \n",
       "std_test_f1                                                              0.038477   \n",
       "rank_test_f1                                                                   13   \n",
       "split0_test_roc_auc                                                      0.780921   \n",
       "split1_test_roc_auc                                                      0.817952   \n",
       "split2_test_roc_auc                                                      0.800464   \n",
       "mean_test_roc_auc                                                        0.799779   \n",
       "std_test_roc_auc                                                         0.015126   \n",
       "rank_test_roc_auc                                                             412   \n",
       "\n",
       "                                                                              385  \\\n",
       "mean_fit_time                                                            0.269808   \n",
       "std_fit_time                                                             0.005075   \n",
       "mean_score_time                                                          0.050032   \n",
       "std_score_time                                                            0.00201   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__n_estimators                                                       100   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                       0.571429   \n",
       "split2_test_recall                                                       0.333333   \n",
       "mean_test_recall                                                         0.468254   \n",
       "std_test_recall                                                           0.09976   \n",
       "rank_test_recall                                                              183   \n",
       "split0_test_precision                                                    0.555556   \n",
       "split1_test_precision                                                    0.571429   \n",
       "split2_test_precision                                                    0.636364   \n",
       "mean_test_precision                                                      0.587783   \n",
       "std_test_precision                                                       0.034958   \n",
       "rank_test_precision                                                            44   \n",
       "split0_test_f1                                                           0.526316   \n",
       "split1_test_f1                                                           0.571429   \n",
       "split2_test_f1                                                             0.4375   \n",
       "mean_test_f1                                                             0.511748   \n",
       "std_test_f1                                                              0.055638   \n",
       "rank_test_f1                                                                   14   \n",
       "split0_test_roc_auc                                                      0.808114   \n",
       "split1_test_roc_auc                                                      0.854825   \n",
       "split2_test_roc_auc                                                      0.843447   \n",
       "mean_test_roc_auc                                                        0.835462   \n",
       "std_test_roc_auc                                                         0.019888   \n",
       "rank_test_roc_auc                                                              38   \n",
       "\n",
       "                                                                              12   \\\n",
       "mean_fit_time                                                            0.221702   \n",
       "std_fit_time                                                             0.001596   \n",
       "mean_score_time                                                           0.04086   \n",
       "std_score_time                                                           0.000486   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__n_estimators                                                       100   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                        0.52381   \n",
       "split2_test_recall                                                       0.333333   \n",
       "mean_test_recall                                                         0.452381   \n",
       "std_test_recall                                                          0.084739   \n",
       "rank_test_recall                                                              244   \n",
       "split0_test_precision                                                    0.769231   \n",
       "split1_test_precision                                                     0.52381   \n",
       "split2_test_precision                                                         0.5   \n",
       "mean_test_precision                                                       0.59768   \n",
       "std_test_precision                                                       0.121693   \n",
       "rank_test_precision                                                            33   \n",
       "split0_test_f1                                                           0.606061   \n",
       "split1_test_f1                                                            0.52381   \n",
       "split2_test_f1                                                                0.4   \n",
       "mean_test_f1                                                             0.509957   \n",
       "std_test_f1                                                              0.084692   \n",
       "rank_test_f1                                                                   15   \n",
       "split0_test_roc_auc                                                      0.773684   \n",
       "split1_test_roc_auc                                                      0.864307   \n",
       "split2_test_roc_auc                                                        0.7933   \n",
       "mean_test_roc_auc                                                         0.81043   \n",
       "std_test_roc_auc                                                         0.038929   \n",
       "rank_test_roc_auc                                                             319   \n",
       "\n",
       "                                                                              396  \\\n",
       "mean_fit_time                                                            0.730845   \n",
       "std_fit_time                                                             0.003401   \n",
       "mean_score_time                                                          0.131854   \n",
       "std_score_time                                                            0.00514   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__n_estimators                                                       300   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                       0.571429   \n",
       "split2_test_recall                                                       0.380952   \n",
       "mean_test_recall                                                          0.46746   \n",
       "std_test_recall                                                          0.078736   \n",
       "rank_test_recall                                                              209   \n",
       "split0_test_precision                                                      0.5625   \n",
       "split1_test_precision                                                    0.545455   \n",
       "split2_test_precision                                                    0.615385   \n",
       "mean_test_precision                                                      0.574446   \n",
       "std_test_precision                                                       0.029772   \n",
       "rank_test_precision                                                            58   \n",
       "split0_test_f1                                                                0.5   \n",
       "split1_test_f1                                                            0.55814   \n",
       "split2_test_f1                                                           0.470588   \n",
       "mean_test_f1                                                             0.509576   \n",
       "std_test_f1                                                              0.036378   \n",
       "rank_test_f1                                                                   16   \n",
       "split0_test_roc_auc                                                      0.777193   \n",
       "split1_test_roc_auc                                                      0.887063   \n",
       "split2_test_roc_auc                                                      0.809313   \n",
       "mean_test_roc_auc                                                        0.824523   \n",
       "std_test_roc_auc                                                         0.046126   \n",
       "rank_test_roc_auc                                                             147   \n",
       "\n",
       "                                                                              169  \\\n",
       "mean_fit_time                                                            0.303432   \n",
       "std_fit_time                                                              0.00162   \n",
       "mean_score_time                                                          0.048707   \n",
       "std_score_time                                                           0.000659   \n",
       "param_clf__class_weight                                        balanced_subsample   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__n_estimators                                                       100   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': 'balanced_subsample', 'c...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                        0.52381   \n",
       "split2_test_recall                                                       0.333333   \n",
       "mean_test_recall                                                         0.469048   \n",
       "std_test_recall                                                          0.096558   \n",
       "rank_test_recall                                                              168   \n",
       "split0_test_precision                                                    0.611111   \n",
       "split1_test_precision                                                         0.5   \n",
       "split2_test_precision                                                    0.636364   \n",
       "mean_test_precision                                                      0.582492   \n",
       "std_test_precision                                                       0.059234   \n",
       "rank_test_precision                                                            49   \n",
       "split0_test_f1                                                           0.578947   \n",
       "split1_test_f1                                                           0.511628   \n",
       "split2_test_f1                                                             0.4375   \n",
       "mean_test_f1                                                             0.509358   \n",
       "std_test_f1                                                              0.057768   \n",
       "rank_test_f1                                                                   17   \n",
       "split0_test_roc_auc                                                      0.761623   \n",
       "split1_test_roc_auc                                                       0.86515   \n",
       "split2_test_roc_auc                                                      0.793089   \n",
       "mean_test_roc_auc                                                         0.80662   \n",
       "std_test_roc_auc                                                         0.043334   \n",
       "rank_test_roc_auc                                                             360   \n",
       "\n",
       "                                ...  \\\n",
       "mean_fit_time                   ...   \n",
       "std_fit_time                    ...   \n",
       "mean_score_time                 ...   \n",
       "std_score_time                  ...   \n",
       "param_clf__class_weight         ...   \n",
       "param_clf__criterion            ...   \n",
       "param_clf__n_estimators         ...   \n",
       "param_smt__k_neighbors          ...   \n",
       "param_smt__sampling_strategy    ...   \n",
       "param_under__sampling_strategy  ...   \n",
       "params                          ...   \n",
       "split0_test_recall              ...   \n",
       "split1_test_recall              ...   \n",
       "split2_test_recall              ...   \n",
       "mean_test_recall                ...   \n",
       "std_test_recall                 ...   \n",
       "rank_test_recall                ...   \n",
       "split0_test_precision           ...   \n",
       "split1_test_precision           ...   \n",
       "split2_test_precision           ...   \n",
       "mean_test_precision             ...   \n",
       "std_test_precision              ...   \n",
       "rank_test_precision             ...   \n",
       "split0_test_f1                  ...   \n",
       "split1_test_f1                  ...   \n",
       "split2_test_f1                  ...   \n",
       "mean_test_f1                    ...   \n",
       "std_test_f1                     ...   \n",
       "rank_test_f1                    ...   \n",
       "split0_test_roc_auc             ...   \n",
       "split1_test_roc_auc             ...   \n",
       "split2_test_roc_auc             ...   \n",
       "mean_test_roc_auc               ...   \n",
       "std_test_roc_auc                ...   \n",
       "rank_test_roc_auc               ...   \n",
       "\n",
       "                                                                              295  \\\n",
       "mean_fit_time                                                            0.949684   \n",
       "std_fit_time                                                             0.008231   \n",
       "mean_score_time                                                          0.132821   \n",
       "std_score_time                                                           0.003062   \n",
       "param_clf__class_weight                                        balanced_subsample   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__n_estimators                                                       300   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': 'balanced_subsample', 'c...   \n",
       "split0_test_recall                                                           0.35   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                       0.333333   \n",
       "mean_test_recall                                                         0.370635   \n",
       "std_test_recall                                                          0.041528   \n",
       "rank_test_recall                                                              464   \n",
       "split0_test_precision                                                    0.411765   \n",
       "split1_test_precision                                                        0.45   \n",
       "split2_test_precision                                                    0.538462   \n",
       "mean_test_precision                                                      0.466742   \n",
       "std_test_precision                                                       0.053061   \n",
       "rank_test_precision                                                           301   \n",
       "split0_test_f1                                                           0.378378   \n",
       "split1_test_f1                                                           0.439024   \n",
       "split2_test_f1                                                           0.411765   \n",
       "mean_test_f1                                                             0.409722   \n",
       "std_test_f1                                                              0.024801   \n",
       "rank_test_f1                                                                  470   \n",
       "split0_test_roc_auc                                                      0.743202   \n",
       "split1_test_roc_auc                                                       0.86831   \n",
       "split2_test_roc_auc                                                       0.82638   \n",
       "mean_test_roc_auc                                                        0.812631   \n",
       "std_test_roc_auc                                                         0.051992   \n",
       "rank_test_roc_auc                                                             298   \n",
       "\n",
       "                                                                              344  \\\n",
       "mean_fit_time                                                            0.702515   \n",
       "std_fit_time                                                             0.002932   \n",
       "mean_score_time                                                          0.132466   \n",
       "std_score_time                                                           0.002179   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__n_estimators                                                       300   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                            0.6   \n",
       "split1_test_recall                                                       0.571429   \n",
       "split2_test_recall                                                       0.333333   \n",
       "mean_test_recall                                                         0.501587   \n",
       "std_test_recall                                                          0.119544   \n",
       "rank_test_recall                                                               76   \n",
       "split0_test_precision                                                    0.342857   \n",
       "split1_test_precision                                                    0.363636   \n",
       "split2_test_precision                                                        0.35   \n",
       "mean_test_precision                                                      0.352165   \n",
       "std_test_precision                                                        0.00862   \n",
       "rank_test_precision                                                           483   \n",
       "split0_test_f1                                                           0.436364   \n",
       "split1_test_f1                                                           0.444444   \n",
       "split2_test_f1                                                           0.341463   \n",
       "mean_test_f1                                                             0.407424   \n",
       "std_test_f1                                                              0.046758   \n",
       "rank_test_f1                                                                  471   \n",
       "split0_test_roc_auc                                                      0.777193   \n",
       "split1_test_roc_auc                                                      0.827855   \n",
       "split2_test_roc_auc                                                      0.761905   \n",
       "mean_test_roc_auc                                                        0.788984   \n",
       "std_test_roc_auc                                                         0.028186   \n",
       "rank_test_roc_auc                                                             460   \n",
       "\n",
       "                                                                              237  \\\n",
       "mean_fit_time                                                            0.898744   \n",
       "std_fit_time                                                             0.015098   \n",
       "mean_score_time                                                          0.124929   \n",
       "std_score_time                                                           0.000686   \n",
       "param_clf__class_weight                                        balanced_subsample   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__n_estimators                                                       300   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': 'balanced_subsample', 'c...   \n",
       "split0_test_recall                                                            0.4   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                       0.190476   \n",
       "mean_test_recall                                                         0.355556   \n",
       "std_test_recall                                                          0.120802   \n",
       "rank_test_recall                                                              470   \n",
       "split0_test_precision                                                    0.533333   \n",
       "split1_test_precision                                                         0.5   \n",
       "split2_test_precision                                                         0.5   \n",
       "mean_test_precision                                                      0.511111   \n",
       "std_test_precision                                                       0.015713   \n",
       "rank_test_precision                                                           179   \n",
       "split0_test_f1                                                           0.457143   \n",
       "split1_test_f1                                                           0.487805   \n",
       "split2_test_f1                                                           0.275862   \n",
       "mean_test_f1                                                             0.406937   \n",
       "std_test_f1                                                              0.093525   \n",
       "rank_test_f1                                                                  472   \n",
       "split0_test_roc_auc                                                      0.800877   \n",
       "split1_test_roc_auc                                                      0.844711   \n",
       "split2_test_roc_auc                                                      0.822166   \n",
       "mean_test_roc_auc                                                        0.822585   \n",
       "std_test_roc_auc                                                         0.017898   \n",
       "rank_test_roc_auc                                                             170   \n",
       "\n",
       "                                                                              263  \\\n",
       "mean_fit_time                                                            1.416587   \n",
       "std_fit_time                                                             0.004891   \n",
       "mean_score_time                                                          0.210636   \n",
       "std_score_time                                                           0.001729   \n",
       "param_clf__class_weight                                        balanced_subsample   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__n_estimators                                                       500   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': 'balanced_subsample', 'c...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                       0.714286   \n",
       "split2_test_recall                                                       0.333333   \n",
       "mean_test_recall                                                         0.515873   \n",
       "std_test_recall                                                          0.155928   \n",
       "rank_test_recall                                                               66   \n",
       "split0_test_precision                                                    0.322581   \n",
       "split1_test_precision                                                    0.384615   \n",
       "split2_test_precision                                                    0.304348   \n",
       "mean_test_precision                                                      0.337181   \n",
       "std_test_precision                                                       0.034357   \n",
       "rank_test_precision                                                           486   \n",
       "split0_test_f1                                                           0.392157   \n",
       "split1_test_f1                                                                0.5   \n",
       "split2_test_f1                                                           0.318182   \n",
       "mean_test_f1                                                             0.403446   \n",
       "std_test_f1                                                              0.074655   \n",
       "rank_test_f1                                                                  473   \n",
       "split0_test_roc_auc                                                       0.79386   \n",
       "split1_test_roc_auc                                                      0.841551   \n",
       "split2_test_roc_auc                                                      0.788453   \n",
       "mean_test_roc_auc                                                        0.807955   \n",
       "std_test_roc_auc                                                         0.023858   \n",
       "rank_test_roc_auc                                                             337   \n",
       "\n",
       "                                                                              136  \\\n",
       "mean_fit_time                                                            0.683938   \n",
       "std_fit_time                                                             0.000443   \n",
       "mean_score_time                                                          0.120998   \n",
       "std_score_time                                                           0.000975   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__n_estimators                                                       300   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                       0.571429   \n",
       "split2_test_recall                                                       0.285714   \n",
       "mean_test_recall                                                         0.435714   \n",
       "std_test_recall                                                          0.117079   \n",
       "rank_test_recall                                                              330   \n",
       "split0_test_precision                                                    0.428571   \n",
       "split1_test_precision                                                    0.352941   \n",
       "split2_test_precision                                                         0.4   \n",
       "mean_test_precision                                                      0.393838   \n",
       "std_test_precision                                                       0.031182   \n",
       "rank_test_precision                                                           443   \n",
       "split0_test_f1                                                           0.439024   \n",
       "split1_test_f1                                                           0.436364   \n",
       "split2_test_f1                                                           0.333333   \n",
       "mean_test_f1                                                             0.402907   \n",
       "std_test_f1                                                              0.049208   \n",
       "rank_test_f1                                                                  474   \n",
       "split0_test_roc_auc                                                      0.792763   \n",
       "split1_test_roc_auc                                                      0.796249   \n",
       "split2_test_roc_auc                                                      0.761905   \n",
       "mean_test_roc_auc                                                        0.783639   \n",
       "std_test_roc_auc                                                         0.015434   \n",
       "rank_test_roc_auc                                                             471   \n",
       "\n",
       "                                                                              134  \\\n",
       "mean_fit_time                                                            0.742342   \n",
       "std_fit_time                                                             0.000499   \n",
       "mean_score_time                                                          0.121703   \n",
       "std_score_time                                                           0.000913   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__n_estimators                                                       300   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                            0.4   \n",
       "split1_test_recall                                                        0.52381   \n",
       "split2_test_recall                                                       0.333333   \n",
       "mean_test_recall                                                         0.419048   \n",
       "std_test_recall                                                          0.078919   \n",
       "rank_test_recall                                                              375   \n",
       "split0_test_precision                                                    0.363636   \n",
       "split1_test_precision                                                    0.407407   \n",
       "split2_test_precision                                                    0.411765   \n",
       "mean_test_precision                                                      0.394269   \n",
       "std_test_precision                                                       0.021734   \n",
       "rank_test_precision                                                           441   \n",
       "split0_test_f1                                                           0.380952   \n",
       "split1_test_f1                                                           0.458333   \n",
       "split2_test_f1                                                           0.368421   \n",
       "mean_test_f1                                                             0.402569   \n",
       "std_test_f1                                                              0.039762   \n",
       "rank_test_f1                                                                  475   \n",
       "split0_test_roc_auc                                                       0.77193   \n",
       "split1_test_roc_auc                                                      0.851665   \n",
       "split2_test_roc_auc                                                      0.806153   \n",
       "mean_test_roc_auc                                                        0.809916   \n",
       "std_test_roc_auc                                                          0.03266   \n",
       "rank_test_roc_auc                                                             321   \n",
       "\n",
       "                                                                              191  \\\n",
       "mean_fit_time                                                            0.825548   \n",
       "std_fit_time                                                             0.003674   \n",
       "mean_score_time                                                          0.126653   \n",
       "std_score_time                                                            0.00376   \n",
       "param_clf__class_weight                                        balanced_subsample   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__n_estimators                                                       300   \n",
       "param_smt__k_neighbors                                                          5   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': 'balanced_subsample', 'c...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                        0.52381   \n",
       "split2_test_recall                                                       0.380952   \n",
       "mean_test_recall                                                         0.484921   \n",
       "std_test_recall                                                           0.07429   \n",
       "rank_test_recall                                                              116   \n",
       "split0_test_precision                                                       0.275   \n",
       "split1_test_precision                                                     0.37931   \n",
       "split2_test_precision                                                    0.421053   \n",
       "mean_test_precision                                                      0.358454   \n",
       "std_test_precision                                                       0.061422   \n",
       "rank_test_precision                                                           480   \n",
       "split0_test_f1                                                           0.366667   \n",
       "split1_test_f1                                                               0.44   \n",
       "split2_test_f1                                                                0.4   \n",
       "mean_test_f1                                                             0.402222   \n",
       "std_test_f1                                                              0.029979   \n",
       "rank_test_f1                                                                  476   \n",
       "split0_test_roc_auc                                                      0.730044   \n",
       "split1_test_roc_auc                                                      0.825116   \n",
       "split2_test_roc_auc                                                      0.762115   \n",
       "mean_test_roc_auc                                                        0.772425   \n",
       "std_test_roc_auc                                                         0.039492   \n",
       "rank_test_roc_auc                                                             484   \n",
       "\n",
       "                                                                              311  \\\n",
       "mean_fit_time                                                            1.498386   \n",
       "std_fit_time                                                              0.01603   \n",
       "mean_score_time                                                          0.206228   \n",
       "std_score_time                                                           0.000496   \n",
       "param_clf__class_weight                                        balanced_subsample   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__n_estimators                                                       500   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': 'balanced_subsample', 'c...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                        0.52381   \n",
       "split2_test_recall                                                       0.285714   \n",
       "mean_test_recall                                                         0.453175   \n",
       "std_test_recall                                                          0.118894   \n",
       "rank_test_recall                                                              224   \n",
       "split0_test_precision                                                     0.34375   \n",
       "split1_test_precision                                                    0.392857   \n",
       "split2_test_precision                                                         0.4   \n",
       "mean_test_precision                                                      0.378869   \n",
       "std_test_precision                                                       0.025004   \n",
       "rank_test_precision                                                           463   \n",
       "split0_test_f1                                                           0.423077   \n",
       "split1_test_f1                                                            0.44898   \n",
       "split2_test_f1                                                           0.333333   \n",
       "mean_test_f1                                                             0.401797   \n",
       "std_test_f1                                                              0.049552   \n",
       "rank_test_f1                                                                  477   \n",
       "split0_test_roc_auc                                                      0.766667   \n",
       "split1_test_roc_auc                                                      0.861568   \n",
       "split2_test_roc_auc                                                      0.803624   \n",
       "mean_test_roc_auc                                                        0.810619   \n",
       "std_test_roc_auc                                                         0.039058   \n",
       "rank_test_roc_auc                                                             315   \n",
       "\n",
       "                                                                              74   \\\n",
       "mean_fit_time                                                            0.680617   \n",
       "std_fit_time                                                             0.009003   \n",
       "mean_score_time                                                           0.12314   \n",
       "std_score_time                                                           0.002828   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__n_estimators                                                       300   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                       0.571429   \n",
       "split2_test_recall                                                       0.285714   \n",
       "mean_test_recall                                                         0.469048   \n",
       "std_test_recall                                                          0.129931   \n",
       "rank_test_recall                                                              177   \n",
       "split0_test_precision                                                    0.392857   \n",
       "split1_test_precision                                                    0.352941   \n",
       "split2_test_precision                                                    0.333333   \n",
       "mean_test_precision                                                      0.359711   \n",
       "std_test_precision                                                       0.024767   \n",
       "rank_test_precision                                                           477   \n",
       "split0_test_f1                                                           0.458333   \n",
       "split1_test_f1                                                           0.436364   \n",
       "split2_test_f1                                                           0.307692   \n",
       "mean_test_f1                                                             0.400796   \n",
       "std_test_f1                                                              0.066443   \n",
       "rank_test_f1                                                                  478   \n",
       "split0_test_roc_auc                                                      0.790789   \n",
       "split1_test_roc_auc                                                       0.82322   \n",
       "split2_test_roc_auc                                                      0.767804   \n",
       "mean_test_roc_auc                                                        0.793938   \n",
       "std_test_roc_auc                                                         0.022732   \n",
       "rank_test_roc_auc                                                             442   \n",
       "\n",
       "                                                                              146  \\\n",
       "mean_fit_time                                                             1.11247   \n",
       "std_fit_time                                                             0.012754   \n",
       "mean_score_time                                                           0.19264   \n",
       "std_score_time                                                             0.0014   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__n_estimators                                                       500   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                       0.571429   \n",
       "split2_test_recall                                                       0.380952   \n",
       "mean_test_recall                                                          0.46746   \n",
       "std_test_recall                                                          0.078736   \n",
       "rank_test_recall                                                              209   \n",
       "split0_test_precision                                                     0.28125   \n",
       "split1_test_precision                                                    0.428571   \n",
       "split2_test_precision                                                    0.347826   \n",
       "mean_test_precision                                                      0.352549   \n",
       "std_test_precision                                                       0.060236   \n",
       "rank_test_precision                                                           482   \n",
       "split0_test_f1                                                           0.346154   \n",
       "split1_test_f1                                                           0.489796   \n",
       "split2_test_f1                                                           0.363636   \n",
       "mean_test_f1                                                             0.399862   \n",
       "std_test_f1                                                              0.063992   \n",
       "rank_test_f1                                                                  479   \n",
       "split0_test_roc_auc                                                      0.745395   \n",
       "split1_test_roc_auc                                                      0.873788   \n",
       "split2_test_roc_auc                                                      0.799831   \n",
       "mean_test_roc_auc                                                        0.806338   \n",
       "std_test_roc_auc                                                         0.052618   \n",
       "rank_test_roc_auc                                                             362   \n",
       "\n",
       "                                                                              384  \\\n",
       "mean_fit_time                                                            0.279356   \n",
       "std_fit_time                                                             0.006316   \n",
       "mean_score_time                                                          0.049579   \n",
       "std_score_time                                                           0.002818   \n",
       "param_clf__class_weight                                                      None   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__n_estimators                                                       100   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.4   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': None, 'clf__criterion': ...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                       0.285714   \n",
       "split2_test_recall                                                       0.285714   \n",
       "mean_test_recall                                                         0.340476   \n",
       "std_test_recall                                                          0.077445   \n",
       "rank_test_recall                                                              479   \n",
       "split0_test_precision                                                      0.5625   \n",
       "split1_test_precision                                                       0.375   \n",
       "split2_test_precision                                                    0.545455   \n",
       "mean_test_precision                                                      0.494318   \n",
       "std_test_precision                                                       0.084657   \n",
       "rank_test_precision                                                           227   \n",
       "split0_test_f1                                                                0.5   \n",
       "split1_test_f1                                                           0.324324   \n",
       "split2_test_f1                                                              0.375   \n",
       "mean_test_f1                                                             0.399775   \n",
       "std_test_f1                                                              0.073828   \n",
       "rank_test_f1                                                                  480   \n",
       "split0_test_roc_auc                                                      0.739912   \n",
       "split1_test_roc_auc                                                      0.836072   \n",
       "split2_test_roc_auc                                                      0.827434   \n",
       "mean_test_roc_auc                                                        0.801139   \n",
       "std_test_roc_auc                                                         0.043438   \n",
       "rank_test_roc_auc                                                             404   \n",
       "\n",
       "                                                                              3    \\\n",
       "mean_fit_time                                                            0.208524   \n",
       "std_fit_time                                                             0.002627   \n",
       "mean_score_time                                                          0.037107   \n",
       "std_score_time                                                            0.00125   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__n_estimators                                                       100   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                            0.4   \n",
       "split1_test_recall                                                       0.285714   \n",
       "split2_test_recall                                                       0.285714   \n",
       "mean_test_recall                                                          0.32381   \n",
       "std_test_recall                                                          0.053875   \n",
       "rank_test_recall                                                              483   \n",
       "split0_test_precision                                                    0.571429   \n",
       "split1_test_precision                                                         0.5   \n",
       "split2_test_precision                                                         0.5   \n",
       "mean_test_precision                                                       0.52381   \n",
       "std_test_precision                                                       0.033672   \n",
       "rank_test_precision                                                           146   \n",
       "split0_test_f1                                                           0.470588   \n",
       "split1_test_f1                                                           0.363636   \n",
       "split2_test_f1                                                           0.363636   \n",
       "mean_test_f1                                                             0.399287   \n",
       "std_test_f1                                                              0.050418   \n",
       "rank_test_f1                                                                  481   \n",
       "split0_test_roc_auc                                                      0.773465   \n",
       "split1_test_roc_auc                                                      0.831226   \n",
       "split2_test_roc_auc                                                      0.812895   \n",
       "mean_test_roc_auc                                                        0.805862   \n",
       "std_test_roc_auc                                                           0.0241   \n",
       "rank_test_roc_auc                                                             367   \n",
       "\n",
       "                                                                              165  \\\n",
       "mean_fit_time                                                            0.312586   \n",
       "std_fit_time                                                             0.014267   \n",
       "mean_score_time                                                          0.047235   \n",
       "std_score_time                                                           0.000818   \n",
       "param_clf__class_weight                                        balanced_subsample   \n",
       "param_clf__criterion                                                         gini   \n",
       "param_clf__n_estimators                                                       100   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': 'balanced_subsample', 'c...   \n",
       "split0_test_recall                                                           0.25   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                       0.285714   \n",
       "mean_test_recall                                                         0.337302   \n",
       "std_test_recall                                                          0.099286   \n",
       "rank_test_recall                                                              482   \n",
       "split0_test_precision                                                    0.454545   \n",
       "split1_test_precision                                                    0.526316   \n",
       "split2_test_precision                                                    0.545455   \n",
       "mean_test_precision                                                      0.508772   \n",
       "std_test_precision                                                       0.039132   \n",
       "rank_test_precision                                                           187   \n",
       "split0_test_f1                                                           0.322581   \n",
       "split1_test_f1                                                                0.5   \n",
       "split2_test_f1                                                              0.375   \n",
       "mean_test_f1                                                             0.399194   \n",
       "std_test_f1                                                              0.074424   \n",
       "rank_test_f1                                                                  482   \n",
       "split0_test_roc_auc                                                       0.79057   \n",
       "split1_test_roc_auc                                                        0.8504   \n",
       "split2_test_roc_auc                                                      0.816266   \n",
       "mean_test_roc_auc                                                        0.819079   \n",
       "std_test_roc_auc                                                         0.024506   \n",
       "rank_test_roc_auc                                                             217   \n",
       "\n",
       "                                                                              272  \\\n",
       "mean_fit_time                                                            0.289324   \n",
       "std_fit_time                                                             0.004533   \n",
       "mean_score_time                                                          0.048287   \n",
       "std_score_time                                                           0.000101   \n",
       "param_clf__class_weight                                        balanced_subsample   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__n_estimators                                                       100   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': 'balanced_subsample', 'c...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                       0.571429   \n",
       "split2_test_recall                                                       0.333333   \n",
       "mean_test_recall                                                         0.451587   \n",
       "std_test_recall                                                          0.097208   \n",
       "rank_test_recall                                                              267   \n",
       "split0_test_precision                                                        0.36   \n",
       "split1_test_precision                                                         0.4   \n",
       "split2_test_precision                                                    0.318182   \n",
       "mean_test_precision                                                      0.359394   \n",
       "std_test_precision                                                       0.033405   \n",
       "rank_test_precision                                                           478   \n",
       "split0_test_f1                                                                0.4   \n",
       "split1_test_f1                                                           0.470588   \n",
       "split2_test_f1                                                           0.325581   \n",
       "mean_test_f1                                                             0.398723   \n",
       "std_test_f1                                                              0.059206   \n",
       "rank_test_f1                                                                  483   \n",
       "split0_test_roc_auc                                                      0.736842   \n",
       "split1_test_roc_auc                                                      0.769701   \n",
       "split2_test_roc_auc                                                       0.76043   \n",
       "mean_test_roc_auc                                                        0.755658   \n",
       "std_test_roc_auc                                                         0.013832   \n",
       "rank_test_roc_auc                                                             486   \n",
       "\n",
       "                                                                              75   \\\n",
       "mean_fit_time                                                            0.734914   \n",
       "std_fit_time                                                              0.00434   \n",
       "mean_score_time                                                          0.120059   \n",
       "std_score_time                                                           0.002434   \n",
       "param_clf__class_weight                                                  balanced   \n",
       "param_clf__criterion                                                      entropy   \n",
       "param_clf__n_estimators                                                       300   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.3   \n",
       "param_under__sampling_strategy                                                0.5   \n",
       "params                          {'clf__class_weight': 'balanced', 'clf__criter...   \n",
       "split0_test_recall                                                            0.4   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                       0.238095   \n",
       "mean_test_recall                                                         0.355556   \n",
       "std_test_recall                                                          0.083872   \n",
       "rank_test_recall                                                              470   \n",
       "split0_test_precision                                                    0.444444   \n",
       "split1_test_precision                                                    0.409091   \n",
       "split2_test_precision                                                       0.625   \n",
       "mean_test_precision                                                      0.492845   \n",
       "std_test_precision                                                       0.094556   \n",
       "rank_test_precision                                                           233   \n",
       "split0_test_f1                                                           0.421053   \n",
       "split1_test_f1                                                           0.418605   \n",
       "split2_test_f1                                                           0.344828   \n",
       "mean_test_f1                                                             0.394828   \n",
       "std_test_f1                                                               0.03537   \n",
       "rank_test_f1                                                                  484   \n",
       "split0_test_roc_auc                                                      0.773904   \n",
       "split1_test_roc_auc                                                      0.861778   \n",
       "split2_test_roc_auc                                                       0.80236   \n",
       "mean_test_roc_auc                                                        0.812681   \n",
       "std_test_roc_auc                                                          0.03661   \n",
       "rank_test_roc_auc                                                             297   \n",
       "\n",
       "                                                                              290  \\\n",
       "mean_fit_time                                                            0.825505   \n",
       "std_fit_time                                                             0.006375   \n",
       "mean_score_time                                                          0.126528   \n",
       "std_score_time                                                           0.002071   \n",
       "param_clf__class_weight                                        balanced_subsample   \n",
       "param_clf__criterion                                                     log_loss   \n",
       "param_clf__n_estimators                                                       300   \n",
       "param_smt__k_neighbors                                                          3   \n",
       "param_smt__sampling_strategy                                                  0.2   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__class_weight': 'balanced_subsample', 'c...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                       0.619048   \n",
       "split2_test_recall                                                       0.380952   \n",
       "mean_test_recall                                                         0.483333   \n",
       "std_test_recall                                                          0.100019   \n",
       "rank_test_recall                                                              159   \n",
       "split0_test_precision                                                         0.3   \n",
       "split1_test_precision                                                    0.342105   \n",
       "split2_test_precision                                                    0.380952   \n",
       "mean_test_precision                                                      0.341019   \n",
       "std_test_precision                                                       0.033058   \n",
       "rank_test_precision                                                           485   \n",
       "split0_test_f1                                                               0.36   \n",
       "split1_test_f1                                                           0.440678   \n",
       "split2_test_f1                                                           0.380952   \n",
       "mean_test_f1                                                             0.393877   \n",
       "std_test_f1                                                              0.034181   \n",
       "rank_test_f1                                                                  485   \n",
       "split0_test_roc_auc                                                      0.775658   \n",
       "split1_test_roc_auc                                                      0.803413   \n",
       "split2_test_roc_auc                                                      0.780236   \n",
       "mean_test_roc_auc                                                        0.786436   \n",
       "std_test_roc_auc                                                          0.01215   \n",
       "rank_test_roc_auc                                                             464   \n",
       "\n",
       "                                                                              219  \n",
       "mean_fit_time                                                             0.31888  \n",
       "std_fit_time                                                             0.006258  \n",
       "mean_score_time                                                          0.050307  \n",
       "std_score_time                                                           0.000442  \n",
       "param_clf__class_weight                                        balanced_subsample  \n",
       "param_clf__criterion                                                      entropy  \n",
       "param_clf__n_estimators                                                       100  \n",
       "param_smt__k_neighbors                                                          3  \n",
       "param_smt__sampling_strategy                                                  0.3  \n",
       "param_under__sampling_strategy                                                0.5  \n",
       "params                          {'clf__class_weight': 'balanced_subsample', 'c...  \n",
       "split0_test_recall                                                            0.4  \n",
       "split1_test_recall                                                       0.238095  \n",
       "split2_test_recall                                                       0.333333  \n",
       "mean_test_recall                                                          0.32381  \n",
       "std_test_recall                                                           0.06644  \n",
       "rank_test_recall                                                              484  \n",
       "split0_test_precision                                                    0.571429  \n",
       "split1_test_precision                                                    0.384615  \n",
       "split2_test_precision                                                    0.411765  \n",
       "mean_test_precision                                                      0.455936  \n",
       "std_test_precision                                                       0.082414  \n",
       "rank_test_precision                                                           327  \n",
       "split0_test_f1                                                           0.470588  \n",
       "split1_test_f1                                                           0.294118  \n",
       "split2_test_f1                                                           0.368421  \n",
       "mean_test_f1                                                             0.377709  \n",
       "std_test_f1                                                              0.072343  \n",
       "rank_test_f1                                                                  486  \n",
       "split0_test_roc_auc                                                      0.819079  \n",
       "split1_test_roc_auc                                                      0.849136  \n",
       "split2_test_roc_auc                                                      0.818584  \n",
       "mean_test_roc_auc                                                        0.828933  \n",
       "std_test_roc_auc                                                         0.014287  \n",
       "rank_test_roc_auc                                                              88  \n",
       "\n",
       "[35 rows x 486 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Random forest with grid search for parameters, testing on 5-fold CV with shuffling\n",
    "\n",
    "pipeline = IMBLpipeline([\n",
    "   ('smt', SMOTE()), \n",
    "   ('under', RandomUnderSampler()), \n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "              'clf__n_estimators': [100,300,500],\n",
    "              'clf__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "              'clf__class_weight': ['balanced', 'balanced_subsample', None],\n",
    "              'smt__sampling_strategy': [ 0.2, 0.3, 0.4],\n",
    "              'under__sampling_strategy': [0.5, 0.6, 0.7],\n",
    "              'smt__k_neighbors': [3, 5]\n",
    "             }\n",
    "\n",
    "nfolds = 3\n",
    "scores = ['recall', 'precision', 'f1', 'roc_auc']\n",
    "refit_score = 'f1'\n",
    "gscv_rf = GridSearchCV(pipeline, \n",
    "                    parameters, \n",
    "                    scoring=scores,\n",
    "                    cv=StratifiedKFold(n_splits=nfolds, shuffle=True),\n",
    "                    return_train_score=False, \n",
    "                    verbose=1, \n",
    "                    refit=refit_score,\n",
    "                    n_jobs=-1)\n",
    "resultsGSCV = gscv_rf.fit(X, y)\n",
    "\n",
    "# Get the classifier name from the pipeline\n",
    "classifier_name = resultsGSCV.best_estimator_.named_steps['clf'].__class__.__name__\n",
    "    \n",
    "# save the best estimator\n",
    "save_best_estimator(gscv_rf, classifier_name, notebook_name)\n",
    "\n",
    "results = pd.DataFrame(resultsGSCV.cv_results_)\n",
    "display(results.sort_values(by=[f'rank_test_f1']).transpose())\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b6c5dc6-c5f1-4118-9af1-da0c03851db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.91       147\n",
      "           1       0.48      0.62      0.54        26\n",
      "\n",
      "    accuracy                           0.84       173\n",
      "   macro avg       0.71      0.75      0.72       173\n",
      "weighted avg       0.86      0.84      0.85       173\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation RF on test set\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = gscv_rf.best_estimator_.steps[2][1].predict(X_eval)\n",
    "RF_classification_report = classification_report(y_eval, y_pred)\n",
    "\n",
    "# Create classification report as dictionary\n",
    "RF_report_dict = classification_report(y_eval, y_pred, output_dict=True)\n",
    "\n",
    "print(classification_report(y_eval, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f416e105-b46a-497f-8347-aa1f623f305f",
   "metadata": {},
   "source": [
    "Plot the mean ROC curve of the algorithm with best performing parameter selection. We will perform CV once again and plot the ROC curve for each fold and compute and plot the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d97807f2-1c2c-404d-9ded-0b4e3a782ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Run classifier with cross-validation and plot ROC curves\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "classifier = resultsGSCV.best_estimator_\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "for i, (train, test) in enumerate(cv.split(X_eval, y_eval)):\n",
    "    classifier.fit(X_eval.iloc[train], y_eval[train])\n",
    "    viz = RocCurveDisplay.from_estimator(\n",
    "        classifier,\n",
    "        X_eval.iloc[test],\n",
    "        y_eval[test],\n",
    "        name=\"fold {}\".format(i),\n",
    "        alpha=0.3,\n",
    "        lw=1,\n",
    "        ax=ax,\n",
    "    )\n",
    "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(viz.roc_auc)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Baseline (random prediction)\", alpha=0.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(\n",
    "    mean_fpr,\n",
    "    mean_tpr,\n",
    "    color=\"b\",\n",
    "    label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n",
    "    lw=2,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax.fill_between(\n",
    "    mean_fpr,\n",
    "    tprs_lower,\n",
    "    tprs_upper,\n",
    "    color=\"grey\",\n",
    "    alpha=0.2,\n",
    "    label=r\"$\\pm$ 1 std. dev.\",\n",
    ")\n",
    "\n",
    "clfname = [str(step[1].__class__.__name__) for step in classifier.steps if step[0]=='clf'][0]\n",
    "ax.set(\n",
    "    xlim=[-0.05, 1.05],\n",
    "    ylim=[-0.05, 1.05],\n",
    "    title=f'{clfname} evaluation (ROC-AUC, {nfolds}-fold CV)',\n",
    ")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ab2296-3bf0-49c5-86ae-843d9a0488db",
   "metadata": {},
   "source": [
    "Plot the mean precision-recall curve. The approach is the same as for the mean ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f497691-967e-4591-afcf-71c9c7ce3377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "# cv = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "# classifier = resultsGSCV.best_estimator_\n",
    "\n",
    "# prs = []\n",
    "# aucs = []\n",
    "# mean_r = np.linspace(0, 1, 100)\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(10,8))\n",
    "# for i, (train, test) in enumerate(cv.split(X_eval, y_eval)):\n",
    "#     classifier.fit(X.iloc[train], y[train])\n",
    "#     viz = PrecisionRecallDisplay.from_estimator(\n",
    "#         classifier,\n",
    "#         X_eval.iloc[test],\n",
    "#         y_eval[test],\n",
    "#         name=\"fold {}\".format(i),\n",
    "#         alpha=0.3,\n",
    "#         lw=1,\n",
    "#         ax=ax,\n",
    "#     )\n",
    "#     interp_pr = np.interp(mean_r, viz.recall[::-1], viz.precision[::-1])\n",
    "#     prs.append(interp_pr)\n",
    "\n",
    "# mean_p = np.mean(prs, axis=0)\n",
    "# ax.plot(\n",
    "#     mean_r,\n",
    "#     mean_p,\n",
    "#     color=\"b\",\n",
    "#     label=f\"mean\",\n",
    "#     lw=2,\n",
    "#     alpha=0.8,\n",
    "# )\n",
    "# ax.legend(loc=\"lower left\")\n",
    "# clfname = [str(step[1].__class__.__name__) for step in classifier.steps if step[0]=='clf'][0]\n",
    "# ax.set(\n",
    "#     # xlim=[-0.05, 1.05],\n",
    "#     # ylim=[-0.05, 1.05],\n",
    "#     title=f'{clfname} evaluation (precision-recall, {nfolds}-fold CV)')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e419d1-f53e-4a68-a922-6ae77cac13e9",
   "metadata": {},
   "source": [
    "#### Feature importance (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d2cfb82-7f43-466c-b492-90b58e7c6966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Permutation Importance Random Forest')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feature importance of model (best RandomForest from gridsearch) with three methods!\n",
    "\n",
    "fig, (ax2) = plt.subplots(1, 1, figsize=(10,9))\n",
    "plt.subplots_adjust(wspace=1.1)\n",
    "\n",
    "rf = gscv_rf.best_estimator_.steps[2][1]\n",
    "\n",
    "# Get feature importance with Permutation Based Feature Importance (randomly shuffles each feature and compute the \n",
    "# change in the model’s performance. The features which impact the performance the most are the most important one).\n",
    "perm_importance = permutation_importance(rf, X, y)\n",
    "perm_sorted_idx = perm_importance.importances_mean.argsort()\n",
    "x2 = X.columns[perm_sorted_idx]\n",
    "y2 = perm_importance.importances_mean[perm_sorted_idx]\n",
    "ax2.barh(x2, y2)\n",
    "ax2.set_title(\"Permutation Importance Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "917caad5-b4d1-4364-a63e-b0f77f687670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probaj enako z X_eval in primerjaj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0547d681-1201-4853-a89a-006a0efddb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get feature importance with SHAP\n",
    "# explainer = shap.TreeExplainer(rf)\n",
    "# shap_values = explainer.shap_values(X)\n",
    "# RF_shap = shap.summary_plot(shap_values, X, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d081269b-9944-4784-abf0-d36ad02267aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probaj enako z X_eval in primerjaj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88d84a41-047b-4a58-a46e-689374f7d873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x597.6 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SHAP summary plot\n",
    "explainer = shap.TreeExplainer(rf)\n",
    "shap_values = explainer.shap_values(X)\n",
    "classid = 1\n",
    "shap.summary_plot(shap_values[classid], X, max_display=len(X.columns), class_names=le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ca727-c52f-412b-a499-d6e9cdee947f",
   "metadata": {},
   "source": [
    "### Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc6e6b4-1a36-47c3-92c2-51a1340b79c1",
   "metadata": {},
   "source": [
    "#### Model Evaluation (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b70de528-5c6d-428b-a0f4-890707fc436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for NN in scikit_learn\n",
    "\n",
    "# Model evaluation with the pipeline of SMOTE oversampling and undersampling on the training dataset only (within each cross-validation fold)!\n",
    "\n",
    "# one-hot encoding of month feature\n",
    "Xohe = pd.get_dummies(X, columns=[\"month\"])\n",
    "\n",
    "X_display = Xohe.copy()  # *used for SHAP visualization so we can show unscaled values\n",
    "\n",
    "# scalling numeric values for NN\n",
    "scaled_array = StandardScaler().fit_transform(Xohe)\n",
    "Xsc = pd.DataFrame(scaled_array, columns=Xohe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a550fb63-51e5-464b-ae88-7b8579958cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c60b9414-2528-450c-b46e-44967eee00a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 648 candidates, totalling 1944 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "432 fits failed out of a total of 1944.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "432 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mmarzi/.pyenv/versions/3.8.0/envs/hab/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/mmarzi/.pyenv/versions/3.8.0/envs/hab/lib/python3.8/site-packages/imblearn/pipeline.py\", line 268, in fit\n",
      "    Xt, yt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/mmarzi/.pyenv/versions/3.8.0/envs/hab/lib/python3.8/site-packages/imblearn/pipeline.py\", line 226, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "  File \"/Users/mmarzi/.pyenv/versions/3.8.0/envs/hab/lib/python3.8/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/mmarzi/.pyenv/versions/3.8.0/envs/hab/lib/python3.8/site-packages/imblearn/pipeline.py\", line 394, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **fit_params)\n",
      "  File \"/Users/mmarzi/.pyenv/versions/3.8.0/envs/hab/lib/python3.8/site-packages/imblearn/base.py\", line 79, in fit_resample\n",
      "    self.sampling_strategy_ = check_sampling_strategy(\n",
      "  File \"/Users/mmarzi/.pyenv/versions/3.8.0/envs/hab/lib/python3.8/site-packages/imblearn/utils/_validation.py\", line 534, in check_sampling_strategy\n",
      "    _sampling_strategy_float(sampling_strategy, y, sampling_type).items()\n",
      "  File \"/Users/mmarzi/.pyenv/versions/3.8.0/envs/hab/lib/python3.8/site-packages/imblearn/utils/_validation.py\", line 393, in _sampling_strategy_float\n",
      "    raise ValueError(\n",
      "ValueError: The specified ratio required to generate new sample in the majority class while trying to remove samples. Please increase the ratio.\n",
      "\n",
      "One or more of the test scores are non-finite: [0.46825397 0.53253968 0.48492063 0.3547619  0.32380952 0.53174603\n",
      "        nan        nan 0.38571429 0.43809524 0.48571429 0.42142857\n",
      " 0.3531746  0.38650794 0.30714286        nan        nan 0.4484127\n",
      " 0.38809524 0.48492063 0.43571429 0.4031746  0.40634921 0.41825397\n",
      "        nan        nan 0.43492063 0.42142857 0.51746032 0.48333333\n",
      " 0.28888889 0.4031746  0.37142857        nan        nan 0.46587302\n",
      " 0.45238095 0.35634921 0.43650794 0.28968254 0.46746032 0.51428571\n",
      "        nan        nan 0.53253968 0.4531746  0.46666667 0.48333333\n",
      " 0.46825397 0.43492063 0.42142857        nan        nan 0.38650794\n",
      " 0.38809524 0.34126984 0.37619048 0.45555556 0.5015873  0.56666667\n",
      "        nan        nan 0.46904762 0.32380952 0.41825397 0.58174603\n",
      " 0.23809524 0.26111111 0.45396825        nan        nan 0.45238095\n",
      " 0.48492063 0.38571429 0.46746032 0.46746032 0.46904762 0.4031746\n",
      "        nan        nan 0.55079365 0.40396825 0.40555556 0.5\n",
      " 0.2547619  0.45396825 0.53333333        nan        nan 0.4531746\n",
      " 0.26111111 0.4031746  0.49920635 0.29603175 0.43492063 0.53253968\n",
      "        nan        nan 0.38888889 0.22857143 0.51666667 0.48571429\n",
      " 0.37142857 0.49761905 0.43650794        nan        nan 0.54920635\n",
      " 0.41904762 0.51984127 0.5468254  0.48412698 0.46825397 0.41746032\n",
      "        nan        nan 0.50238095 0.38730159 0.46587302 0.41904762\n",
      " 0.41904762 0.43571429 0.57936508        nan        nan 0.33968254\n",
      " 0.45396825 0.46825397 0.5        0.41904762 0.38968254 0.45238095\n",
      "        nan        nan 0.48650794 0.40396825 0.45238095 0.5484127\n",
      " 0.43412698 0.4047619  0.45079365        nan        nan 0.45238095\n",
      " 0.46904762 0.43571429 0.48492063 0.51825397 0.37063492 0.38650794\n",
      "        nan        nan 0.49920635 0.4515873  0.55       0.37142857\n",
      " 0.46904762 0.45079365 0.4515873         nan        nan 0.45079365\n",
      " 0.5515873  0.28888889 0.53015873 0.17460317 0.37222222 0.38412698\n",
      "        nan        nan 0.46825397 0.38730159 0.4515873  0.22222222\n",
      " 0.53253968 0.35555556 0.40238095        nan        nan 0.48333333\n",
      " 0.48571429 0.45       0.53412698 0.38650794 0.43412698 0.49920635\n",
      "        nan        nan 0.43888889 0.51825397 0.57936508 0.47063492\n",
      " 0.36904762 0.21666667 0.43412698        nan        nan 0.36507937\n",
      " 0.40079365 0.54603175 0.43492063 0.4515873  0.38730159 0.45\n",
      "        nan        nan 0.53095238 0.45396825 0.36507937 0.22777778\n",
      " 0.56587302 0.48333333 0.4515873         nan        nan 0.37301587\n",
      " 0.21269841 0.         0.45555556 0.17460317 0.24047619 0.56587302\n",
      "        nan        nan 0.26984127 0.19761905 0.2        0.31746032\n",
      " 0.26111111 0.12698413 0.33333333        nan        nan 0.41984127\n",
      " 0.18015873 0.18015873 0.19285714 0.         0.15873016 0.55555556\n",
      "        nan        nan 0.41904762 0.01666667 0.17460317 0.25396825\n",
      " 0.2547619  0.23333333 0.42857143        nan        nan 0.\n",
      " 0.11111111 0.12698413 0.38095238 0.08333333 0.36190476 0.26111111\n",
      "        nan        nan 0.11111111 0.01587302 0.42857143 0.3515873\n",
      " 0.3015873  0.12698413 0.36507937        nan        nan 0.4547619\n",
      " 0.32301587 0.43730159 0.42063492 0.5484127  0.23809524 0.40396825\n",
      "        nan        nan 0.5        0.50079365 0.46587302 0.46825397\n",
      " 0.43809524 0.48333333 0.31825397        nan        nan 0.31031746\n",
      " 0.38888889 0.38809524 0.53253968 0.4515873  0.33333333 0.48492063\n",
      "        nan        nan 0.51904762 0.5515873  0.57777778 0.51349206\n",
      " 0.38650794 0.29444444 0.47142857        nan        nan 0.46825397\n",
      " 0.48333333 0.4047619  0.46746032 0.22380952 0.51825397 0.5515873\n",
      "        nan        nan 0.53095238 0.40396825 0.46825397 0.44920635\n",
      " 0.45396825 0.48412698 0.46825397        nan        nan 0.51825397\n",
      " 0.48333333 0.48412698 0.43571429 0.43492063 0.46904762 0.43333333\n",
      "        nan        nan 0.43571429 0.46825397 0.42063492 0.5015873\n",
      " 0.49920635 0.43730159 0.38730159        nan        nan 0.38809524\n",
      " 0.40238095 0.48333333 0.4015873  0.4031746  0.4531746  0.48492063\n",
      "        nan        nan 0.51587302 0.48412698 0.43730159 0.48412698\n",
      " 0.32301587 0.4531746  0.45079365        nan        nan 0.43492063\n",
      " 0.48253968 0.42142857 0.56349206 0.50079365 0.43492063 0.35555556\n",
      "        nan        nan 0.43571429 0.40396825 0.41904762 0.43730159\n",
      " 0.40396825 0.48650794 0.43730159        nan        nan 0.32063492\n",
      " 0.30634921 0.46984127 0.43730159 0.43650794 0.43730159 0.42142857\n",
      "        nan        nan 0.40079365 0.33888889 0.38888889 0.4515873\n",
      " 0.38730159 0.38650794 0.56507937        nan        nan 0.56507937\n",
      " 0.1952381  0.5031746  0.43730159 0.42460317 0.45079365 0.43492063\n",
      "        nan        nan 0.38809524 0.38809524 0.40555556 0.41984127\n",
      " 0.48492063 0.51825397 0.54920635        nan        nan 0.48492063\n",
      " 0.34047619 0.36984127 0.46825397 0.37063492 0.4984127  0.45079365\n",
      "        nan        nan 0.48571429 0.40396825 0.37063492 0.38968254\n",
      " 0.43730159 0.4031746  0.43650794        nan        nan 0.40396825\n",
      " 0.43730159 0.45396825 0.46825397 0.46904762 0.43492063 0.46904762\n",
      "        nan        nan 0.48492063 0.41904762 0.43571429 0.55\n",
      " 0.38809524 0.38650794 0.4515873         nan        nan 0.48650794\n",
      " 0.5484127  0.48571429 0.46904762 0.43730159 0.4515873  0.46746032\n",
      "        nan        nan 0.43492063 0.48492063 0.42142857 0.51349206\n",
      " 0.38650794 0.4515873  0.4031746         nan        nan 0.5\n",
      " 0.43571429 0.45238095 0.46666667 0.46666667 0.56507937 0.40238095\n",
      "        nan        nan 0.51825397 0.4515873  0.51507937 0.43492063\n",
      " 0.4531746  0.48571429 0.5015873         nan        nan 0.43492063\n",
      " 0.45       0.4031746  0.38888889 0.19047619 0.45238095 0.43650794\n",
      "        nan        nan 0.56190476 0.43571429 0.54761905 0.4984127\n",
      " 0.43492063 0.53174603 0.58174603        nan        nan 0.33809524\n",
      " 0.38571429 0.53253968 0.48253968 0.41904762 0.38809524 0.50238095\n",
      "        nan        nan 0.37301587 0.38888889 0.4984127  0.41984127\n",
      " 0.42063492 0.46666667 0.56587302        nan        nan 0.45079365\n",
      " 0.32460317 0.4515873  0.41825397 0.2452381  0.51666667 0.43650794\n",
      "        nan        nan 0.42063492 0.51746032 0.4031746  0.40396825\n",
      " 0.4515873  0.4031746  0.4031746         nan        nan 0.45079365\n",
      " 0.27857143 0.30873016 0.4515873  0.30793651 0.42142857 0.27380952\n",
      "        nan        nan 0.34126984 0.19047619 0.33968254 0.4515873\n",
      " 0.35634921 0.23968254 0.27063492        nan        nan 0.49920635\n",
      " 0.         0.37301587 0.17460317 0.24603175 0.30396825 0.33333333\n",
      "        nan        nan 0.51904762 0.28571429 0.22619048 0.38730159\n",
      " 0.2547619  0.36507937 0.30714286        nan        nan 0.46904762\n",
      " 0.18333333 0.49920635 0.35555556 0.41904762 0.41984127 0.54920635\n",
      "        nan        nan 0.51825397 0.21031746 0.48571429 0.36984127\n",
      " 0.43492063 0.19047619 0.48730159        nan        nan 0.33571429\n",
      " 0.48492063 0.45396825 0.43253968 0.43571429 0.53333333 0.40396825\n",
      "        nan        nan 0.37142857 0.45079365 0.49920635 0.42063492\n",
      " 0.42063492 0.43650794 0.38571429        nan        nan 0.35396825\n",
      " 0.4531746  0.46825397 0.48253968 0.37063492 0.46666667 0.42142857\n",
      "        nan        nan 0.45238095 0.38809524 0.46984127 0.46904762\n",
      " 0.48412698 0.46666667 0.43650794        nan        nan 0.42063492\n",
      " 0.5484127  0.32222222 0.51746032 0.38809524 0.4515873  0.40396825\n",
      "        nan        nan 0.46984127 0.50238095 0.5015873  0.43650794\n",
      " 0.40396825 0.25793651 0.46746032        nan        nan 0.42063492]\n",
      "One or more of the test scores are non-finite: [0.32594628 0.30197133 0.42270531 0.26349206 0.21858289 0.26919192\n",
      "        nan        nan 0.25684866 0.28940621 0.39350649 0.31320028\n",
      " 0.38888889 0.32038332 0.24777778        nan        nan 0.33210715\n",
      " 0.32056277 0.26993338 0.29918478 0.35985827 0.28309608 0.27301587\n",
      "        nan        nan 0.3224731  0.33131105 0.36217346 0.25\n",
      " 0.31314518 0.24775112 0.27598566        nan        nan 0.30502137\n",
      " 0.26978752 0.33430592 0.26636901 0.23200418 0.31007326 0.24221857\n",
      "        nan        nan 0.22169513 0.41957672 0.35050589 0.2989934\n",
      " 0.31507937 0.31301587 0.31250298        nan        nan 0.26692007\n",
      " 0.39735265 0.21625712 0.2237037  0.27250958 0.30022492 0.27828135\n",
      "        nan        nan 0.29933505 0.2316157  0.23734568 0.21404094\n",
      " 0.16904762 0.16141975 0.27587904        nan        nan 0.33858774\n",
      " 0.28065101 0.34455031 0.21294372 0.34035088 0.35707659 0.21917211\n",
      "        nan        nan 0.25804424 0.21507937 0.23398693 0.29365716\n",
      " 0.22222222 0.27010753 0.25046491        nan        nan 0.27208995\n",
      " 0.23677249 0.33705206 0.18210223 0.1908939  0.21157796 0.25066138\n",
      "        nan        nan 0.24843675 0.23639847 0.21306352 0.25825593\n",
      " 0.29292441 0.22044606 0.21881092        nan        nan 0.26669508\n",
      " 0.34442718 0.28333333 0.32529534 0.36631879 0.32257873 0.275\n",
      "        nan        nan 0.28604402 0.38109162 0.2616904  0.40117521\n",
      " 0.2960432  0.35571848 0.28134647        nan        nan 0.26064089\n",
      " 0.29618167 0.37496874 0.24639234 0.3714927  0.22766517 0.23375104\n",
      "        nan        nan 0.29334291 0.29620915 0.32063492 0.29354354\n",
      " 0.34655172 0.29573755 0.27995338        nan        nan 0.26654217\n",
      " 0.2278413  0.30941376 0.26984127 0.30686071 0.29666204 0.23015873\n",
      "        nan        nan 0.27364418 0.34523098 0.26847415 0.23631773\n",
      " 0.34514593 0.35728587 0.33879626        nan        nan 0.29013243\n",
      " 0.29098973 0.31308561 0.26388889 0.25641026 0.21685454 0.2372364\n",
      "        nan        nan 0.22287682 0.29185185 0.2551052  0.17673993\n",
      " 0.21269775 0.14509804 0.32709989        nan        nan 0.25824373\n",
      " 0.30118598 0.24021529 0.30816327 0.24076589 0.29005384 0.18391945\n",
      "        nan        nan 0.18023715 0.23954248 0.30521514 0.25661935\n",
      " 0.31210317 0.06280193 0.24305556        nan        nan 0.1688771\n",
      " 0.23605511 0.22435769 0.22442529 0.3005698  0.29036846 0.31434141\n",
      "        nan        nan 0.27340979 0.2857036  0.19056889 0.25974026\n",
      " 0.24479167 0.28820276 0.26601307        nan        nan 0.18095238\n",
      " 0.13684211 0.         0.27555031 0.08527132 0.26978577 0.25701023\n",
      "        nan        nan 0.18827839 0.21071429 0.1025641  0.14188218\n",
      " 0.16904762 0.05925926 0.16441441        nan        nan 0.30463875\n",
      " 0.15889114 0.11488158 0.13448276 0.         0.10416667 0.11338671\n",
      "        nan        nan 0.19906844 0.05555556 0.17615176 0.11587302\n",
      " 0.24444444 0.09722222 0.18917432        nan        nan 0.\n",
      " 0.09333333 0.11111111 0.14454749 0.18518519 0.23227753 0.15951103\n",
      "        nan        nan 0.07526882 0.16666667 0.18973277 0.22901076\n",
      " 0.14639025 0.06060606 0.17801858        nan        nan 0.20933105\n",
      " 0.50280982 0.27482993 0.35138889 0.32375462 0.2020202  0.32610723\n",
      "        nan        nan 0.30095023 0.34951043 0.21607048 0.29188596\n",
      " 0.33299496 0.30960296 0.56344086        nan        nan 0.16721706\n",
      " 0.27612352 0.25352734 0.2018008  0.37036301 0.12904313 0.32748538\n",
      "        nan        nan 0.24378924 0.30764881 0.26506574 0.23375617\n",
      " 0.28076319 0.21972885 0.28463246        nan        nan 0.366133\n",
      " 0.31205631 0.32276498 0.318438   0.15740741 0.30200513 0.31123981\n",
      "        nan        nan 0.26649507 0.27893683 0.28090153 0.26486928\n",
      " 0.30696544 0.25609756 0.2720073         nan        nan 0.32749767\n",
      " 0.39631665 0.3238694  0.26729831 0.31045752 0.3399699  0.29584305\n",
      "        nan        nan 0.33640553 0.33774929 0.27968805 0.28634259\n",
      " 0.25611251 0.39091943 0.29719005        nan        nan 0.34375\n",
      " 0.31968733 0.36253996 0.25319865 0.39594356 0.32843137 0.27869795\n",
      "        nan        nan 0.30643705 0.3701897  0.22767762 0.31223359\n",
      " 0.32278583 0.36309524 0.2745279         nan        nan 0.39749373\n",
      " 0.34343434 0.41305916 0.25619442 0.3004329  0.27120669 0.25428571\n",
      "        nan        nan 0.29283217 0.24096468 0.22919303 0.25943563\n",
      " 0.30379424 0.26433498 0.31255411        nan        nan 0.27222222\n",
      " 0.25387205 0.31856411 0.25238095 0.36923831 0.25931187 0.26503385\n",
      "        nan        nan 0.29172679 0.27055167 0.23536598 0.26780666\n",
      " 0.33552324 0.27586207 0.24775641        nan        nan 0.27948403\n",
      " 0.19172113 0.36865575 0.25619565 0.25829563 0.30128205 0.26161674\n",
      "        nan        nan 0.3149677  0.29670078 0.30592396 0.25675676\n",
      " 0.30699526 0.24648554 0.26666667        nan        nan 0.28229238\n",
      " 0.29444444 0.2597781  0.25120998 0.33492063 0.34023464 0.25163399\n",
      "        nan        nan 0.2526738  0.28265107 0.26416122 0.2365929\n",
      " 0.33896151 0.22779258 0.23590613        nan        nan 0.24985146\n",
      " 0.31303198 0.35372214 0.28715243 0.38585434 0.31203574 0.34747447\n",
      "        nan        nan 0.32714372 0.30966725 0.28250453 0.28777367\n",
      " 0.28333333 0.29906832 0.33111854        nan        nan 0.32847594\n",
      " 0.36989469 0.28932909 0.29285262 0.36751601 0.31161302 0.29447725\n",
      "        nan        nan 0.34920635 0.32569463 0.28897348 0.26954736\n",
      " 0.28925098 0.31283203 0.37611408        nan        nan 0.34718137\n",
      " 0.30557279 0.3078853  0.2663093  0.35454002 0.33489621 0.35094851\n",
      "        nan        nan 0.31911489 0.28869602 0.36944444 0.28730159\n",
      " 0.37142857 0.29285714 0.29249603        nan        nan 0.29539295\n",
      " 0.33402778 0.31190476 0.2784611  0.21632997 0.25412311 0.3500115\n",
      "        nan        nan 0.3062596  0.29227053 0.25152697 0.22328747\n",
      " 0.26810967 0.35631841 0.30252958        nan        nan 0.27632338\n",
      " 0.32505176 0.23920635 0.26925546 0.33928115 0.26818182 0.31710605\n",
      "        nan        nan 0.24150621 0.33722491 0.25046149 0.2560307\n",
      " 0.26340326 0.2776907  0.37423011        nan        nan 0.2692024\n",
      " 0.23261858 0.33872875 0.28986148 0.22134387 0.29972222 0.30518519\n",
      "        nan        nan 0.28307255 0.29646544 0.25951374 0.29995591\n",
      " 0.29008818 0.34555556 0.20172089        nan        nan 0.27700028\n",
      " 0.15845649 0.18891836 0.31922631 0.22098765 0.25538462 0.16330532\n",
      "        nan        nan 0.24883287 0.22051282 0.19166667 0.26728682\n",
      " 0.50336927 0.20565217 0.18851541        nan        nan 0.25228252\n",
      " 0.         0.35926541 0.11458333 0.21164021 0.23295455 0.18095238\n",
      "        nan        nan 0.30027367 0.15370986 0.16858238 0.27568548\n",
      " 0.22385621 0.22473868 0.25741542        nan        nan 0.26878734\n",
      " 0.09649123 0.2899729  0.2232948  0.28024758 0.23845982 0.22373407\n",
      "        nan        nan 0.34736842 0.34412955 0.26711324 0.24074074\n",
      " 0.37142857 0.18768328 0.2845098         nan        nan 0.24847826\n",
      " 0.27666033 0.22285884 0.34479246 0.35714286 0.26404151 0.26126288\n",
      "        nan        nan 0.28981685 0.33573323 0.28044371 0.25615852\n",
      " 0.33613445 0.3653302  0.27316645        nan        nan 0.26390805\n",
      " 0.33274607 0.35603865 0.2603572  0.30750751 0.28913759 0.29144214\n",
      "        nan        nan 0.28725038 0.30909091 0.22911338 0.3046595\n",
      " 0.34989107 0.35091823 0.2915522         nan        nan 0.30233964\n",
      " 0.38562092 0.22877902 0.2677063  0.27361111 0.33097798 0.25884647\n",
      "        nan        nan 0.24857881 0.3108838  0.27320162 0.25010423\n",
      " 0.2922807  0.27083333 0.31545419        nan        nan 0.29388354]\n",
      "One or more of the test scores are non-finite: [0.38037087 0.38313934 0.45111833 0.29656085 0.25925926 0.33894029\n",
      "        nan        nan 0.29961353 0.33119242 0.42784657 0.33704471\n",
      " 0.36788049 0.34456419 0.27242108        nan        nan 0.34905336\n",
      " 0.32254719 0.34402064 0.34957118 0.37578728 0.32533681 0.3288961\n",
      "        nan        nan 0.36745685 0.36730055 0.41642353 0.32748538\n",
      " 0.29891775 0.30358412 0.28389549        nan        nan 0.36758632\n",
      " 0.33274004 0.33098039 0.33047228 0.25026241 0.37031408 0.31962691\n",
      "        nan        nan 0.30702856 0.42393162 0.39585343 0.35543672\n",
      " 0.36904762 0.36285561 0.35286132        nan        nan 0.31102333\n",
      " 0.38218194 0.26038993 0.27402453 0.33281046 0.35969162 0.35651601\n",
      "        nan        nan 0.36426204 0.26691603 0.29558288 0.31206349\n",
      " 0.19603175 0.19722222 0.33000333        nan        nan 0.38459878\n",
      " 0.35228189 0.36126228 0.29227448 0.38220503 0.39008633 0.27858999\n",
      "        nan        nan 0.34246817 0.27481326 0.28577797 0.36960136\n",
      " 0.23502734 0.33111961 0.335             nan        nan 0.33815657\n",
      " 0.24465812 0.35828532 0.2666491  0.22311828 0.28347763 0.33796671\n",
      "        nan        nan 0.28295739 0.22294151 0.28904895 0.33220794\n",
      " 0.32268696 0.29552443 0.29094977        nan        nan 0.35639731\n",
      " 0.37724694 0.34060762 0.40372965 0.41082389 0.37980461 0.32877473\n",
      "        nan        nan 0.34670091 0.38141026 0.31648114 0.40310419\n",
      " 0.34363995 0.39034878 0.37161011        nan        nan 0.29337006\n",
      " 0.35375624 0.38734774 0.32054885 0.39254494 0.28051236 0.30795293\n",
      "        nan        nan 0.35911765 0.3405713  0.37349896 0.38079905\n",
      " 0.38333333 0.34005698 0.34441978        nan        nan 0.33528139\n",
      " 0.30550045 0.35905258 0.34285242 0.3824851  0.31974003 0.28714146\n",
      "        nan        nan 0.34404172 0.39063714 0.35405471 0.28465298\n",
      " 0.39692493 0.39589744 0.38684397        nan        nan 0.349003\n",
      " 0.36627075 0.28848456 0.34809737 0.1912448  0.26397933 0.27915188\n",
      "        nan        nan 0.28428471 0.32969864 0.32395557 0.17742988\n",
      " 0.30213072 0.20132953 0.35857638        nan        nan 0.32955999\n",
      " 0.36158549 0.30791804 0.38253968 0.29607127 0.34603355 0.26836389\n",
      "        nan        nan 0.25485904 0.32298475 0.3951549  0.32846096\n",
      " 0.32840054 0.09737828 0.30583764        nan        nan 0.22294574\n",
      " 0.28496732 0.3097076  0.29159238 0.35632925 0.31977671 0.35188724\n",
      "        nan        nan 0.35205205 0.34039294 0.23484848 0.21346618\n",
      " 0.33325552 0.35197795 0.3306472         nan        nan 0.23412698\n",
      " 0.16441779 0.         0.30080074 0.11458333 0.237532   0.3457322\n",
      "        nan        nan 0.22061803 0.17805643 0.13559322 0.18980392\n",
      " 0.204329   0.08080808 0.2193903         nan        nan 0.35160173\n",
      " 0.16541353 0.13710879 0.15846339 0.         0.12578616 0.1868932\n",
      "        nan        nan 0.26959523 0.02564103 0.15268817 0.15671809\n",
      " 0.2447832  0.1372549  0.19945767        nan        nan 0.\n",
      " 0.10144928 0.11851852 0.20945946 0.11494253 0.23885918 0.19452888\n",
      "        nan        nan 0.08974359 0.02898551 0.25045752 0.25255032\n",
      " 0.19673203 0.08205128 0.23589744        nan        nan 0.26986745\n",
      " 0.28709856 0.31128802 0.3782503  0.39640758 0.21802326 0.35882566\n",
      "        nan        nan 0.36556983 0.410587   0.29236452 0.35884654\n",
      " 0.35746606 0.36693999 0.29205818        nan        nan 0.21145632\n",
      " 0.3223677  0.30277778 0.28476874 0.39710169 0.18603019 0.36300063\n",
      "        nan        nan 0.32393794 0.35539906 0.35581323 0.31785999\n",
      " 0.32419685 0.24985146 0.34855226        nan        nan 0.39241664\n",
      " 0.37676768 0.35639183 0.36329798 0.15428571 0.33705634 0.39545755\n",
      "        nan        nan 0.34410983 0.32222222 0.33857624 0.31346465\n",
      " 0.36542592 0.33491701 0.34385965        nan        nan 0.37712336\n",
      " 0.4241453  0.38605442 0.32826279 0.36102198 0.39238095 0.34620625\n",
      "        nan        nan 0.37588882 0.3902783  0.33452363 0.36349994\n",
      " 0.32734984 0.40973207 0.33299005        nan        nan 0.35249326\n",
      " 0.34646602 0.40668547 0.31007782 0.39013939 0.3678991  0.34761699\n",
      "        nan        nan 0.38418869 0.39778864 0.29706937 0.37474747\n",
      " 0.31060606 0.39891395 0.33014343        nan        nan 0.40262515\n",
      " 0.39858907 0.38018433 0.35083333 0.37318153 0.33058526 0.29454438\n",
      "        nan        nan 0.34613936 0.30027065 0.29239766 0.32264465\n",
      " 0.33751832 0.33761905 0.35908289        nan        nan 0.29175833\n",
      " 0.27029157 0.37300801 0.31696429 0.38587008 0.30952381 0.31277431\n",
      "        nan        nan 0.32968685 0.29777568 0.28639716 0.33409728\n",
      " 0.35608229 0.32190476 0.33767247        nan        nan 0.37327586\n",
      " 0.19036954 0.39166339 0.31426901 0.29782568 0.35642212 0.32559942\n",
      "        nan        nan 0.3373664  0.32834008 0.34824142 0.31732348\n",
      " 0.37355252 0.32927742 0.35502169        nan        nan 0.35673981\n",
      " 0.30104618 0.3049806  0.32501034 0.35022676 0.39548896 0.32085561\n",
      "        nan        nan 0.32905033 0.32297144 0.30353535 0.29419576\n",
      " 0.36604532 0.29071359 0.30498153        nan        nan 0.30871821\n",
      " 0.36100804 0.37537913 0.35373406 0.41434078 0.35980686 0.39404423\n",
      "        nan        nan 0.38743712 0.34929132 0.3411533  0.37432181\n",
      " 0.32190195 0.33268509 0.37769913        nan        nan 0.38433169\n",
      " 0.43991115 0.36038001 0.36027799 0.39047619 0.36537205 0.36052769\n",
      "        nan        nan 0.3822458  0.38509049 0.33564391 0.35217947\n",
      " 0.3305833  0.36907854 0.38021918        nan        nan 0.40863986\n",
      " 0.35878505 0.36636501 0.33867134 0.39583176 0.4150226  0.3590291\n",
      "        nan        nan 0.3909482  0.35104932 0.42696857 0.33834439\n",
      " 0.40530806 0.36202546 0.36780338        nan        nan 0.34870527\n",
      " 0.38168259 0.35076253 0.32106594 0.18603801 0.32512917 0.38239538\n",
      "        nan        nan 0.39549372 0.34317292 0.33034692 0.29735735\n",
      " 0.3315376  0.40316206 0.3978363         nan        nan 0.30332016\n",
      " 0.33124967 0.3271547  0.34513746 0.36705517 0.30820007 0.37950162\n",
      "        nan        nan 0.28662173 0.35950842 0.33292065 0.31728475\n",
      " 0.32317178 0.34564103 0.44046605        nan        nan 0.33606228\n",
      " 0.27007954 0.38131533 0.33952494 0.23255814 0.37573922 0.3515174\n",
      "        nan        nan 0.32995341 0.36138797 0.31017069 0.34271284\n",
      " 0.34100896 0.36895016 0.26221359        nan        nan 0.34121533\n",
      " 0.18925519 0.23356009 0.36520748 0.25726603 0.31085285 0.20458554\n",
      "        nan        nan 0.28358238 0.19607843 0.24464061 0.33125\n",
      " 0.29842342 0.22134592 0.21925134        nan        nan 0.33426676\n",
      " 0.         0.32711039 0.13836478 0.21428571 0.25052965 0.23412698\n",
      "        nan        nan 0.37768049 0.19940476 0.19302721 0.32146966\n",
      " 0.18855219 0.27583937 0.24947393        nan        nan 0.34062078\n",
      " 0.12643678 0.36421871 0.27227134 0.33558664 0.30392777 0.31055901\n",
      "        nan        nan 0.40505637 0.21374637 0.33560305 0.273671\n",
      " 0.39134543 0.17788462 0.34796748        nan        nan 0.26729579\n",
      " 0.34858069 0.29050691 0.37211298 0.37099567 0.35296388 0.31649832\n",
      "        nan        nan 0.32549493 0.38458629 0.35560651 0.31143317\n",
      " 0.36895944 0.39364548 0.31733477        nan        nan 0.30222222\n",
      " 0.37445552 0.40249554 0.33699098 0.32763926 0.33827607 0.33972349\n",
      "        nan        nan 0.34894471 0.33386492 0.30675991 0.36913401\n",
      " 0.39845322 0.39711335 0.34789485        nan        nan 0.35037245\n",
      " 0.44669598 0.26432068 0.34379509 0.3204717  0.37710068 0.31487707\n",
      "        nan        nan 0.32246838 0.37063492 0.3509843  0.31563882\n",
      " 0.33647752 0.25815438 0.36922393        nan        nan 0.3367674 ]\n",
      "One or more of the test scores are non-finite: [0.653724   0.64519244 0.68338213 0.59536951 0.56248253 0.64312551\n",
      "        nan        nan 0.61062132 0.61394692 0.67975026 0.63243378\n",
      " 0.66038585 0.64697086 0.58483118        nan        nan 0.6839976\n",
      " 0.62957172 0.62569763 0.67222435 0.65160634 0.63905154 0.65565758\n",
      "        nan        nan 0.62314488 0.65909676 0.68275002 0.63136603\n",
      " 0.6194828  0.59783188 0.62271017        nan        nan 0.64497712\n",
      " 0.65198237 0.63287117 0.66483077 0.61249455 0.65454362 0.6187372\n",
      "        nan        nan 0.6156101  0.66923356 0.68055149 0.63225911\n",
      " 0.66182639 0.61208497 0.63275501        nan        nan 0.60976427\n",
      " 0.65808862 0.58516701 0.70514043 0.67996928 0.64655481 0.69754419\n",
      "        nan        nan 0.70154109 0.62017082 0.61809917 0.61324421\n",
      " 0.64436294 0.6834154  0.6453172         nan        nan 0.66143336\n",
      " 0.63476362 0.63134366 0.61097961 0.67362137 0.70776702 0.64382231\n",
      "        nan        nan 0.63018072 0.61856855 0.58963162 0.67270684\n",
      " 0.52419674 0.65848526 0.65593981        nan        nan 0.67257598\n",
      " 0.62028125 0.65809842 0.57705566 0.68696834 0.63226226 0.62803839\n",
      "        nan        nan 0.62788313 0.5347469  0.6044209  0.63232953\n",
      " 0.66062058 0.61342497 0.6271148         nan        nan 0.68102742\n",
      " 0.70163046 0.66300874 0.68354053 0.70847195 0.65330084 0.66387484\n",
      "        nan        nan 0.67136952 0.6802821  0.65067629 0.66669458\n",
      " 0.62849972 0.62821314 0.67821536        nan        nan 0.61945073\n",
      " 0.61742076 0.67163336 0.60802559 0.69471882 0.59935098 0.62921944\n",
      "        nan        nan 0.6788227  0.60907061 0.66064405 0.67868676\n",
      " 0.71577755 0.64950947 0.65688428        nan        nan 0.64557947\n",
      " 0.64556764 0.66476165 0.64798593 0.71418443 0.62133717 0.60154146\n",
      "        nan        nan 0.67819503 0.64835411 0.67053234 0.63451697\n",
      " 0.65760215 0.68193945 0.66257033        nan        nan 0.67005373\n",
      " 0.66428461 0.60952113 0.64769954 0.57902269 0.61508574 0.57816776\n",
      "        nan        nan 0.61192029 0.62346464 0.65250932 0.5243714\n",
      " 0.60433689 0.57526615 0.67398151        nan        nan 0.59566154\n",
      " 0.65306592 0.59809858 0.66134575 0.60156401 0.64252214 0.59742498\n",
      "        nan        nan 0.62769193 0.65167685 0.69883106 0.65356514\n",
      " 0.615671   0.53865665 0.60221498        nan        nan 0.58442197\n",
      " 0.57893794 0.61352099 0.58411848 0.65631353 0.59073181 0.58390713\n",
      "        nan        nan 0.58837174 0.60390153 0.58947886 0.55860411\n",
      " 0.64643901 0.63053929 0.62878583        nan        nan 0.59366299\n",
      " 0.53170565 0.51425189 0.62634305 0.53810448 0.4782075  0.60324188\n",
      "        nan        nan 0.6791346  0.56307703 0.50735874 0.58421034\n",
      " 0.56799909 0.60356422 0.59825818        nan        nan 0.6480363\n",
      " 0.59355561 0.54792494 0.56622983 0.48023941 0.56586858 0.54713582\n",
      "        nan        nan 0.6048338  0.60472374 0.59561163 0.56038492\n",
      " 0.62425459 0.54707399 0.53912066        nan        nan 0.54298357\n",
      " 0.52199572 0.63118868 0.56071928 0.55368066 0.59989335 0.58293216\n",
      "        nan        nan 0.50738877 0.55502815 0.59984216 0.57203361\n",
      " 0.54268498 0.55373352 0.58185573        nan        nan 0.57456852\n",
      " 0.61156431 0.60425012 0.62102722 0.65889992 0.61356932 0.64713277\n",
      "        nan        nan 0.66011452 0.69493516 0.60810738 0.65806792\n",
      " 0.67133486 0.66450242 0.6053747         nan        nan 0.57837237\n",
      " 0.66027781 0.60898818 0.60705608 0.67409129 0.55450204 0.63177218\n",
      "        nan        nan 0.63813756 0.62597959 0.65636131 0.61691711\n",
      " 0.61947559 0.61745773 0.65509191        nan        nan 0.64389403\n",
      " 0.61232894 0.64432782 0.62132156 0.56956347 0.65361033 0.67254438\n",
      "        nan        nan 0.64013537 0.6664373  0.64314862 0.63298743\n",
      " 0.65245756 0.62712644 0.62207066        nan        nan 0.67973603\n",
      " 0.62958336 0.66911074 0.61780382 0.61887037 0.6281552  0.6656564\n",
      "        nan        nan 0.69099667 0.71461304 0.64339952 0.65237347\n",
      " 0.64615447 0.69172563 0.62818865        nan        nan 0.66801083\n",
      " 0.60982111 0.69646055 0.62277735 0.65579583 0.66949139 0.64407849\n",
      "        nan        nan 0.69836113 0.68776745 0.62595815 0.65541767\n",
      " 0.62679098 0.66151367 0.64538328        nan        nan 0.71945276\n",
      " 0.62681325 0.61718733 0.63529676 0.66208867 0.62911889 0.57486147\n",
      "        nan        nan 0.65351145 0.59668927 0.58185406 0.66685279\n",
      " 0.6469898  0.65058415 0.64713138        nan        nan 0.63432235\n",
      " 0.63017268 0.67930168 0.63500344 0.69007604 0.64310001 0.67332186\n",
      "        nan        nan 0.66269656 0.64567817 0.62291218 0.65881259\n",
      " 0.65166548 0.62739352 0.67266387        nan        nan 0.69346199\n",
      " 0.62989036 0.69563455 0.64531406 0.65472402 0.66397798 0.65142696\n",
      "        nan        nan 0.65052158 0.64255671 0.67222906 0.62616904\n",
      " 0.68125226 0.64371142 0.68236613        nan        nan 0.67047514\n",
      " 0.64639891 0.66345196 0.64522941 0.68849077 0.6766214  0.64592103\n",
      "        nan        nan 0.67577295 0.66610923 0.6442062  0.64444223\n",
      " 0.66639719 0.61597079 0.64453889        nan        nan 0.64338483\n",
      " 0.66904429 0.67968428 0.61561352 0.66163565 0.66374269 0.69939737\n",
      "        nan        nan 0.70816477 0.62909153 0.63289843 0.69326672\n",
      " 0.66250545 0.68152636 0.64099103        nan        nan 0.64217642\n",
      " 0.70200464 0.66849138 0.64273423 0.71204948 0.65233622 0.6557\n",
      "        nan        nan 0.66001323 0.69931983 0.63436745 0.65854792\n",
      " 0.65869772 0.67647659 0.65243973        nan        nan 0.72237378\n",
      " 0.68118101 0.64323438 0.61484177 0.65971372 0.70465886 0.62053123\n",
      "        nan        nan 0.68794008 0.6294684  0.70606152 0.66503011\n",
      " 0.66273205 0.69793085 0.6630372         nan        nan 0.65921875\n",
      " 0.64907595 0.64892301 0.61674411 0.55175411 0.64394781 0.63831906\n",
      "        nan        nan 0.68904507 0.65886139 0.62319377 0.58204194\n",
      " 0.60936578 0.68465522 0.69954301        nan        nan 0.63065407\n",
      " 0.60555038 0.62412872 0.67650672 0.64077321 0.61126535 0.66756308\n",
      "        nan        nan 0.62517115 0.60241552 0.63788269 0.63396757\n",
      " 0.61206935 0.63692214 0.67442962        nan        nan 0.62709909\n",
      " 0.59555064 0.58859372 0.61182686 0.56315882 0.64720586 0.62703791\n",
      "        nan        nan 0.6209068  0.69074715 0.61287187 0.62513492\n",
      " 0.60863848 0.62858141 0.60489276        nan        nan 0.63609494\n",
      " 0.61560003 0.58981247 0.65189467 0.61491394 0.64315564 0.61203091\n",
      "        nan        nan 0.5950675  0.55821948 0.59302127 0.66648461\n",
      " 0.61664486 0.65748682 0.55582263        nan        nan 0.62715971\n",
      " 0.50979015 0.64928583 0.64175538 0.58735195 0.6644369  0.58594514\n",
      "        nan        nan 0.66281338 0.59475995 0.52246083 0.64745437\n",
      " 0.50187231 0.60920628 0.5682573         nan        nan 0.64483822\n",
      " 0.5981288  0.65157252 0.59171971 0.62849814 0.6207107  0.6185606\n",
      "        nan        nan 0.71513796 0.5806707  0.66946755 0.54697474\n",
      " 0.70173498 0.54835743 0.61800094        nan        nan 0.5703928\n",
      " 0.6687335  0.6296207  0.62875672 0.63353951 0.65308459 0.62385989\n",
      "        nan        nan 0.64895886 0.66315549 0.66384213 0.64107448\n",
      " 0.6817757  0.67693921 0.62582526        nan        nan 0.59069355\n",
      " 0.66969904 0.69844708 0.65521307 0.62892371 0.63997503 0.6201601\n",
      "        nan        nan 0.6859224  0.64575081 0.62504214 0.67459726\n",
      " 0.67776937 0.68480484 0.62551659        nan        nan 0.62697581\n",
      " 0.69018971 0.60397842 0.67171515 0.65196879 0.66725738 0.59639844\n",
      "        nan        nan 0.65151171 0.67085607 0.65721235 0.60792699\n",
      " 0.64299345 0.64450581 0.66122423        nan        nan 0.61301096]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator saved as: objects/estimators/MLPClassifier-HAB_modelling_5_7-10042023_1012.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>630</th>\n",
       "      <th>518</th>\n",
       "      <th>450</th>\n",
       "      <th>10</th>\n",
       "      <th>478</th>\n",
       "      <th>324</th>\n",
       "      <th>45</th>\n",
       "      <th>28</th>\n",
       "      <th>472</th>\n",
       "      <th>435</th>\n",
       "      <th>111</th>\n",
       "      <th>279</th>\n",
       "      <th>337</th>\n",
       "      <th>467</th>\n",
       "      <th>343</th>\n",
       "      <th>480</th>\n",
       "      <th>...</th>\n",
       "      <th>34</th>\n",
       "      <th>447</th>\n",
       "      <th>448</th>\n",
       "      <th>33</th>\n",
       "      <th>187</th>\n",
       "      <th>456</th>\n",
       "      <th>186</th>\n",
       "      <th>465</th>\n",
       "      <th>466</th>\n",
       "      <th>178</th>\n",
       "      <th>177</th>\n",
       "      <th>25</th>\n",
       "      <th>24</th>\n",
       "      <th>474</th>\n",
       "      <th>475</th>\n",
       "      <th>205</th>\n",
       "      <th>366</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>0.096046</td>\n",
       "      <td>2.236606</td>\n",
       "      <td>0.223405</td>\n",
       "      <td>1.856235</td>\n",
       "      <td>0.121908</td>\n",
       "      <td>2.220821</td>\n",
       "      <td>0.169622</td>\n",
       "      <td>0.25882</td>\n",
       "      <td>0.157731</td>\n",
       "      <td>1.982491</td>\n",
       "      <td>1.636988</td>\n",
       "      <td>1.415834</td>\n",
       "      <td>2.012022</td>\n",
       "      <td>0.145217</td>\n",
       "      <td>2.569585</td>\n",
       "      <td>0.123824</td>\n",
       "      <td>1.959548</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013454</td>\n",
       "      <td>0.015102</td>\n",
       "      <td>0.015454</td>\n",
       "      <td>0.013627</td>\n",
       "      <td>0.016792</td>\n",
       "      <td>0.015104</td>\n",
       "      <td>0.015972</td>\n",
       "      <td>0.014783</td>\n",
       "      <td>0.015349</td>\n",
       "      <td>0.015387</td>\n",
       "      <td>0.016693</td>\n",
       "      <td>0.013597</td>\n",
       "      <td>0.013714</td>\n",
       "      <td>0.014947</td>\n",
       "      <td>0.015416</td>\n",
       "      <td>0.015197</td>\n",
       "      <td>0.01451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.018244</td>\n",
       "      <td>0.386596</td>\n",
       "      <td>0.023009</td>\n",
       "      <td>0.133278</td>\n",
       "      <td>0.032731</td>\n",
       "      <td>0.306948</td>\n",
       "      <td>0.02184</td>\n",
       "      <td>0.135135</td>\n",
       "      <td>0.059129</td>\n",
       "      <td>0.515691</td>\n",
       "      <td>0.234323</td>\n",
       "      <td>0.247048</td>\n",
       "      <td>0.312014</td>\n",
       "      <td>0.016811</td>\n",
       "      <td>0.354952</td>\n",
       "      <td>0.006223</td>\n",
       "      <td>0.316683</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.00039</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.00014</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>0.00014</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.009466</td>\n",
       "      <td>0.010885</td>\n",
       "      <td>0.010978</td>\n",
       "      <td>0.010696</td>\n",
       "      <td>0.00981</td>\n",
       "      <td>0.010801</td>\n",
       "      <td>0.011024</td>\n",
       "      <td>0.009896</td>\n",
       "      <td>0.010055</td>\n",
       "      <td>0.010983</td>\n",
       "      <td>0.01005</td>\n",
       "      <td>0.010511</td>\n",
       "      <td>0.010882</td>\n",
       "      <td>0.010608</td>\n",
       "      <td>0.010891</td>\n",
       "      <td>0.010594</td>\n",
       "      <td>0.011542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.00044</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__hidden_layer_sizes</th>\n",
       "      <td>(2,)</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(2,)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(2,)</td>\n",
       "      <td>(2,)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(2,)</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>...</td>\n",
       "      <td>(2,)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(2,)</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>(2,)</td>\n",
       "      <td>(2,)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(3,)</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>(3,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_clf__solver</th>\n",
       "      <td>lbfgs</td>\n",
       "      <td>adam</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>adam</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>adam</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>adam</td>\n",
       "      <td>adam</td>\n",
       "      <td>adam</td>\n",
       "      <td>adam</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>adam</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>adam</td>\n",
       "      <td>...</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>adam</td>\n",
       "      <td>adam</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>adam</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>adam</td>\n",
       "      <td>adam</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>adam</td>\n",
       "      <td>adam</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>lbfgs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_over__k_neighbors</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_over__sampling_strategy</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_under__sampling_strategy</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'clf__hidden_layer_sizes': (2,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3, 3), 'clf__solv...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3, 3), 'clf__solv...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (2,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (2,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (2,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (2,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (2, 2), 'clf__solv...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (2,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (2,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (2, 2), 'clf__solv...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (2, 2), 'clf__solv...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (2, 2), 'clf__solv...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (2, 2), 'clf__solv...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (2,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (2,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (2, 2), 'clf__solv...</td>\n",
       "      <td>{'clf__hidden_layer_sizes': (3,), 'clf__solver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_recall</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.55</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_recall</th>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_recall</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.47619</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_recall</th>\n",
       "      <td>0.484921</td>\n",
       "      <td>0.548413</td>\n",
       "      <td>0.565873</td>\n",
       "      <td>0.548413</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>0.515079</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.453175</td>\n",
       "      <td>0.51746</td>\n",
       "      <td>0.565079</td>\n",
       "      <td>0.469048</td>\n",
       "      <td>0.484127</td>\n",
       "      <td>0.500794</td>\n",
       "      <td>0.437302</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.453175</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_recall</th>\n",
       "      <td>0.049956</td>\n",
       "      <td>0.019473</td>\n",
       "      <td>0.071066</td>\n",
       "      <td>0.019473</td>\n",
       "      <td>0.080812</td>\n",
       "      <td>0.049956</td>\n",
       "      <td>0.030553</td>\n",
       "      <td>0.071172</td>\n",
       "      <td>0.097228</td>\n",
       "      <td>0.100289</td>\n",
       "      <td>0.069198</td>\n",
       "      <td>0.040468</td>\n",
       "      <td>0.034794</td>\n",
       "      <td>0.088669</td>\n",
       "      <td>0.097202</td>\n",
       "      <td>0.030553</td>\n",
       "      <td>0.071172</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_recall</th>\n",
       "      <td>108</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>96</td>\n",
       "      <td>62</td>\n",
       "      <td>119</td>\n",
       "      <td>183</td>\n",
       "      <td>56</td>\n",
       "      <td>12</td>\n",
       "      <td>138</td>\n",
       "      <td>113</td>\n",
       "      <td>75</td>\n",
       "      <td>230</td>\n",
       "      <td>76</td>\n",
       "      <td>119</td>\n",
       "      <td>183</td>\n",
       "      <td>...</td>\n",
       "      <td>608</td>\n",
       "      <td>634</td>\n",
       "      <td>636</td>\n",
       "      <td>610</td>\n",
       "      <td>542</td>\n",
       "      <td>637</td>\n",
       "      <td>532</td>\n",
       "      <td>642</td>\n",
       "      <td>643</td>\n",
       "      <td>528</td>\n",
       "      <td>527</td>\n",
       "      <td>541</td>\n",
       "      <td>544</td>\n",
       "      <td>645</td>\n",
       "      <td>646</td>\n",
       "      <td>612</td>\n",
       "      <td>553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_precision</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.323529</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.34375</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_precision</th>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.40625</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_precision</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.243902</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.25641</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.358974</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.37037</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.297297</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_precision</th>\n",
       "      <td>0.422705</td>\n",
       "      <td>0.385621</td>\n",
       "      <td>0.37423</td>\n",
       "      <td>0.369895</td>\n",
       "      <td>0.393506</td>\n",
       "      <td>0.369444</td>\n",
       "      <td>0.396317</td>\n",
       "      <td>0.419577</td>\n",
       "      <td>0.362173</td>\n",
       "      <td>0.334896</td>\n",
       "      <td>0.385854</td>\n",
       "      <td>0.366319</td>\n",
       "      <td>0.34951</td>\n",
       "      <td>0.390919</td>\n",
       "      <td>0.347181</td>\n",
       "      <td>0.36254</td>\n",
       "      <td>0.371429</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_precision</th>\n",
       "      <td>0.068575</td>\n",
       "      <td>0.080977</td>\n",
       "      <td>0.126459</td>\n",
       "      <td>0.037607</td>\n",
       "      <td>0.036687</td>\n",
       "      <td>0.087753</td>\n",
       "      <td>0.154185</td>\n",
       "      <td>0.07783</td>\n",
       "      <td>0.063498</td>\n",
       "      <td>0.034779</td>\n",
       "      <td>0.072214</td>\n",
       "      <td>0.064154</td>\n",
       "      <td>0.015479</td>\n",
       "      <td>0.075217</td>\n",
       "      <td>0.079157</td>\n",
       "      <td>0.097241</td>\n",
       "      <td>0.035475</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_precision</th>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>36</td>\n",
       "      <td>84</td>\n",
       "      <td>15</td>\n",
       "      <td>31</td>\n",
       "      <td>53</td>\n",
       "      <td>13</td>\n",
       "      <td>57</td>\n",
       "      <td>35</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>643</td>\n",
       "      <td>631</td>\n",
       "      <td>632</td>\n",
       "      <td>624</td>\n",
       "      <td>603</td>\n",
       "      <td>636</td>\n",
       "      <td>606</td>\n",
       "      <td>639</td>\n",
       "      <td>640</td>\n",
       "      <td>607</td>\n",
       "      <td>611</td>\n",
       "      <td>610</td>\n",
       "      <td>612</td>\n",
       "      <td>644</td>\n",
       "      <td>645</td>\n",
       "      <td>648</td>\n",
       "      <td>553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_f1</th>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.440678</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.436364</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.37037</td>\n",
       "      <td>0.415094</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_f1</th>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.55814</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>0.390244</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.490566</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.390244</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_f1</th>\n",
       "      <td>0.375</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.369231</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.311111</td>\n",
       "      <td>0.290909</td>\n",
       "      <td>0.37931</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_f1</th>\n",
       "      <td>0.451118</td>\n",
       "      <td>0.446696</td>\n",
       "      <td>0.440466</td>\n",
       "      <td>0.439911</td>\n",
       "      <td>0.427847</td>\n",
       "      <td>0.426969</td>\n",
       "      <td>0.424145</td>\n",
       "      <td>0.423932</td>\n",
       "      <td>0.416424</td>\n",
       "      <td>0.415023</td>\n",
       "      <td>0.414341</td>\n",
       "      <td>0.410824</td>\n",
       "      <td>0.410587</td>\n",
       "      <td>0.409732</td>\n",
       "      <td>0.40864</td>\n",
       "      <td>0.406685</td>\n",
       "      <td>0.405308</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_f1</th>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.046251</td>\n",
       "      <td>0.096167</td>\n",
       "      <td>0.020048</td>\n",
       "      <td>0.007268</td>\n",
       "      <td>0.075001</td>\n",
       "      <td>0.100416</td>\n",
       "      <td>0.02692</td>\n",
       "      <td>0.052587</td>\n",
       "      <td>0.036558</td>\n",
       "      <td>0.03895</td>\n",
       "      <td>0.029337</td>\n",
       "      <td>0.007514</td>\n",
       "      <td>0.070125</td>\n",
       "      <td>0.085351</td>\n",
       "      <td>0.058362</td>\n",
       "      <td>0.038642</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_f1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>632</td>\n",
       "      <td>633</td>\n",
       "      <td>634</td>\n",
       "      <td>635</td>\n",
       "      <td>636</td>\n",
       "      <td>637</td>\n",
       "      <td>638</td>\n",
       "      <td>639</td>\n",
       "      <td>640</td>\n",
       "      <td>641</td>\n",
       "      <td>642</td>\n",
       "      <td>643</td>\n",
       "      <td>644</td>\n",
       "      <td>645</td>\n",
       "      <td>646</td>\n",
       "      <td>647</td>\n",
       "      <td>648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_roc_auc</th>\n",
       "      <td>0.709649</td>\n",
       "      <td>0.644737</td>\n",
       "      <td>0.680263</td>\n",
       "      <td>0.672807</td>\n",
       "      <td>0.678947</td>\n",
       "      <td>0.617763</td>\n",
       "      <td>0.602193</td>\n",
       "      <td>0.696491</td>\n",
       "      <td>0.709649</td>\n",
       "      <td>0.714693</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.673246</td>\n",
       "      <td>0.690789</td>\n",
       "      <td>0.719298</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.657018</td>\n",
       "      <td>0.67193</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_roc_auc</th>\n",
       "      <td>0.66772</td>\n",
       "      <td>0.741888</td>\n",
       "      <td>0.750316</td>\n",
       "      <td>0.780868</td>\n",
       "      <td>0.67657</td>\n",
       "      <td>0.750316</td>\n",
       "      <td>0.681837</td>\n",
       "      <td>0.612937</td>\n",
       "      <td>0.651707</td>\n",
       "      <td>0.633165</td>\n",
       "      <td>0.616098</td>\n",
       "      <td>0.702065</td>\n",
       "      <td>0.68268</td>\n",
       "      <td>0.663928</td>\n",
       "      <td>0.743784</td>\n",
       "      <td>0.758323</td>\n",
       "      <td>0.642014</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_roc_auc</th>\n",
       "      <td>0.672777</td>\n",
       "      <td>0.683944</td>\n",
       "      <td>0.59271</td>\n",
       "      <td>0.652339</td>\n",
       "      <td>0.683734</td>\n",
       "      <td>0.750105</td>\n",
       "      <td>0.60472</td>\n",
       "      <td>0.698272</td>\n",
       "      <td>0.686894</td>\n",
       "      <td>0.766119</td>\n",
       "      <td>0.710914</td>\n",
       "      <td>0.750105</td>\n",
       "      <td>0.711336</td>\n",
       "      <td>0.691951</td>\n",
       "      <td>0.712811</td>\n",
       "      <td>0.674041</td>\n",
       "      <td>0.674252</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_roc_auc</th>\n",
       "      <td>0.683382</td>\n",
       "      <td>0.69019</td>\n",
       "      <td>0.67443</td>\n",
       "      <td>0.702005</td>\n",
       "      <td>0.67975</td>\n",
       "      <td>0.706062</td>\n",
       "      <td>0.629583</td>\n",
       "      <td>0.669234</td>\n",
       "      <td>0.68275</td>\n",
       "      <td>0.704659</td>\n",
       "      <td>0.661636</td>\n",
       "      <td>0.708472</td>\n",
       "      <td>0.694935</td>\n",
       "      <td>0.691726</td>\n",
       "      <td>0.722374</td>\n",
       "      <td>0.696461</td>\n",
       "      <td>0.662732</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_roc_auc</th>\n",
       "      <td>0.018688</td>\n",
       "      <td>0.039907</td>\n",
       "      <td>0.064475</td>\n",
       "      <td>0.056387</td>\n",
       "      <td>0.002979</td>\n",
       "      <td>0.062436</td>\n",
       "      <td>0.036964</td>\n",
       "      <td>0.039814</td>\n",
       "      <td>0.023836</td>\n",
       "      <td>0.05474</td>\n",
       "      <td>0.038799</td>\n",
       "      <td>0.031703</td>\n",
       "      <td>0.01206</td>\n",
       "      <td>0.022606</td>\n",
       "      <td>0.015168</td>\n",
       "      <td>0.044292</td>\n",
       "      <td>0.01468</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_roc_auc</th>\n",
       "      <td>48</td>\n",
       "      <td>35</td>\n",
       "      <td>76</td>\n",
       "      <td>14</td>\n",
       "      <td>60</td>\n",
       "      <td>11</td>\n",
       "      <td>288</td>\n",
       "      <td>98</td>\n",
       "      <td>49</td>\n",
       "      <td>13</td>\n",
       "      <td>134</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>128</td>\n",
       "      <td>...</td>\n",
       "      <td>587</td>\n",
       "      <td>617</td>\n",
       "      <td>616</td>\n",
       "      <td>643</td>\n",
       "      <td>634</td>\n",
       "      <td>642</td>\n",
       "      <td>614</td>\n",
       "      <td>635</td>\n",
       "      <td>633</td>\n",
       "      <td>567</td>\n",
       "      <td>574</td>\n",
       "      <td>534</td>\n",
       "      <td>520</td>\n",
       "      <td>589</td>\n",
       "      <td>588</td>\n",
       "      <td>535</td>\n",
       "      <td>551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34 rows × 648 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              2    \\\n",
       "mean_fit_time                                                            0.096046   \n",
       "std_fit_time                                                             0.018244   \n",
       "mean_score_time                                                          0.009466   \n",
       "std_score_time                                                           0.000288   \n",
       "param_clf__hidden_layer_sizes                                                (2,)   \n",
       "param_clf__solver                                                           lbfgs   \n",
       "param_over__k_neighbors                                                         1   \n",
       "param_over__sampling_strategy                                                 0.5   \n",
       "param_under__sampling_strategy                                                0.8   \n",
       "params                          {'clf__hidden_layer_sizes': (2,), 'clf__solver...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                       0.428571   \n",
       "mean_test_recall                                                         0.484921   \n",
       "std_test_recall                                                          0.049956   \n",
       "rank_test_recall                                                              108   \n",
       "split0_test_precision                                                         0.5   \n",
       "split1_test_precision                                                    0.434783   \n",
       "split2_test_precision                                                    0.333333   \n",
       "mean_test_precision                                                      0.422705   \n",
       "std_test_precision                                                       0.068575   \n",
       "rank_test_precision                                                             4   \n",
       "split0_test_f1                                                            0.52381   \n",
       "split1_test_f1                                                           0.454545   \n",
       "split2_test_f1                                                              0.375   \n",
       "mean_test_f1                                                             0.451118   \n",
       "std_test_f1                                                                0.0608   \n",
       "rank_test_f1                                                                    1   \n",
       "split0_test_roc_auc                                                      0.709649   \n",
       "split1_test_roc_auc                                                       0.66772   \n",
       "split2_test_roc_auc                                                      0.672777   \n",
       "mean_test_roc_auc                                                        0.683382   \n",
       "std_test_roc_auc                                                         0.018688   \n",
       "rank_test_roc_auc                                                              48   \n",
       "\n",
       "                                                                              630  \\\n",
       "mean_fit_time                                                            2.236606   \n",
       "std_fit_time                                                             0.386596   \n",
       "mean_score_time                                                          0.010885   \n",
       "std_score_time                                                           0.000782   \n",
       "param_clf__hidden_layer_sizes                                              (3, 3)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         5   \n",
       "param_over__sampling_strategy                                                 0.5   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__hidden_layer_sizes': (3, 3), 'clf__solv...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                        0.52381   \n",
       "split2_test_recall                                                       0.571429   \n",
       "mean_test_recall                                                         0.548413   \n",
       "std_test_recall                                                          0.019473   \n",
       "rank_test_recall                                                               25   \n",
       "split0_test_precision                                                    0.323529   \n",
       "split1_test_precision                                                         0.5   \n",
       "split2_test_precision                                                    0.333333   \n",
       "mean_test_precision                                                      0.385621   \n",
       "std_test_precision                                                       0.080977   \n",
       "rank_test_precision                                                            16   \n",
       "split0_test_f1                                                           0.407407   \n",
       "split1_test_f1                                                           0.511628   \n",
       "split2_test_f1                                                           0.421053   \n",
       "mean_test_f1                                                             0.446696   \n",
       "std_test_f1                                                              0.046251   \n",
       "rank_test_f1                                                                    2   \n",
       "split0_test_roc_auc                                                      0.644737   \n",
       "split1_test_roc_auc                                                      0.741888   \n",
       "split2_test_roc_auc                                                      0.683944   \n",
       "mean_test_roc_auc                                                         0.69019   \n",
       "std_test_roc_auc                                                         0.039907   \n",
       "rank_test_roc_auc                                                              35   \n",
       "\n",
       "                                                                              518  \\\n",
       "mean_fit_time                                                            0.223405   \n",
       "std_fit_time                                                             0.023009   \n",
       "mean_score_time                                                          0.010978   \n",
       "std_score_time                                                           0.000415   \n",
       "param_clf__hidden_layer_sizes                                              (3, 3)   \n",
       "param_clf__solver                                                           lbfgs   \n",
       "param_over__k_neighbors                                                         4   \n",
       "param_over__sampling_strategy                                                 0.6   \n",
       "param_under__sampling_strategy                                                0.8   \n",
       "params                          {'clf__hidden_layer_sizes': (3, 3), 'clf__solv...   \n",
       "split0_test_recall                                                           0.65   \n",
       "split1_test_recall                                                       0.571429   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.565873   \n",
       "std_test_recall                                                          0.071066   \n",
       "rank_test_recall                                                                7   \n",
       "split0_test_precision                                                    0.333333   \n",
       "split1_test_precision                                                    0.545455   \n",
       "split2_test_precision                                                    0.243902   \n",
       "mean_test_precision                                                       0.37423   \n",
       "std_test_precision                                                       0.126459   \n",
       "rank_test_precision                                                            20   \n",
       "split0_test_f1                                                           0.440678   \n",
       "split1_test_f1                                                            0.55814   \n",
       "split2_test_f1                                                           0.322581   \n",
       "mean_test_f1                                                             0.440466   \n",
       "std_test_f1                                                              0.096167   \n",
       "rank_test_f1                                                                    3   \n",
       "split0_test_roc_auc                                                      0.680263   \n",
       "split1_test_roc_auc                                                      0.750316   \n",
       "split2_test_roc_auc                                                       0.59271   \n",
       "mean_test_roc_auc                                                         0.67443   \n",
       "std_test_roc_auc                                                         0.064475   \n",
       "rank_test_roc_auc                                                              76   \n",
       "\n",
       "                                                                              450  \\\n",
       "mean_fit_time                                                            1.856235   \n",
       "std_fit_time                                                             0.133278   \n",
       "mean_score_time                                                          0.010696   \n",
       "std_score_time                                                           0.000008   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         3   \n",
       "param_over__sampling_strategy                                                 0.5   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                        0.52381   \n",
       "split2_test_recall                                                       0.571429   \n",
       "mean_test_recall                                                         0.548413   \n",
       "std_test_recall                                                          0.019473   \n",
       "rank_test_recall                                                               25   \n",
       "split0_test_precision                                                     0.34375   \n",
       "split1_test_precision                                                    0.423077   \n",
       "split2_test_precision                                                    0.342857   \n",
       "mean_test_precision                                                      0.369895   \n",
       "std_test_precision                                                       0.037607   \n",
       "rank_test_precision                                                            26   \n",
       "split0_test_f1                                                           0.423077   \n",
       "split1_test_f1                                                           0.468085   \n",
       "split2_test_f1                                                           0.428571   \n",
       "mean_test_f1                                                             0.439911   \n",
       "std_test_f1                                                              0.020048   \n",
       "rank_test_f1                                                                    4   \n",
       "split0_test_roc_auc                                                      0.672807   \n",
       "split1_test_roc_auc                                                      0.780868   \n",
       "split2_test_roc_auc                                                      0.652339   \n",
       "mean_test_roc_auc                                                        0.702005   \n",
       "std_test_roc_auc                                                         0.056387   \n",
       "rank_test_roc_auc                                                              14   \n",
       "\n",
       "                                                                              10   \\\n",
       "mean_fit_time                                                            0.121908   \n",
       "std_fit_time                                                             0.032731   \n",
       "mean_score_time                                                           0.00981   \n",
       "std_score_time                                                           0.000031   \n",
       "param_clf__hidden_layer_sizes                                                (2,)   \n",
       "param_clf__solver                                                           lbfgs   \n",
       "param_over__k_neighbors                                                         2   \n",
       "param_over__sampling_strategy                                                 0.5   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__hidden_layer_sizes': (2,), 'clf__solver...   \n",
       "split0_test_recall                                                            0.6   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                       0.428571   \n",
       "mean_test_recall                                                         0.485714   \n",
       "std_test_recall                                                          0.080812   \n",
       "rank_test_recall                                                               96   \n",
       "split0_test_precision                                                    0.342857   \n",
       "split1_test_precision                                                    0.428571   \n",
       "split2_test_precision                                                    0.409091   \n",
       "mean_test_precision                                                      0.393506   \n",
       "std_test_precision                                                       0.036687   \n",
       "rank_test_precision                                                            12   \n",
       "split0_test_f1                                                           0.436364   \n",
       "split1_test_f1                                                           0.428571   \n",
       "split2_test_f1                                                           0.418605   \n",
       "mean_test_f1                                                             0.427847   \n",
       "std_test_f1                                                              0.007268   \n",
       "rank_test_f1                                                                    5   \n",
       "split0_test_roc_auc                                                      0.678947   \n",
       "split1_test_roc_auc                                                       0.67657   \n",
       "split2_test_roc_auc                                                      0.683734   \n",
       "mean_test_roc_auc                                                         0.67975   \n",
       "std_test_roc_auc                                                         0.002979   \n",
       "rank_test_roc_auc                                                              60   \n",
       "\n",
       "                                                                              478  \\\n",
       "mean_fit_time                                                            2.220821   \n",
       "std_fit_time                                                             0.306948   \n",
       "mean_score_time                                                          0.010801   \n",
       "std_score_time                                                           0.000052   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         6   \n",
       "param_over__sampling_strategy                                                 0.5   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                        0.52381   \n",
       "split2_test_recall                                                       0.571429   \n",
       "mean_test_recall                                                         0.515079   \n",
       "std_test_recall                                                          0.049956   \n",
       "rank_test_recall                                                               62   \n",
       "split0_test_precision                                                        0.25   \n",
       "split1_test_precision                                                    0.458333   \n",
       "split2_test_precision                                                         0.4   \n",
       "mean_test_precision                                                      0.369444   \n",
       "std_test_precision                                                       0.087753   \n",
       "rank_test_precision                                                            27   \n",
       "split0_test_f1                                                           0.321429   \n",
       "split1_test_f1                                                           0.488889   \n",
       "split2_test_f1                                                           0.470588   \n",
       "mean_test_f1                                                             0.426969   \n",
       "std_test_f1                                                              0.075001   \n",
       "rank_test_f1                                                                    6   \n",
       "split0_test_roc_auc                                                      0.617763   \n",
       "split1_test_roc_auc                                                      0.750316   \n",
       "split2_test_roc_auc                                                      0.750105   \n",
       "mean_test_roc_auc                                                        0.706062   \n",
       "std_test_roc_auc                                                         0.062436   \n",
       "rank_test_roc_auc                                                              11   \n",
       "\n",
       "                                                                              324  \\\n",
       "mean_fit_time                                                            0.169622   \n",
       "std_fit_time                                                              0.02184   \n",
       "mean_score_time                                                          0.011024   \n",
       "std_score_time                                                            0.00044   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                           lbfgs   \n",
       "param_over__k_neighbors                                                         1   \n",
       "param_over__sampling_strategy                                                 0.5   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                        0.52381   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.483333   \n",
       "std_test_recall                                                          0.030553   \n",
       "rank_test_recall                                                              119   \n",
       "split0_test_precision                                                    0.321429   \n",
       "split1_test_precision                                                    0.611111   \n",
       "split2_test_precision                                                     0.25641   \n",
       "mean_test_precision                                                      0.396317   \n",
       "std_test_precision                                                       0.154185   \n",
       "rank_test_precision                                                            10   \n",
       "split0_test_f1                                                              0.375   \n",
       "split1_test_f1                                                           0.564103   \n",
       "split2_test_f1                                                           0.333333   \n",
       "mean_test_f1                                                             0.424145   \n",
       "std_test_f1                                                              0.100416   \n",
       "rank_test_f1                                                                    7   \n",
       "split0_test_roc_auc                                                      0.602193   \n",
       "split1_test_roc_auc                                                      0.681837   \n",
       "split2_test_roc_auc                                                       0.60472   \n",
       "mean_test_roc_auc                                                        0.629583   \n",
       "std_test_roc_auc                                                         0.036964   \n",
       "rank_test_roc_auc                                                             288   \n",
       "\n",
       "                                                                              45   \\\n",
       "mean_fit_time                                                             0.25882   \n",
       "std_fit_time                                                             0.135135   \n",
       "mean_score_time                                                          0.009896   \n",
       "std_score_time                                                           0.000033   \n",
       "param_clf__hidden_layer_sizes                                                (2,)   \n",
       "param_clf__solver                                                           lbfgs   \n",
       "param_over__k_neighbors                                                         6   \n",
       "param_over__sampling_strategy                                                 0.5   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__hidden_layer_sizes': (2,), 'clf__solver...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                       0.380952   \n",
       "split2_test_recall                                                       0.428571   \n",
       "mean_test_recall                                                         0.453175   \n",
       "std_test_recall                                                          0.071172   \n",
       "rank_test_recall                                                              183   \n",
       "split0_test_precision                                                    0.314286   \n",
       "split1_test_precision                                                    0.444444   \n",
       "split2_test_precision                                                         0.5   \n",
       "mean_test_precision                                                      0.419577   \n",
       "std_test_precision                                                        0.07783   \n",
       "rank_test_precision                                                             5   \n",
       "split0_test_f1                                                                0.4   \n",
       "split1_test_f1                                                           0.410256   \n",
       "split2_test_f1                                                           0.461538   \n",
       "mean_test_f1                                                             0.423932   \n",
       "std_test_f1                                                               0.02692   \n",
       "rank_test_f1                                                                    8   \n",
       "split0_test_roc_auc                                                      0.696491   \n",
       "split1_test_roc_auc                                                      0.612937   \n",
       "split2_test_roc_auc                                                      0.698272   \n",
       "mean_test_roc_auc                                                        0.669234   \n",
       "std_test_roc_auc                                                         0.039814   \n",
       "rank_test_roc_auc                                                              98   \n",
       "\n",
       "                                                                              28   \\\n",
       "mean_fit_time                                                            0.157731   \n",
       "std_fit_time                                                             0.059129   \n",
       "mean_score_time                                                          0.010055   \n",
       "std_score_time                                                           0.000067   \n",
       "param_clf__hidden_layer_sizes                                                (2,)   \n",
       "param_clf__solver                                                           lbfgs   \n",
       "param_over__k_neighbors                                                         4   \n",
       "param_over__sampling_strategy                                                 0.5   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__hidden_layer_sizes': (2,), 'clf__solver...   \n",
       "split0_test_recall                                                            0.6   \n",
       "split1_test_recall                                                       0.380952   \n",
       "split2_test_recall                                                       0.571429   \n",
       "mean_test_recall                                                          0.51746   \n",
       "std_test_recall                                                          0.097228   \n",
       "rank_test_recall                                                               56   \n",
       "split0_test_precision                                                    0.413793   \n",
       "split1_test_precision                                                         0.4   \n",
       "split2_test_precision                                                    0.272727   \n",
       "mean_test_precision                                                      0.362173   \n",
       "std_test_precision                                                       0.063498   \n",
       "rank_test_precision                                                            36   \n",
       "split0_test_f1                                                           0.489796   \n",
       "split1_test_f1                                                           0.390244   \n",
       "split2_test_f1                                                           0.369231   \n",
       "mean_test_f1                                                             0.416424   \n",
       "std_test_f1                                                              0.052587   \n",
       "rank_test_f1                                                                    9   \n",
       "split0_test_roc_auc                                                      0.709649   \n",
       "split1_test_roc_auc                                                      0.651707   \n",
       "split2_test_roc_auc                                                      0.686894   \n",
       "mean_test_roc_auc                                                         0.68275   \n",
       "std_test_roc_auc                                                         0.023836   \n",
       "rank_test_roc_auc                                                              49   \n",
       "\n",
       "                                                                              472  \\\n",
       "mean_fit_time                                                            1.982491   \n",
       "std_fit_time                                                             0.515691   \n",
       "mean_score_time                                                          0.010983   \n",
       "std_score_time                                                           0.000138   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         5   \n",
       "param_over__sampling_strategy                                                 0.6   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                            0.6   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                       0.666667   \n",
       "mean_test_recall                                                         0.565079   \n",
       "std_test_recall                                                          0.100289   \n",
       "rank_test_recall                                                               12   \n",
       "split0_test_precision                                                    0.285714   \n",
       "split1_test_precision                                                        0.36   \n",
       "split2_test_precision                                                    0.358974   \n",
       "mean_test_precision                                                      0.334896   \n",
       "std_test_precision                                                       0.034779   \n",
       "rank_test_precision                                                            84   \n",
       "split0_test_f1                                                           0.387097   \n",
       "split1_test_f1                                                           0.391304   \n",
       "split2_test_f1                                                           0.466667   \n",
       "mean_test_f1                                                             0.415023   \n",
       "std_test_f1                                                              0.036558   \n",
       "rank_test_f1                                                                   10   \n",
       "split0_test_roc_auc                                                      0.714693   \n",
       "split1_test_roc_auc                                                      0.633165   \n",
       "split2_test_roc_auc                                                      0.766119   \n",
       "mean_test_roc_auc                                                        0.704659   \n",
       "std_test_roc_auc                                                          0.05474   \n",
       "rank_test_roc_auc                                                              13   \n",
       "\n",
       "                                                                              435  \\\n",
       "mean_fit_time                                                            1.636988   \n",
       "std_fit_time                                                             0.234323   \n",
       "mean_score_time                                                           0.01005   \n",
       "std_score_time                                                           0.000951   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         1   \n",
       "param_over__sampling_strategy                                                 0.6   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                       0.380952   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.469048   \n",
       "std_test_recall                                                          0.069198   \n",
       "rank_test_recall                                                              138   \n",
       "split0_test_precision                                                    0.392857   \n",
       "split1_test_precision                                                    0.470588   \n",
       "split2_test_precision                                                    0.294118   \n",
       "mean_test_precision                                                      0.385854   \n",
       "std_test_precision                                                       0.072214   \n",
       "rank_test_precision                                                            15   \n",
       "split0_test_f1                                                           0.458333   \n",
       "split1_test_f1                                                           0.421053   \n",
       "split2_test_f1                                                           0.363636   \n",
       "mean_test_f1                                                             0.414341   \n",
       "std_test_f1                                                               0.03895   \n",
       "rank_test_f1                                                                   11   \n",
       "split0_test_roc_auc                                                      0.657895   \n",
       "split1_test_roc_auc                                                      0.616098   \n",
       "split2_test_roc_auc                                                      0.710914   \n",
       "mean_test_roc_auc                                                        0.661636   \n",
       "std_test_roc_auc                                                         0.038799   \n",
       "rank_test_roc_auc                                                             134   \n",
       "\n",
       "                                                                              111  \\\n",
       "mean_fit_time                                                            1.415834   \n",
       "std_fit_time                                                             0.247048   \n",
       "mean_score_time                                                          0.010511   \n",
       "std_score_time                                                           0.000042   \n",
       "param_clf__hidden_layer_sizes                                                (2,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         1   \n",
       "param_over__sampling_strategy                                                 0.6   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__hidden_layer_sizes': (2,), 'clf__solver...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.484127   \n",
       "std_test_recall                                                          0.040468   \n",
       "rank_test_recall                                                              113   \n",
       "split0_test_precision                                                    0.294118   \n",
       "split1_test_precision                                                        0.45   \n",
       "split2_test_precision                                                    0.354839   \n",
       "mean_test_precision                                                      0.366319   \n",
       "std_test_precision                                                       0.064154   \n",
       "rank_test_precision                                                            31   \n",
       "split0_test_f1                                                            0.37037   \n",
       "split1_test_f1                                                           0.439024   \n",
       "split2_test_f1                                                           0.423077   \n",
       "mean_test_f1                                                             0.410824   \n",
       "std_test_f1                                                              0.029337   \n",
       "rank_test_f1                                                                   12   \n",
       "split0_test_roc_auc                                                      0.673246   \n",
       "split1_test_roc_auc                                                      0.702065   \n",
       "split2_test_roc_auc                                                      0.750105   \n",
       "mean_test_roc_auc                                                        0.708472   \n",
       "std_test_roc_auc                                                         0.031703   \n",
       "rank_test_roc_auc                                                               8   \n",
       "\n",
       "                                                                              279  \\\n",
       "mean_fit_time                                                            2.012022   \n",
       "std_fit_time                                                             0.312014   \n",
       "mean_score_time                                                          0.010882   \n",
       "std_score_time                                                           0.000078   \n",
       "param_clf__hidden_layer_sizes                                              (2, 2)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         2   \n",
       "param_over__sampling_strategy                                                 0.5   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__hidden_layer_sizes': (2, 2), 'clf__solv...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                        0.47619   \n",
       "mean_test_recall                                                         0.500794   \n",
       "std_test_recall                                                          0.034794   \n",
       "rank_test_recall                                                               75   \n",
       "split0_test_precision                                                    0.333333   \n",
       "split1_test_precision                                                    0.344828   \n",
       "split2_test_precision                                                     0.37037   \n",
       "mean_test_precision                                                       0.34951   \n",
       "std_test_precision                                                       0.015479   \n",
       "rank_test_precision                                                            53   \n",
       "split0_test_f1                                                           0.415094   \n",
       "split1_test_f1                                                                0.4   \n",
       "split2_test_f1                                                           0.416667   \n",
       "mean_test_f1                                                             0.410587   \n",
       "std_test_f1                                                              0.007514   \n",
       "rank_test_f1                                                                   13   \n",
       "split0_test_roc_auc                                                      0.690789   \n",
       "split1_test_roc_auc                                                       0.68268   \n",
       "split2_test_roc_auc                                                      0.711336   \n",
       "mean_test_roc_auc                                                        0.694935   \n",
       "std_test_roc_auc                                                          0.01206   \n",
       "rank_test_roc_auc                                                              28   \n",
       "\n",
       "                                                                              337  \\\n",
       "mean_fit_time                                                            0.145217   \n",
       "std_fit_time                                                             0.016811   \n",
       "mean_score_time                                                          0.010608   \n",
       "std_score_time                                                           0.000158   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                           lbfgs   \n",
       "param_over__k_neighbors                                                         2   \n",
       "param_over__sampling_strategy                                                 0.6   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                       0.428571   \n",
       "split2_test_recall                                                       0.333333   \n",
       "mean_test_recall                                                         0.437302   \n",
       "std_test_recall                                                          0.088669   \n",
       "rank_test_recall                                                              230   \n",
       "split0_test_precision                                                    0.407407   \n",
       "split1_test_precision                                                    0.473684   \n",
       "split2_test_precision                                                    0.291667   \n",
       "mean_test_precision                                                      0.390919   \n",
       "std_test_precision                                                       0.075217   \n",
       "rank_test_precision                                                            13   \n",
       "split0_test_f1                                                           0.468085   \n",
       "split1_test_f1                                                               0.45   \n",
       "split2_test_f1                                                           0.311111   \n",
       "mean_test_f1                                                             0.409732   \n",
       "std_test_f1                                                              0.070125   \n",
       "rank_test_f1                                                                   14   \n",
       "split0_test_roc_auc                                                      0.719298   \n",
       "split1_test_roc_auc                                                      0.663928   \n",
       "split2_test_roc_auc                                                      0.691951   \n",
       "mean_test_roc_auc                                                        0.691726   \n",
       "std_test_roc_auc                                                         0.022606   \n",
       "rank_test_roc_auc                                                              32   \n",
       "\n",
       "                                                                              467  \\\n",
       "mean_fit_time                                                            2.569585   \n",
       "std_fit_time                                                             0.354952   \n",
       "mean_score_time                                                          0.010891   \n",
       "std_score_time                                                           0.000452   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         4   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.8   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                            0.5   \n",
       "split1_test_recall                                                       0.619048   \n",
       "split2_test_recall                                                       0.380952   \n",
       "mean_test_recall                                                              0.5   \n",
       "std_test_recall                                                          0.097202   \n",
       "rank_test_recall                                                               76   \n",
       "split0_test_precision                                                         0.4   \n",
       "split1_test_precision                                                     0.40625   \n",
       "split2_test_precision                                                    0.235294   \n",
       "mean_test_precision                                                      0.347181   \n",
       "std_test_precision                                                       0.079157   \n",
       "rank_test_precision                                                            57   \n",
       "split0_test_f1                                                           0.444444   \n",
       "split1_test_f1                                                           0.490566   \n",
       "split2_test_f1                                                           0.290909   \n",
       "mean_test_f1                                                              0.40864   \n",
       "std_test_f1                                                              0.085351   \n",
       "rank_test_f1                                                                   15   \n",
       "split0_test_roc_auc                                                      0.710526   \n",
       "split1_test_roc_auc                                                      0.743784   \n",
       "split2_test_roc_auc                                                      0.712811   \n",
       "mean_test_roc_auc                                                        0.722374   \n",
       "std_test_roc_auc                                                         0.015168   \n",
       "rank_test_roc_auc                                                               1   \n",
       "\n",
       "                                                                              343  \\\n",
       "mean_fit_time                                                            0.123824   \n",
       "std_fit_time                                                             0.006223   \n",
       "mean_score_time                                                          0.010594   \n",
       "std_score_time                                                           0.000057   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                           lbfgs   \n",
       "param_over__k_neighbors                                                         3   \n",
       "param_over__sampling_strategy                                                 0.5   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                           0.45   \n",
       "split1_test_recall                                                        0.47619   \n",
       "split2_test_recall                                                        0.52381   \n",
       "mean_test_recall                                                         0.483333   \n",
       "std_test_recall                                                          0.030553   \n",
       "rank_test_recall                                                              119   \n",
       "split0_test_precision                                                    0.290323   \n",
       "split1_test_precision                                                         0.5   \n",
       "split2_test_precision                                                    0.297297   \n",
       "mean_test_precision                                                       0.36254   \n",
       "std_test_precision                                                       0.097241   \n",
       "rank_test_precision                                                            35   \n",
       "split0_test_f1                                                           0.352941   \n",
       "split1_test_f1                                                           0.487805   \n",
       "split2_test_f1                                                            0.37931   \n",
       "mean_test_f1                                                             0.406685   \n",
       "std_test_f1                                                              0.058362   \n",
       "rank_test_f1                                                                   16   \n",
       "split0_test_roc_auc                                                      0.657018   \n",
       "split1_test_roc_auc                                                      0.758323   \n",
       "split2_test_roc_auc                                                      0.674041   \n",
       "mean_test_roc_auc                                                        0.696461   \n",
       "std_test_roc_auc                                                         0.044292   \n",
       "rank_test_roc_auc                                                              26   \n",
       "\n",
       "                                                                              480  \\\n",
       "mean_fit_time                                                            1.959548   \n",
       "std_fit_time                                                             0.316683   \n",
       "mean_score_time                                                          0.011542   \n",
       "std_score_time                                                           0.000646   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         6   \n",
       "param_over__sampling_strategy                                                 0.6   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                           0.55   \n",
       "split1_test_recall                                                       0.380952   \n",
       "split2_test_recall                                                       0.428571   \n",
       "mean_test_recall                                                         0.453175   \n",
       "std_test_recall                                                          0.071172   \n",
       "rank_test_recall                                                              183   \n",
       "split0_test_precision                                                    0.392857   \n",
       "split1_test_precision                                                         0.4   \n",
       "split2_test_precision                                                    0.321429   \n",
       "mean_test_precision                                                      0.371429   \n",
       "std_test_precision                                                       0.035475   \n",
       "rank_test_precision                                                            22   \n",
       "split0_test_f1                                                           0.458333   \n",
       "split1_test_f1                                                           0.390244   \n",
       "split2_test_f1                                                           0.367347   \n",
       "mean_test_f1                                                             0.405308   \n",
       "std_test_f1                                                              0.038642   \n",
       "rank_test_f1                                                                   17   \n",
       "split0_test_roc_auc                                                       0.67193   \n",
       "split1_test_roc_auc                                                      0.642014   \n",
       "split2_test_roc_auc                                                      0.674252   \n",
       "mean_test_roc_auc                                                        0.662732   \n",
       "std_test_roc_auc                                                          0.01468   \n",
       "rank_test_roc_auc                                                             128   \n",
       "\n",
       "                                ...  \\\n",
       "mean_fit_time                   ...   \n",
       "std_fit_time                    ...   \n",
       "mean_score_time                 ...   \n",
       "std_score_time                  ...   \n",
       "param_clf__hidden_layer_sizes   ...   \n",
       "param_clf__solver               ...   \n",
       "param_over__k_neighbors         ...   \n",
       "param_over__sampling_strategy   ...   \n",
       "param_under__sampling_strategy  ...   \n",
       "params                          ...   \n",
       "split0_test_recall              ...   \n",
       "split1_test_recall              ...   \n",
       "split2_test_recall              ...   \n",
       "mean_test_recall                ...   \n",
       "std_test_recall                 ...   \n",
       "rank_test_recall                ...   \n",
       "split0_test_precision           ...   \n",
       "split1_test_precision           ...   \n",
       "split2_test_precision           ...   \n",
       "mean_test_precision             ...   \n",
       "std_test_precision              ...   \n",
       "rank_test_precision             ...   \n",
       "split0_test_f1                  ...   \n",
       "split1_test_f1                  ...   \n",
       "split2_test_f1                  ...   \n",
       "mean_test_f1                    ...   \n",
       "std_test_f1                     ...   \n",
       "rank_test_f1                    ...   \n",
       "split0_test_roc_auc             ...   \n",
       "split1_test_roc_auc             ...   \n",
       "split2_test_roc_auc             ...   \n",
       "mean_test_roc_auc               ...   \n",
       "std_test_roc_auc                ...   \n",
       "rank_test_roc_auc               ...   \n",
       "\n",
       "                                                                              34   \\\n",
       "mean_fit_time                                                            0.013454   \n",
       "std_fit_time                                                             0.000079   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                                (2,)   \n",
       "param_clf__solver                                                           lbfgs   \n",
       "param_over__k_neighbors                                                         4   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__hidden_layer_sizes': (2,), 'clf__solver...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              608   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           643   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  632   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             587   \n",
       "\n",
       "                                                                              447  \\\n",
       "mean_fit_time                                                            0.015102   \n",
       "std_fit_time                                                             0.000617   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         2   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              634   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           631   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  633   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             617   \n",
       "\n",
       "                                                                              448  \\\n",
       "mean_fit_time                                                            0.015454   \n",
       "std_fit_time                                                             0.000807   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         2   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              636   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           632   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  634   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             616   \n",
       "\n",
       "                                                                              33   \\\n",
       "mean_fit_time                                                            0.013627   \n",
       "std_fit_time                                                             0.000217   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                                (2,)   \n",
       "param_clf__solver                                                           lbfgs   \n",
       "param_over__k_neighbors                                                         4   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__hidden_layer_sizes': (2,), 'clf__solver...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              610   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           624   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  635   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             643   \n",
       "\n",
       "                                                                              187  \\\n",
       "mean_fit_time                                                            0.016792   \n",
       "std_fit_time                                                             0.002312   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                              (2, 2)   \n",
       "param_clf__solver                                                           lbfgs   \n",
       "param_over__k_neighbors                                                         3   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__hidden_layer_sizes': (2, 2), 'clf__solv...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              542   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           603   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  636   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             634   \n",
       "\n",
       "                                                                              456  \\\n",
       "mean_fit_time                                                            0.015104   \n",
       "std_fit_time                                                              0.00039   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         3   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              637   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           636   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  637   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             642   \n",
       "\n",
       "                                                                              186  \\\n",
       "mean_fit_time                                                            0.015972   \n",
       "std_fit_time                                                             0.000817   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                              (2, 2)   \n",
       "param_clf__solver                                                           lbfgs   \n",
       "param_over__k_neighbors                                                         3   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__hidden_layer_sizes': (2, 2), 'clf__solv...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              532   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           606   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  638   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             614   \n",
       "\n",
       "                                                                              465  \\\n",
       "mean_fit_time                                                            0.014783   \n",
       "std_fit_time                                                              0.00014   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         4   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              642   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           639   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  639   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             635   \n",
       "\n",
       "                                                                              466  \\\n",
       "mean_fit_time                                                            0.015349   \n",
       "std_fit_time                                                             0.001179   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         4   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              643   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           640   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  640   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             633   \n",
       "\n",
       "                                                                              178  \\\n",
       "mean_fit_time                                                            0.015387   \n",
       "std_fit_time                                                             0.000497   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                              (2, 2)   \n",
       "param_clf__solver                                                           lbfgs   \n",
       "param_over__k_neighbors                                                         2   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__hidden_layer_sizes': (2, 2), 'clf__solv...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              528   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           607   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  641   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             567   \n",
       "\n",
       "                                                                              177  \\\n",
       "mean_fit_time                                                            0.016693   \n",
       "std_fit_time                                                             0.001261   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                              (2, 2)   \n",
       "param_clf__solver                                                           lbfgs   \n",
       "param_over__k_neighbors                                                         2   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__hidden_layer_sizes': (2, 2), 'clf__solv...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              527   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           611   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  642   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             574   \n",
       "\n",
       "                                                                              25   \\\n",
       "mean_fit_time                                                            0.013597   \n",
       "std_fit_time                                                              0.00014   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                                (2,)   \n",
       "param_clf__solver                                                           lbfgs   \n",
       "param_over__k_neighbors                                                         3   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__hidden_layer_sizes': (2,), 'clf__solver...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              541   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           610   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  643   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             534   \n",
       "\n",
       "                                                                              24   \\\n",
       "mean_fit_time                                                            0.013714   \n",
       "std_fit_time                                                             0.000635   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                                (2,)   \n",
       "param_clf__solver                                                           lbfgs   \n",
       "param_over__k_neighbors                                                         3   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__hidden_layer_sizes': (2,), 'clf__solver...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              544   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           612   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  644   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             520   \n",
       "\n",
       "                                                                              474  \\\n",
       "mean_fit_time                                                            0.014947   \n",
       "std_fit_time                                                             0.000089   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         5   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.6   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              645   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           644   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  645   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             589   \n",
       "\n",
       "                                                                              475  \\\n",
       "mean_fit_time                                                            0.015416   \n",
       "std_fit_time                                                             0.000928   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                                (3,)   \n",
       "param_clf__solver                                                            adam   \n",
       "param_over__k_neighbors                                                         5   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              646   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           645   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  646   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             588   \n",
       "\n",
       "                                                                              205  \\\n",
       "mean_fit_time                                                            0.015197   \n",
       "std_fit_time                                                             0.000138   \n",
       "mean_score_time                                                               0.0   \n",
       "std_score_time                                                                0.0   \n",
       "param_clf__hidden_layer_sizes                                              (2, 2)   \n",
       "param_clf__solver                                                           lbfgs   \n",
       "param_over__k_neighbors                                                         5   \n",
       "param_over__sampling_strategy                                                 0.8   \n",
       "param_under__sampling_strategy                                                0.7   \n",
       "params                          {'clf__hidden_layer_sizes': (2, 2), 'clf__solv...   \n",
       "split0_test_recall                                                            NaN   \n",
       "split1_test_recall                                                            NaN   \n",
       "split2_test_recall                                                            NaN   \n",
       "mean_test_recall                                                              NaN   \n",
       "std_test_recall                                                               NaN   \n",
       "rank_test_recall                                                              612   \n",
       "split0_test_precision                                                         NaN   \n",
       "split1_test_precision                                                         NaN   \n",
       "split2_test_precision                                                         NaN   \n",
       "mean_test_precision                                                           NaN   \n",
       "std_test_precision                                                            NaN   \n",
       "rank_test_precision                                                           648   \n",
       "split0_test_f1                                                                NaN   \n",
       "split1_test_f1                                                                NaN   \n",
       "split2_test_f1                                                                NaN   \n",
       "mean_test_f1                                                                  NaN   \n",
       "std_test_f1                                                                   NaN   \n",
       "rank_test_f1                                                                  647   \n",
       "split0_test_roc_auc                                                           NaN   \n",
       "split1_test_roc_auc                                                           NaN   \n",
       "split2_test_roc_auc                                                           NaN   \n",
       "mean_test_roc_auc                                                             NaN   \n",
       "std_test_roc_auc                                                              NaN   \n",
       "rank_test_roc_auc                                                             535   \n",
       "\n",
       "                                                                              366  \n",
       "mean_fit_time                                                             0.01451  \n",
       "std_fit_time                                                             0.000106  \n",
       "mean_score_time                                                               0.0  \n",
       "std_score_time                                                                0.0  \n",
       "param_clf__hidden_layer_sizes                                                (3,)  \n",
       "param_clf__solver                                                           lbfgs  \n",
       "param_over__k_neighbors                                                         5  \n",
       "param_over__sampling_strategy                                                 0.8  \n",
       "param_under__sampling_strategy                                                0.6  \n",
       "params                          {'clf__hidden_layer_sizes': (3,), 'clf__solver...  \n",
       "split0_test_recall                                                            NaN  \n",
       "split1_test_recall                                                            NaN  \n",
       "split2_test_recall                                                            NaN  \n",
       "mean_test_recall                                                              NaN  \n",
       "std_test_recall                                                               NaN  \n",
       "rank_test_recall                                                              553  \n",
       "split0_test_precision                                                         NaN  \n",
       "split1_test_precision                                                         NaN  \n",
       "split2_test_precision                                                         NaN  \n",
       "mean_test_precision                                                           NaN  \n",
       "std_test_precision                                                            NaN  \n",
       "rank_test_precision                                                           553  \n",
       "split0_test_f1                                                                NaN  \n",
       "split1_test_f1                                                                NaN  \n",
       "split2_test_f1                                                                NaN  \n",
       "mean_test_f1                                                                  NaN  \n",
       "std_test_f1                                                                   NaN  \n",
       "rank_test_f1                                                                  648  \n",
       "split0_test_roc_auc                                                           NaN  \n",
       "split1_test_roc_auc                                                           NaN  \n",
       "split2_test_roc_auc                                                           NaN  \n",
       "mean_test_roc_auc                                                             NaN  \n",
       "std_test_roc_auc                                                              NaN  \n",
       "rank_test_roc_auc                                                             551  \n",
       "\n",
       "[34 rows x 648 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MLP with grid search for parameters, testing on 5-fold CV with shuffling\n",
    "\n",
    "pipeline = IMBLpipeline([\n",
    "    ('over', SMOTE()),\n",
    "    ('under', RandomUnderSampler()),\n",
    "    ('clf', MLPClassifier(solver='lbfgs', max_iter=5000))\n",
    "])\n",
    "\n",
    "parameters = {'over__k_neighbors': range(1,7),\n",
    "              'over__sampling_strategy': [0.5, 0.6, 0.8], # probaj poveča ovresampling do 0.9\n",
    "              'under__sampling_strategy': [0.6, 0.7, 0.8],\n",
    "              'clf__hidden_layer_sizes': [(2, ), (2, 2), (3,), (3,3)],\n",
    "              'clf__solver': ['lbfgs', 'sgd', 'adam']\n",
    "             }\n",
    "nfolds = 3\n",
    "scores = ['recall', \"precision\", 'f1', 'roc_auc']\n",
    "gscv_NN = GridSearchCV(pipeline, \n",
    "                    parameters, \n",
    "                    scoring=scores,\n",
    "                    cv=StratifiedKFold(n_splits=nfolds, shuffle=True),\n",
    "                    n_jobs= -1, \n",
    "                    return_train_score=False, \n",
    "                    verbose=1, \n",
    "                    refit= \"f1\")\n",
    "resultsGSCV = gscv_NN.fit(Xsc, y)\n",
    "\n",
    "# Get the classifier name from the pipeline\n",
    "classifier_name = resultsGSCV.best_estimator_.named_steps['clf'].__class__.__name__\n",
    "    \n",
    "# save the best estimator\n",
    "save_best_estimator(gscv_NN, classifier_name, notebook_name)\n",
    "\n",
    "results = pd.DataFrame(resultsGSCV.cv_results_)\n",
    "display(results.sort_values(by=[f'rank_test_f1']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "017735d1-2fe5-4b37-b40a-a45fb91b8be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.87       147\n",
      "           1       0.29      0.35      0.32        26\n",
      "\n",
      "    accuracy                           0.77       173\n",
      "   macro avg       0.59      0.60      0.59       173\n",
      "weighted avg       0.79      0.77      0.78       173\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of NN on test set\n",
    "from sklearn.metrics import classification_report\n",
    "X_eval_ohe = pd.get_dummies(X_eval, columns=[\"month\"])\n",
    "scaler = StandardScaler().fit(Xohe)\n",
    "X_eval_sc = scaler.transform(X_eval_ohe)\n",
    "X_eval_sc = pd.DataFrame(X_eval_sc, columns=Xohe.columns)\n",
    "y_pred = gscv_NN.best_estimator_.steps[2][1].predict(X_eval_sc)\n",
    "NN_classification_report = classification_report(y_eval, y_pred)\n",
    "\n",
    "# Create classification report as dictionary\n",
    "NN_report_dict = classification_report(y_eval, y_pred, output_dict=True)\n",
    "\n",
    "print(classification_report(y_eval, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ba333f-3931-4bf7-ae33-757478d69a30",
   "metadata": {},
   "source": [
    "#### Feature Importance (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67f68acd-b343-48c6-a372-d46ed52e6951",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Permutation Importance MLP')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feature importance of model (MLP)  (no cross-validation!)\n",
    "\n",
    "fig, (ax2) = plt.subplots(1, 1, figsize=(10,9))\n",
    "plt.subplots_adjust(wspace=3)\n",
    "\n",
    "MLP = gscv_NN.best_estimator_.steps[2][1]\n",
    "\n",
    "# Get feature importance with Permutation Based Feature Importance (randomly shuffles each feature and compute the \n",
    "# change in the model’s performance. The features which impact the performance the most are the most important one).\n",
    "perm_importance = permutation_importance(MLP, Xsc, y)\n",
    "perm_sorted_idx = perm_importance.importances_mean.argsort()\n",
    "x2 = Xsc.columns[perm_sorted_idx]\n",
    "y2 = perm_importance.importances_mean[perm_sorted_idx]\n",
    "ax2.barh(x2, y2)\n",
    "ax2.set_title(\"Permutation Importance MLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09700f24-b683-41ee-85b1-1c957b12325a",
   "metadata": {},
   "source": [
    "#### Feature importance with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ec901eb-8260-445e-8b66-919df83ffd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Xsc.copy()\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, stratify=y, test_size=0.25)\n",
    "\n",
    "nn = MLPClassifier(hidden_layer_sizes=(3,3), solver='lbfgs', max_iter=5000)\n",
    "model = nn.fit(X.to_numpy(), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902cad31-dc72-4a66-b376-2ca9ea8e0e80",
   "metadata": {},
   "source": [
    "First, visualize the impact of all features on both classes in one chart. We are using KernelExplainer but simpler general Explainer should be also tested once the SHAP code fixes all bugs.\n",
    "\n",
    "**Note: SHAP explanations change between runs because of sampling and probably other random factors!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "241febf0-4e3c-40b1-b0ff-a0d4d4d99787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # explain the model's predictions using SHAP\n",
    "# import shap\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# shap.initjs()\n",
    "\n",
    "# explainer = shap.KernelExplainer(model.predict_proba, shap.sample(X_eval_sc,20))\n",
    "# shap_values = explainer.shap_values(X_eval_sc, nsamples=50)\n",
    "# shap.summary_plot(shap_values, X_eval_sc, max_display=len(X.columns), class_names=le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9123988d-5f60-4676-8f57-cccbbff97cbe",
   "metadata": {},
   "source": [
    "Now for each class separately. We observe the impact of features on the returned model's probability for a given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "385ff88c-0dce-4fb5-a974-fec0a9b91d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c473899631c640b59a8747ad896361ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.647e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.746e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 36 iterations, alpha=3.668e-04, previous alpha=3.494e-04, with an active set of 19 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.647e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.746e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 36 iterations, alpha=3.668e-04, previous alpha=3.494e-04, with an active set of 19 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.187e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.155e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.092e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 20 iterations, alpha=1.146e-04, previous alpha=8.949e-05, with an active set of 19 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=9.187e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.155e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.092e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 20 iterations, alpha=1.146e-04, previous alpha=8.949e-05, with an active set of 19 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.472e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.183e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=5.844e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.323e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=8.525e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.472e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.183e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=5.844e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.323e-05, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=8.525e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.247e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=5.209e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=2.247e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=5.209e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.971e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=4.508e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=3.308e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.972e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.971e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=4.508e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=3.308e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.972e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.659e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.653e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.595e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.179e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=8.227e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 24 iterations, alpha=6.979e-05, previous alpha=4.894e-05, with an active set of 21 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.179e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=8.227e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 24 iterations, alpha=6.979e-05, previous alpha=4.894e-05, with an active set of 21 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.644e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=2.035e-04, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=6.939e-05, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.538e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.212e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 17 iterations, alpha=1.112e-04, previous alpha=1.076e-04, with an active set of 16 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.538e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.212e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 17 iterations, alpha=1.112e-04, previous alpha=1.076e-04, with an active set of 16 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=5.108e-06, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.664e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.984e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.614e-07, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.233e-07, with an active set of 24 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=5.108e-06, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.664e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.984e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.614e-07, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.233e-07, with an active set of 24 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.883e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=7.923e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=6.632e-05, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=5.156e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.348e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 22 iterations, alpha=5.080e-05, previous alpha=3.681e-05, with an active set of 19 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.883e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=7.923e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=6.632e-05, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=5.156e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.348e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 22 iterations, alpha=5.080e-05, previous alpha=3.681e-05, with an active set of 19 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.211e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.500e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 29 iterations, alpha=1.067e-04, previous alpha=9.278e-05, with an active set of 22 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.134e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.169e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 21 iterations, alpha=2.593e-04, previous alpha=2.291e-04, with an active set of 18 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.218e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.350e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 17 iterations, alpha=1.838e-04, previous alpha=1.596e-04, with an active set of 16 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.106e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.284e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=8.783e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 30 iterations, alpha=5.752e-05, previous alpha=4.543e-05, with an active set of 21 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.762e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.492e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=3.647e-05, previous alpha=3.344e-05, with an active set of 23 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.762e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.492e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=3.647e-05, previous alpha=3.344e-05, with an active set of 23 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.047e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.820e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.667e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 19 iterations, alpha=5.902e-04, previous alpha=5.874e-04, with an active set of 16 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.047e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.820e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.667e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 19 iterations, alpha=5.902e-04, previous alpha=5.874e-04, with an active set of 16 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.021e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.610e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.373e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 28 iterations, alpha=1.465e-05, previous alpha=4.848e-06, with an active set of 21 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.021e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.610e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.373e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 28 iterations, alpha=1.465e-05, previous alpha=4.848e-06, with an active set of 21 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.517e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.739e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.739e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.664e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.797e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.779e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=9.517e-05, with an active set of 14 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=7.628e-05, with an active set of 15 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=3.923e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.498e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.012e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=2.269e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.714e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.379e-07, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.365e-07, with an active set of 23 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=3.226e-05, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.613e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.575e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.552e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=3.395e-06, with an active set of 21 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=5.336e-07, with an active set of 23 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=3.937e-07, with an active set of 23 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=3.937e-07, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=7.325e-05, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=4.835e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=3.942e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 25 iterations, alpha=4.122e-05, previous alpha=3.390e-05, with an active set of 20 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=7.325e-05, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=4.835e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=3.942e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 25 iterations, alpha=4.122e-05, previous alpha=3.390e-05, with an active set of 20 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=3.054e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=3.271e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.159e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=3.668e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.159e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=3.668e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=6.669e-06, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 40 iterations, alpha=4.992e-06, previous alpha=2.583e-06, with an active set of 23 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=6.669e-06, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 40 iterations, alpha=4.992e-06, previous alpha=2.583e-06, with an active set of 23 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.685e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.398e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 27 iterations, alpha=1.371e-05, previous alpha=8.227e-06, with an active set of 22 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=8.703e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=5.386e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 23 iterations, alpha=5.263e-05, previous alpha=4.174e-05, with an active set of 20 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=8.703e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=5.386e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 23 iterations, alpha=5.263e-05, previous alpha=4.174e-05, with an active set of 20 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.189e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=9.560e-07, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.174e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=9.442e-07, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.972e-07, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.473e-07, with an active set of 24 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.619e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.619e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=7.299e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=7.180e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=4.405e-05, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=6.660e-07, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=4.405e-05, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=6.660e-07, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.112e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.527e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.112e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=3.527e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=1.136e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=9.553e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=6.977e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.231e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.246e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.533e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=6.674e-07, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=4.554e-07, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.160e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.630e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.781e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.376e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 21 iterations, alpha=1.439e-04, previous alpha=1.273e-04, with an active set of 18 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.160e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.630e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.781e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.376e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 21 iterations, alpha=1.439e-04, previous alpha=1.273e-04, with an active set of 18 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=4.615e-06, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.990e-06, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=5.374e-07, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.927e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 23 iterations, alpha=1.020e-04, previous alpha=6.401e-05, with an active set of 18 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.071e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=2.498e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=2.457e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=9.980e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=9.165e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 37 iterations, alpha=7.656e-05, previous alpha=6.860e-05, with an active set of 22 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.071e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=2.498e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=2.457e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=9.980e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=9.165e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 37 iterations, alpha=7.656e-05, previous alpha=6.860e-05, with an active set of 22 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.831e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.155e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.132e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.694e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 9 iterations, alpha=8.987e-04, previous alpha=8.613e-04, with an active set of 8 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.386e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.693e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 14 iterations, alpha=2.670e-04, previous alpha=2.598e-04, with an active set of 13 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.098e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.512e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.473e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 22 iterations, alpha=1.359e-04, previous alpha=1.338e-04, with an active set of 17 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.098e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.512e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.473e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 22 iterations, alpha=1.359e-04, previous alpha=1.338e-04, with an active set of 17 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=3.923e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.710e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.999e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.345e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.767e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.721e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.612e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=8.722e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=6.718e-05, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=3.504e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.969e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.872e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.629e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 26 iterations, alpha=7.118e-05, previous alpha=1.222e-05, with an active set of 21 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=3.923e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.710e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.999e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.345e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.767e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.721e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.612e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=8.722e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=6.718e-05, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=3.504e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.969e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.872e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.629e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 26 iterations, alpha=7.118e-05, previous alpha=1.222e-05, with an active set of 21 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.964e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.879e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.748e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.452e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 28 iterations, alpha=1.276e-04, previous alpha=7.490e-05, with an active set of 19 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=4.547e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.744e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 26 iterations, alpha=1.325e-05, previous alpha=1.285e-05, with an active set of 21 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=4.547e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.744e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 26 iterations, alpha=1.325e-05, previous alpha=1.285e-05, with an active set of 21 regressors.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x914.4 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "explainer = shap.KernelExplainer(model.predict_proba, shap.sample(Xsc,50)) #morda treba zmanjšat število, ali brez sample in samo X_eval\n",
    "shap_values = explainer.shap_values(Xsc, nsamples=50)\n",
    "classid = 1\n",
    "shap.summary_plot(shap_values[classid], Xsc, max_display=len(X.columns), class_names=le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3cac8790-80dc-44b5-9e13-18d2050b196e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038f83b847e24e05961c4282ed4c24a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=6.691e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 25 iterations, alpha=6.584e-05, previous alpha=4.099e-05, with an active set of 22 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=6.691e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 25 iterations, alpha=6.584e-05, previous alpha=4.099e-05, with an active set of 22 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=3.078e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=3.066e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 17 iterations, alpha=1.355e-04, previous alpha=1.223e-04, with an active set of 14 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.694e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=4.217e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.854e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.257e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.226e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=2.209e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=9.995e-07, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.039e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.520e-06, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=4.940e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 28 iterations, alpha=3.426e-06, previous alpha=3.347e-06, with an active set of 23 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.810e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.559e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=9.321e-07, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.810e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.559e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=9.321e-07, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.441e-04, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=7.095e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.768e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.632e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=6.571e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.441e-04, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=7.095e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.768e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.632e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=6.571e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.296e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.991e-06, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=5.778e-06, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=5.418e-06, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.639e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.323e-05, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.764e-06, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.069e-06, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=5.691e-06, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.682e-06, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.485e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.876e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.097e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 15 iterations, alpha=1.156e-04, previous alpha=9.469e-05, with an active set of 14 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.485e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.876e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.097e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 15 iterations, alpha=1.156e-04, previous alpha=9.469e-05, with an active set of 14 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.680e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.813e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.877e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 27 iterations, alpha=2.527e-05, previous alpha=1.448e-05, with an active set of 22 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.680e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.813e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.877e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 27 iterations, alpha=2.527e-05, previous alpha=1.448e-05, with an active set of 22 regressors.\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 540x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Try dependence contribution plot\n",
    "explainer = shap.KernelExplainer(model.predict_proba, shap.sample(X_eval_sc,50))\n",
    "shap_values = explainer.shap_values(X_eval_sc, nsamples=50)\n",
    "shap.dependence_plot('salinity', shap_values[1], X_eval_sc,) #interaction_index=\"salinity\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38930dca-d69a-4393-a88b-6fc019d83c57",
   "metadata": {},
   "source": [
    "Example intepretation: The fact this slopes upward says the higher the soca flow, the higher the model's prediction is for poz/neg. The spread suggests that other features must interact with Soca flow. \n",
    "In general, high Soca flow increases the chance of poz/neg. But if the sea temp is moderate or low, that trend reverses and even high soca flow does not increase preditions of poz/neg as the sea temp is too low.\n",
    "https://www.kaggle.com/code/dansbecker/advanced-uses-of-shap-values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e17d179-7316-4026-b266-1bc7ef4662b9",
   "metadata": {},
   "source": [
    "Now let's explain the prediction of a single instance. We will show the explanation of the bigger predicted probability to see why the model decided as it did. But in practice we could be interested only in the explanation of the probability of the positive prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d07af4e-262b-45e6-beab-41bdf773fb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real value: 1, \n",
      "predicted: 1, \n",
      "predicted probs: [0.04743911 0.95256089]\n",
      "Explanation for prediction: class=1, p=0.9525608862433694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X has feature names, but MLPClassifier was fitted without feature names\n",
      "X has feature names, but MLPClassifier was fitted without feature names\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681ce9d670e84a2dbec4b46f90c9bd2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div id='iCQXZW88DXR2FYGLL8QCQ'>\n",
       "<div style='color: #900; text-align: center;'>\n",
       "  <b>Visualization omitted, Javascript library not loaded!</b><br>\n",
       "  Have you run `initjs()` in this notebook? If this notebook was from another\n",
       "  user you must also trust this notebook (File -> Trust notebook). If you are viewing\n",
       "  this notebook on github the Javascript has been stripped for security. If you are using\n",
       "  JupyterLab this error is because a JupyterLab extension has not yet been written.\n",
       "</div></div>\n",
       " <script>\n",
       "   if (window.SHAP) SHAP.ReactDom.render(\n",
       "    SHAP.React.createElement(SHAP.AdditiveForceVisualizer, {\"outNames\": [\"f(x)\"], \"baseValue\": 0.23879652262915535, \"outValue\": 0.9525608862433697, \"link\": \"identity\", \"featureNames\": [\"DSP\", \"Dinophysis caudata\", \"Dinophysis fortii\", \"Phalacroma rotundatum\", \"Dinophysis sacculus\", \"Dinophysis tripos\", \"sun [h]\", \"air temp\", \"wind strength\", \"precipitation\", \"Chl-a\", \"salinity\", \"T\", \"DIN\", \"PO4-P\", \"Soca\", \"month_1\", \"month_2\", \"month_3\", \"month_4\", \"month_5\", \"month_6\", \"month_7\", \"month_8\", \"month_9\", \"month_10\", \"month_11\", \"month_12\"], \"features\": {\"0\": {\"effect\": 0.03270000012748703, \"value\": 50.0}, \"2\": {\"effect\": -0.02010646440132735, \"value\": 10.0}, \"3\": {\"effect\": 0.011699044205890852, \"value\": 10.0}, \"5\": {\"effect\": 0.24258295946947983, \"value\": 30.0}, \"6\": {\"effect\": 0.14169618340054596, \"value\": 42.99999999999999}, \"7\": {\"effect\": 0.03902369927815286, \"value\": 6.8428571428571425}, \"10\": {\"effect\": 0.13381694078347856, \"value\": 4.24}, \"11\": {\"effect\": 0.22745548702334523, \"value\": 31.89}, \"12\": {\"effect\": -0.01805207123473329, \"value\": 14.55}, \"13\": {\"effect\": 0.11107422299269593, \"value\": 10.379999905824656}, \"15\": {\"effect\": -0.025071422337949983, \"value\": 10683.316}, \"16\": {\"effect\": -0.04369660410434689, \"value\": 0.0}, \"19\": {\"effect\": 0.028964182040466746, \"value\": 0.0}, \"20\": {\"effect\": 0.038776292171317395, \"value\": 0.0}, \"22\": {\"effect\": -0.027349933640167093, \"value\": 0.0}, \"23\": {\"effect\": -0.04917088943394382, \"value\": 0.0}, \"24\": {\"effect\": 0.04433417242823734, \"value\": 0.0}, \"25\": {\"effect\": 0.05970657613199015, \"value\": 0.0}, \"26\": {\"effect\": 0.0222786179261083, \"value\": 0.0}, \"27\": {\"effect\": -0.23689662921251342, \"value\": 1.0}}, \"plot_cmap\": \"RdBu\", \"labelMargin\": 20}),\n",
       "    document.getElementById('iCQXZW88DXR2FYGLL8QCQ')\n",
       "  );\n",
       "</script>"
      ],
      "text/plain": [
       "<shap.plots._force.AdditiveForceVisualizer at 0x13d8f5e50>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instanceID = 10\n",
    "instance = X.iloc[[instanceID]]\n",
    "display_instance = X_display.iloc[[instanceID]]\n",
    "\n",
    "prediction = model.predict(instance)[0]\n",
    "prediction_probs = model.predict_proba(instance)[0]\n",
    "print(f'real value: {y[instanceID]}, \\npredicted: {prediction}, \\npredicted probs: {prediction_probs}')\n",
    "max_p_id = prediction_probs.argmax()  # we will show the explanation of the bigger predicted probability\n",
    "print(f'Explanation for prediction: class={max_p_id}, p={prediction_probs.max()}')\n",
    "\n",
    "explainer = shap.KernelExplainer(model.predict_proba, shap.sample(X, 50))\n",
    "shap_values = explainer.shap_values(instance, nsamples=500)\n",
    "shap.force_plot(explainer.expected_value[max_p_id], shap_values[max_p_id], features=display_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4908630-eea9-4167-83d9-5511dd820733",
   "metadata": {},
   "source": [
    "Show the mean values of features as it may help understanding this particular instance data in the plot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "642977c5-c5e3-4c96-b81f-c9419b084dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DSP</th>\n",
       "      <td>102.700503</td>\n",
       "      <td>195.860294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dinophysis caudata</th>\n",
       "      <td>25.537688</td>\n",
       "      <td>40.543382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dinophysis fortii</th>\n",
       "      <td>23.725829</td>\n",
       "      <td>72.827941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phalacroma rotundatum</th>\n",
       "      <td>16.438492</td>\n",
       "      <td>13.510294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dinophysis sacculus</th>\n",
       "      <td>24.474070</td>\n",
       "      <td>52.823529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dinophysis tripos</th>\n",
       "      <td>4.902513</td>\n",
       "      <td>8.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sun [h]</th>\n",
       "      <td>165.661647</td>\n",
       "      <td>152.191912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>air temp</th>\n",
       "      <td>17.422122</td>\n",
       "      <td>16.944328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wind strength</th>\n",
       "      <td>2.997017</td>\n",
       "      <td>2.921674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precipitation</th>\n",
       "      <td>56.557731</td>\n",
       "      <td>75.321324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chl-a</th>\n",
       "      <td>0.791451</td>\n",
       "      <td>0.965165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salinity</th>\n",
       "      <td>37.476320</td>\n",
       "      <td>35.692370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T</th>\n",
       "      <td>20.112537</td>\n",
       "      <td>20.241136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIN</th>\n",
       "      <td>2.921092</td>\n",
       "      <td>3.868770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PO4-P</th>\n",
       "      <td>0.058539</td>\n",
       "      <td>0.076768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soca</th>\n",
       "      <td>3208.868147</td>\n",
       "      <td>3703.893351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_1</th>\n",
       "      <td>0.015060</td>\n",
       "      <td>0.022059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_2</th>\n",
       "      <td>0.021084</td>\n",
       "      <td>0.022059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_3</th>\n",
       "      <td>0.028112</td>\n",
       "      <td>0.014706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_4</th>\n",
       "      <td>0.066265</td>\n",
       "      <td>0.014706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_5</th>\n",
       "      <td>0.089357</td>\n",
       "      <td>0.102941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_6</th>\n",
       "      <td>0.115462</td>\n",
       "      <td>0.088235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_7</th>\n",
       "      <td>0.125502</td>\n",
       "      <td>0.132353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_8</th>\n",
       "      <td>0.126506</td>\n",
       "      <td>0.080882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_9</th>\n",
       "      <td>0.143574</td>\n",
       "      <td>0.227941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_10</th>\n",
       "      <td>0.138554</td>\n",
       "      <td>0.176471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_11</th>\n",
       "      <td>0.091365</td>\n",
       "      <td>0.095588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_12</th>\n",
       "      <td>0.039157</td>\n",
       "      <td>0.022059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               neg          pos\n",
       "DSP                     102.700503   195.860294\n",
       "Dinophysis caudata       25.537688    40.543382\n",
       "Dinophysis fortii        23.725829    72.827941\n",
       "Phalacroma rotundatum    16.438492    13.510294\n",
       "Dinophysis sacculus      24.474070    52.823529\n",
       "Dinophysis tripos         4.902513     8.235294\n",
       "sun [h]                 165.661647   152.191912\n",
       "air temp                 17.422122    16.944328\n",
       "wind strength             2.997017     2.921674\n",
       "precipitation            56.557731    75.321324\n",
       "Chl-a                     0.791451     0.965165\n",
       "salinity                 37.476320    35.692370\n",
       "T                        20.112537    20.241136\n",
       "DIN                       2.921092     3.868770\n",
       "PO4-P                     0.058539     0.076768\n",
       "Soca                   3208.868147  3703.893351\n",
       "month_1                   0.015060     0.022059\n",
       "month_2                   0.021084     0.022059\n",
       "month_3                   0.028112     0.014706\n",
       "month_4                   0.066265     0.014706\n",
       "month_5                   0.089357     0.102941\n",
       "month_6                   0.115462     0.088235\n",
       "month_7                   0.125502     0.132353\n",
       "month_8                   0.126506     0.080882\n",
       "month_9                   0.143574     0.227941\n",
       "month_10                  0.138554     0.176471\n",
       "month_11                  0.091365     0.095588\n",
       "month_12                  0.039157     0.022059"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.get_dummies(data, columns=[\"month\"])\n",
    "\n",
    "\n",
    "pd.DataFrame([data[data['lipophylic_toxins']=='neg'].mean(), data[data['lipophylic_toxins']=='poz'].mean()], index=['neg','pos']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9f3aca-b6f0-4ab4-bd20-b828301bfdb2",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8273fc8d-8b18-4f8b-8c19-448fb5843226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Classifier Class  Precision    Recall  F1-score  Support\n",
      "0      Logistic Regression     0   0.871951  0.972789  0.919614    147.0\n",
      "1      Logistic Regression     1   0.555556  0.192308  0.285714     26.0\n",
      "2  Support Vector Mashines     0   0.893805  0.687075  0.776923    147.0\n",
      "3  Support Vector Mashines     1   0.233333  0.538462  0.325581     26.0\n",
      "4            Decision Tree     0   0.881579  0.911565  0.896321    147.0\n",
      "5            Decision Tree     1   0.380952  0.307692  0.340426     26.0\n",
      "6            Random Forest     0   0.928571  0.884354  0.905923    147.0\n",
      "7            Random Forest     1   0.484848  0.615385  0.542373     26.0\n",
      "8           Neural Network     0   0.880282  0.850340  0.865052    147.0\n",
      "9           Neural Network     1   0.290323  0.346154  0.315789     26.0\n"
     ]
    }
   ],
   "source": [
    "# Define a function to extract the metrics from the classification report dictionary\n",
    "def extract_metrics(report_dict):\n",
    "    metrics = {}\n",
    "    for class_label in report_dict:\n",
    "        if class_label in ('accuracy', 'macro avg', 'weighted avg'):\n",
    "            continue\n",
    "        metrics[class_label] = {\n",
    "            'precision': report_dict[class_label]['precision'],\n",
    "            'recall': report_dict[class_label]['recall'],\n",
    "            'f1-score': report_dict[class_label]['f1-score'],\n",
    "            'support': report_dict[class_label]['support']\n",
    "        }\n",
    "    return metrics\n",
    "\n",
    "# Extract the metrics for each classifier\n",
    "lr_metrics = extract_metrics(lr_report_dict)\n",
    "SVM_metrics = extract_metrics(SVM_report_dict)\n",
    "DT_metrics = extract_metrics(DT_report_dict)\n",
    "RF_metrics = extract_metrics(RF_report_dict)\n",
    "NN_metrics = extract_metrics(NN_report_dict)\n",
    "\n",
    "# Create a dictionary to store the metrics for each classifier\n",
    "classifier_metrics = {\n",
    "    'Logistic Regression': lr_metrics,\n",
    "    'Support Vector Mashines': SVM_metrics,\n",
    "    'Decision Tree': DT_metrics,\n",
    "    'Random Forest': RF_metrics,\n",
    "    'Neural Network': NN_metrics\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a pandas DataFrame\n",
    "summary_df = pd.concat({k: pd.DataFrame(v).transpose() for k, v in classifier_metrics.items()}, axis=0)\n",
    "summary_df.reset_index(inplace=True)\n",
    "summary_df.columns = ['Classifier', 'Class', 'Precision', 'Recall', 'F1-score', 'Support']\n",
    "\n",
    "# Display the summary DataFrame\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6edb6f9a-7038-4f5a-9a98-914a7d0e7557",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RF_recall_best_k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Summary table of prediction results\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m RF_recall \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(\u001b[43mRF_recall_best_k\u001b[49m[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      3\u001b[0m RF_auc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(RF_auc_best_k[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      4\u001b[0m MLP_recall \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(MLP_recall_best_k[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RF_recall_best_k' is not defined"
     ]
    }
   ],
   "source": [
    "# Summary table of prediction results\n",
    "RF_recall = round(RF_recall_best_k[1], 2)\n",
    "RF_auc = round(RF_auc_best_k[1], 2)\n",
    "MLP_recall = round(MLP_recall_best_k[1], 2)\n",
    "MLP_auc = round(MLP_auc_best_k[1], 2)\n",
    "\n",
    "summary = pd.DataFrame(\n",
    "    [\n",
    "        (\n",
    "            \"RF\",\n",
    "            RF_recall_score,\n",
    "            RF_auc_score,\n",
    "        ),\n",
    "        (\n",
    "            \"MLP\",\n",
    "            MLP_recall_score,\n",
    "            MLP_auc_score,\n",
    "        ),\n",
    "        (\n",
    "            \"RF (smote)\",\n",
    "            RF_recall,\n",
    "            RF_auc,\n",
    "        ),\n",
    "        (\n",
    "            \"MLP (smote)\",\n",
    "            MLP_recall,\n",
    "            MLP_auc,\n",
    "        ),\n",
    "        (\n",
    "            \"Decision tree (J48)*\",\n",
    "            0.56,\n",
    "            0.18,\n",
    "        ),\n",
    "    ],\n",
    "    columns=(\"Model\", \"Recall\", \"ROC AUC\"),\n",
    ").set_index(\"Model\")\n",
    "\n",
    "print(\"Table summarising the prediction results of the used classifiers, both with and without SMOTE resampling:\\n\")\n",
    "summary.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c712a49-d47a-456a-9905-50a6502a4be2",
   "metadata": {},
   "source": [
    "As can be seen resampling with SMOTE helped to improve the results substantially, especially when calculating recall. The highest recall and ROC AUC was achieved with Random Forest with the re-sampled data. Both recall and ROC AUC suggest Random Forest as beeing the better classifier for this particular problem. Recall is a crucial metric as it gives indication of what fraction of true positive instances have been predicted. Since the models predict toxins in seashells (food) it is crucial that as few positives as possible are missed.\n",
    "\n",
    "Due to the use of SMOTE resampling (upsampling and downsampling) in combinaiton with cross-validation it was curcial to do the resampling within each fold to avoid data lekeage and validate on original (unsampled) data. In addition, I have optimised the model with regard to the k-values of SMOTE, all of which brought along some complexity. So for the parameter tuning of Random Forest and MLP various parameter settings have been tried  and the model with best performing settings has been chosen.\n",
    "\n",
    "The decision tree J48 algorithm was run within Weka on a slightly different dataset (missing values were not removed to use as many instances as possible, cross validation was 10-fold as opposed to 3-fold due to a higher dataset etc.) thus this results are not directly comparable but were provided as a reference to give an indication of the performance of this algorithm. \n",
    "\n",
    "As can be seen in the feature importance bar plots above, similar features were on the top despite using two different classification algorithms and two different feature ranking methods. If we consider just the three highest-ranking features of each of the feature ranking methods for both algortihms (RF and MLP) the features that overlap are DSP, DSP_like, ASP, Dinophysis fortii and Dinophysis caudata. These can be shown to the domain experts for validation and interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d9c9a9-8f66-45d7-9767-8555adcd5d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
